  'torch.CmdLine.dok':
      'prefix': 'torchCmdLinedok'
      'body': """
        <a name="torch.CmdLine.dok"></a>
        # CmdLine #
        
        This class provides a parameter parsing framework which is very
        useful when one needs to run several experiments that rely on
        different parameter settings that are passed in the command line.
        This class will also override the default print function to direct
        all the output to a log file as well as screen at the same time.
        
        A sample `lua` file is given below that makes use of `CmdLine`
        class.
        
        ```lua
        
        cmd = torch.CmdLine()
        cmd:text()
        cmd:text()
        cmd:text('Training a simple network')
        cmd:text()
        cmd:text('Options')
        cmd:option('-seed',123,'initial random seed')
        cmd:option('-booloption',false,'boolean option')
        cmd:option('-stroption','mystring','string option')
        cmd:text()
        
        -- parse input params
        params = cmd:parse(arg)
        
        params.rundir = cmd:string('experiment', params, {dir=true})
        paths.mkdir(params.rundir)
        
        -- create log file
        cmd:log(params.rundir .. '/log', params)
        
        ```
        
        When this file is run on the th command line as follows
        ```shell
        # th myscript.lua
        ```
        
        It will produce the following output:
        
        ```
        [program started on Tue Jan 10 15:33:49 2012]
        [command line arguments]
        booloption	false
        seed	123
        rundir	experiment
        stroption	mystring
        [----------------------]
        booloption	false
        seed	123
        rundir	experiment
        stroption	mystring
        ```
        
        The same output will also be written to file
        `experiment/log`. Whenever one of the options are passed on the
        command line and is different than the default value, the `rundir`
        is name is produced to reflect the parameter setting.
        
        ```shell
        # th myscript.lua -seed 456 -stroption mycustomstring
        ```
        
        This will produce the following output:
        
        ```
        [program started on Tue Jan 10 15:36:55 2012]
        [command line arguments]
        booloption	false
        seed	456
        rundir	experiment,seed=456,stroption=mycustomstring
        stroption	mycustomstring
        [----------------------]
        booloption	false
        seed	456
        rundir	experiment,seed=456,stroption=mycustomstring
        stroption	mycustomstring
        ```
        
        and the output will be logged in
        `experiment,seed=456,stroption=mycustomstring/log`
        
        
        """

   'torch.CmdLine.addtime':
      'prefix': 'torchCmdLineaddtime'
      'body': """
        <a name="torch.CmdLine.addtime"></a>
        ### addTime([name] [,format]) ###
        
        Adds a prefix to every line in the log file with the date/time in the
        given format with an optional name argument. The date/time format is
        the same as `os.date()`. Note that the prefix is only added to the
        log file, not the screen output. The default value for name is empty
        and the default format is '%F %T'.
        
        The final produced output for the following command is:
        
        ```lua
        > cmd:addTime('your project name','%F %T')
        > print('Your log message')
        ```
        
        ```
        2012-02-07 08:21:56[your project name]: Your log message
        ```
        
        
        """

   'torch.CmdLine.log':
      'prefix': 'torchCmdLinelog'
      'body': """
        <a name="torch.CmdLine.log"></a>
        ### log(filename, parameter_table) ###
        
        It sets the log filename to `filename` and prints the values of
        parameters in the `parameter_table`.
        
        
        """

   'torch.CmdLine.option':
      'prefix': 'torchCmdLineoption'
      'body': """
        <a name="torch.CmdLine.option"></a>
        ### option(name, default, help) ###
        
        Stores an option argument. The name should always start with '-'.
        
        
        """

   'torch.CmdLine.parse':
      'prefix': 'torchCmdLineparse'
      'body': """
        <a name="torch.CmdLine.parse"></a>
        ### [table] parse(arg) ###
        
        Parses a given table, `arg` is by default the argument table that 
        is created by `lua` using the command line arguments passed to the 
        executable. Returns a table of option values.
        
        
        """

   'torch.CmdLine.silent':
      'prefix': 'torchCmdLinesilent'
      'body': """
        <a name="torch.CmdLine.silent"></a>
        ### silent() ###
        
        Silences the output to standard output. The only output is written to
        the log file.
        
        
        """

   'torch.CmdLine.string':
      'prefix': 'torchCmdLinestring'
      'body': """
        <a name="torch.CmdLine.string"></a>
        ### [string] string(prefix, params, ignore) ###
        
        Returns a string representation of the options by concatenating the
        non-default options. `ignore` is a table `{dir=true}`, which will
        ensure that option named `dir` will be ignored while creating the
        string representation.
        
        This function is useful for creating unique experiment directories that
        depend on the parameter settings.
        
        
        """

   'torch.CmdLine.text':
      'prefix': 'torchCmdLinetext'
      'body': """
        <a name="torch.CmdLine.text"></a>
        ### text(string) ###
        
        Logs a custom text message.
        
        
        
        
        """

   'nn.Containers':
      'prefix': 'nnContainers'
      'body': """
        <a name="nn.Containers"></a>
        # Containers #
        Complex neural networks are easily built using container classes:
        
        * [Container](#nn.Container) : abstract class inherited by containers ;
        * [Sequential](#nn.Sequential) : plugs layers in a feed-forward fully connected manner ;
        * [Parallel](#nn.Parallel) : applies its `ith` child module to the  `ith` slice of the input Tensor ;
        * [Concat](#nn.Concat) : concatenates in one layer several modules along dimension `dim` ;
        * [DepthConcat](#nn.DepthConcat) : like Concat, but adds zero-padding when non-`dim` sizes don't match;
        
        See also the [Table Containers](#nn.TableContainers) for manipulating tables of [Tensors](https://github.com/torch/torch7/blob/master/doc/tensor.md).
        
        
        """

   'nn.Container':
      'prefix': 'nnContainer'
      'body': """
        <a name="nn.Container"></a>
        ## Container ##
        
        This is an abstract [Module](module.md#nn.Module) class which declares methods defined in all containers.
        It reimplements many of the Module methods such that calls are propagated to the 
        contained modules. For example, a call to [zeroGradParameters](module.md#nn.Module.zeroGradParameters)
        will be propagated to all contained modules.
        
        
        """

   'nn.Container.add':
      'prefix': 'nnContaineradd'
      'body': """
        <a name="nn.Container.add"></a>
        ### add(module) ###
        Adds the given `module` to the container. The order is important
        
        
        """

   'nn.Container.get':
      'prefix': 'nnContainerget'
      'body': """
        <a name="nn.Container.get"></a>
        ### get(index) ###
        Returns the contained modules at index `index`.
        
        
        """

   'nn.Container.size':
      'prefix': 'nnContainersize'
      'body': """
        <a name="nn.Container.size"></a>
        ### size() ###
        Returns the number of contained modules.
        
        
        """

   'nn.Sequential':
      'prefix': 'nnSequential'
      'body': """
        <a name="nn.Sequential"></a>
        ## Sequential ##
        
        Sequential provides a means to plug layers together
        in a feed-forward fully connected manner.
        
        E.g. 
        creating a one hidden-layer multi-layer perceptron is thus just as easy as:
        ```lua
        mlp = nn.Sequential()
        mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units
        mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function
        mlp:add( nn.Linear(25, 1) ) -- 1 output
        
        print(mlp:forward(torch.randn(10)))
        ```
        which gives the output:
        ```lua
        -0.1815
        [torch.Tensor of dimension 1]
        ```
        
        
        """

   'nn.Sequential.remove':
      'prefix': 'nnSequentialremove'
      'body': """
        <a name="nn.Sequential.remove"></a>
        ### remove([index]) ###
        
        Remove the module at the given `index`. If `index` is not specified, remove the last layer.
        
        ```lua
        model = nn.Sequential()
        model:add(nn.Linear(10, 20))
        model:add(nn.Linear(20, 20))
        model:add(nn.Linear(20, 30))
        model:remove(2)
        > model
        nn.Sequential {
        [input -> (1) -> (2) -> output]
        (1): nn.Linear(10 -> 20)
        (2): nn.Linear(20 -> 30)
        }
        ```
        
        
        
        """

   'nn.Sequential.insert':
      'prefix': 'nnSequentialinsert'
      'body': """
        <a name="nn.Sequential.insert"></a>
        ### insert(module, [index]) ###
        
        Inserts the given `module` at the given `index`. If `index` is not specified, the incremented length of the sequence is used and so this is equivalent to use `add(module)`.
        
        ```lua
        model = nn.Sequential()
        model:add(nn.Linear(10, 20))
        model:add(nn.Linear(20, 30))
        model:insert(nn.Linear(20, 20), 2)
        > model
        nn.Sequential {
        [input -> (1) -> (2) -> (3) -> output]
        (1): nn.Linear(10 -> 20)
        (2): nn.Linear(20 -> 20)      -- The inserted layer
        (3): nn.Linear(20 -> 30)
        }
        ```
        
        
        
        
        """

   'nn.Parallel':
      'prefix': 'nnParallel'
      'body': """
        <a name="nn.Parallel"></a>
        ## Parallel ##
        
        `module` = `Parallel(inputDimension,outputDimension)`
        
        Creates a container module that applies its `ith` child module to the  `ith` slice of the input Tensor by using [select](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-selectdim-index) 
        on dimension `inputDimension`. It concatenates the results of its contained modules together along dimension `outputDimension`.
        
        Example:
        ```lua
        mlp=nn.Parallel(2,1);     -- iterate over dimension 2 of input
        mlp:add(nn.Linear(10,3)); -- apply to first slice
        mlp:add(nn.Linear(10,2))  -- apply to first second slice
        print(mlp:forward(torch.randn(10,2)))
        ```
        gives the output:
        ```lua
        -0.5300
        -1.1015
        0.7764
        0.2819
        -0.6026
        [torch.Tensor of dimension 5]
        ```
        
        A more complicated example:
        ```lua
        
        mlp=nn.Sequential();
        c=nn.Parallel(1,2)
        for i=1,10 do
        local t=nn.Sequential()
        t:add(nn.Linear(3,2))
        t:add(nn.Reshape(2,1))
        c:add(t)
        end
        mlp:add(c)
        
        pred=mlp:forward(torch.randn(10,3))
        print(pred)
        
        for i=1,10000 do     -- Train for a few iterations
        x=torch.randn(10,3);
        y=torch.ones(2,10);
        pred=mlp:forward(x)
        
        criterion= nn.MSECriterion()
        local err=criterion:forward(pred,y)
        local gradCriterion = criterion:backward(pred,y);
        mlp:zeroGradParameters();
        mlp:backward(x, gradCriterion); 
        mlp:updateParameters(0.01);
        print(err)
        end
        ```
        
        
        
        """

   'nn.Concat':
      'prefix': 'nnConcat'
      'body': """
        <a name="nn.Concat"></a>
        ## Concat ##
        
        ```lua
        module = nn.Concat(dim)
        ```
        Concat concatenates the output of one layer of "parallel" modules along the
        provided dimension `dim`: they take the same inputs, and their output is
        concatenated.
        ```lua
        mlp=nn.Concat(1);
        mlp:add(nn.Linear(5,3))
        mlp:add(nn.Linear(5,7))
        print(mlp:forward(torch.randn(5)))
        ```
        which gives the output:
        ```lua
        0.7486
        0.1349
        0.7924
        -0.0371
        -0.4794
        0.3044
        -0.0835
        -0.7928
        0.7856
        -0.1815
        [torch.Tensor of dimension 10]
        ```
        
        
        """

   'nn.DepthConcat':
      'prefix': 'nnDepthConcat'
      'body': """
        <a name="nn.DepthConcat"></a>
        ## DepthConcat ##
        
        ```lua
        module = nn.DepthConcat(dim)
        ```
        DepthConcat concatenates the output of one layer of "parallel" modules along the
        provided dimension `dim`: they take the same inputs, and their output is
        concatenated. For dimensions other than `dim` having different sizes,
        the smaller tensors are copied in the center of the output tensor, 
        effectively padding the borders with zeros.
        
        The module is particularly useful for concatenating the output of [Convolutions](convolution.md) 
        along the depth dimension (i.e. `nOutputFrame`). 
        This is used to implement the *DepthConcat* layer 
        of the [Going deeper with convolutions](http://arxiv.org/pdf/1409.4842v1.pdf) article.
        The normal [Concat](#nn.Concat) Module can't be used since the spatial 
        dimensions (height and width) of the output Tensors requiring concatenation 
        may have different values. To deal with this, the output uses the largest 
        spatial dimensions and adds zero-padding around the smaller Tensors.
        ```lua
        inputSize = 3
        outputSize = 2
        input = torch.randn(inputSize,7,7)
        mlp=nn.DepthConcat(1);
        mlp:add(nn.SpatialConvolutionMM(inputSize, outputSize, 1, 1))
        mlp:add(nn.SpatialConvolutionMM(inputSize, outputSize, 3, 3))
        mlp:add(nn.SpatialConvolutionMM(inputSize, outputSize, 4, 4))
        print(mlp:forward(input))
        ```
        which gives the output:
        ```lua
        (1,.,.) = 
        -0.2874  0.6255  1.1122  0.4768  0.9863 -0.2201 -0.1516
        0.2779  0.9295  1.1944  0.4457  1.1470  0.9693  0.1654
        -0.5769 -0.4730  0.3283  0.6729  1.3574 -0.6610  0.0265
        0.3767  1.0300  1.6927  0.4422  0.5837  1.5277  1.1686
        0.8843 -0.7698  0.0539 -0.3547  0.6904 -0.6842  0.2653
        0.4147  0.5062  0.6251  0.4374  0.3252  0.3478  0.0046
        0.7845 -0.0902  0.3499  0.0342  1.0706 -0.0605  0.5525
        
        (2,.,.) = 
        -0.7351 -0.9327 -0.3092 -1.3395 -0.4596 -0.6377 -0.5097
        -0.2406 -0.2617 -0.3400 -0.4339 -0.3648  0.1539 -0.2961
        -0.7124 -1.2228 -0.2632  0.1690  0.4836 -0.9469 -0.7003
        -0.0221  0.1067  0.6975 -0.4221 -0.3121  0.4822  0.6617
        0.2043 -0.9928 -0.9500 -1.6107  0.1409 -1.3548 -0.5212
        -0.3086 -0.0298 -0.2031  0.1026 -0.5785 -0.3275 -0.1630
        0.0596 -0.6097  0.1443 -0.8603 -0.2774 -0.4506 -0.5367
        
        (3,.,.) = 
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        0.0000 -0.7326  0.3544  0.1821  0.4796  1.0164  0.0000
        0.0000 -0.9195 -0.0567 -0.1947  0.0169  0.1924  0.0000
        0.0000  0.2596  0.6766  0.0939  0.5677  0.6359  0.0000
        0.0000 -0.2981 -1.2165 -0.0224 -1.1001  0.0008  0.0000
        0.0000 -0.1911  0.2912  0.5092  0.2955  0.7171  0.0000
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        
        (4,.,.) = 
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        0.0000 -0.8263  0.3646  0.6750  0.2062  0.2785  0.0000
        0.0000 -0.7572  0.0432 -0.0821  0.4871  1.9506  0.0000
        0.0000 -0.4609  0.4362  0.5091  0.8901 -0.6954  0.0000
        0.0000  0.6049 -0.1501 -0.4602 -0.6514  0.5439  0.0000
        0.0000  0.2570  0.4694 -0.1262  0.5602  0.0821  0.0000
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        
        (5,.,.) = 
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        0.0000  0.3158  0.4389 -0.0485 -0.2179  0.0000  0.0000
        0.0000  0.1966  0.6185 -0.9563 -0.3365  0.0000  0.0000
        0.0000 -0.2892 -0.9266 -0.0172 -0.3122  0.0000  0.0000
        0.0000 -0.6269  0.5349 -0.2520 -0.2187  0.0000  0.0000
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        
        (6,.,.) = 
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        0.0000  1.1148  0.2324 -0.1093  0.5024  0.0000  0.0000
        0.0000 -0.2624 -0.5863  0.3444  0.3506  0.0000  0.0000
        0.0000  0.1486  0.8413  0.6229 -0.0130  0.0000  0.0000
        0.0000  0.8446  0.3801 -0.2611  0.8140  0.0000  0.0000
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000
        [torch.DoubleTensor of dimension 6x7x7]
        ```
        Note how the last 2 of 6 filter maps have 1 column of zero-padding 
        on the left and top, as well as 2 on the right and bottom. 
        This is inevitable when the component
        module output tensors non-`dim` sizes aren't all odd or even. 
        Such that in order to keep the mappings aligned, one need 
        only ensure that these be all odd (or even).
        
        
        """

   'nn.TableContainers':
      'prefix': 'nnTableContainers'
      'body': """
        <a name="nn.TableContainers"></a>
        ## Table Containers ##
        While the above containers are used for manipulating input [Tensors](https://github.com/torch/torch7/blob/master/doc/tensor.md), table containers are used for manipulating tables :
        * [ConcatTable](table.md#nn.ConcatTable)
        * [ParallelTable](table.md#nn.ParallelTable)
        
        These, along with all other modules for manipulating tables can be found [here](table.md).
        
        """

   'nn.convlayers.dok':
      'prefix': 'nnconvlayersdok'
      'body': """
        <a name="nn.convlayers.dok"></a>
        # Convolutional layers #
        
        A convolution is an integral that expresses the amount of overlap of one function `g` as it is shifted over another function `f`. It therefore "blends" one function with another. The neural network package supports convolution, pooling, subsampling and other relevant facilities. These are divided base on the dimensionality of the input and output [Tensors](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor):
        
        * [Temporal Modules](#nn.TemporalModules) apply to sequences with a one-dimensional relationship
        (e.g. sequences of words, phonemes and letters. Strings of some kind).
        * [TemporalConvolution](#nn.TemporalConvolution) : a 1D convolution over an input sequence ;
        * [TemporalSubSampling](#nn.TemporalSubSampling) : a 1D sub-sampling over an input sequence ;
        * [TemporalMaxPooling](#nn.TemporalMaxPooling) : a 1D max-pooling operation over an input sequence ;
        * [LookupTable](#nn.LookupTable) : a convolution of width `1`, commonly used for word embeddings ;
        * [Spatial Modules](#nn.SpatialModules) apply to inputs with two-dimensional relationships (e.g. images):
        * [SpatialConvolution](#nn.SpatialConvolution) : a 2D convolution over an input image ;
        * [SpatialFullConvolution](#nn.SpatialFullConvolution) : a 2D full convolution over an input image ;
        * [SpatialSubSampling](#nn.SpatialSubSampling) : a 2D sub-sampling over an input image ;
        * [SpatialMaxPooling](#nn.SpatialMaxPooling) : a 2D max-pooling operation over an input image ;
        * [SpatialFractionalMaxPooling](#nn.SpatialFractionalMaxPooling) : a 2D fractional max-pooling operation over an input image ;
        * [SpatialAveragePooling](#nn.SpatialAveragePooling) : a 2D average-pooling operation over an input image ;
        * [SpatialAdaptiveMaxPooling](#nn.SpatialAdaptiveMaxPooling) : a 2D max-pooling operation which adapts its parameters dynamically such that the output is of fixed size ;
        * [SpatialMaxUnpooling](#nn.SpatialMaxUnpooling) : a 2D max-unpooling operation ;
        * [SpatialLPPooling](#nn.SpatialLPPooling) : computes the `p` norm in a convolutional manner on a set of input images ;
        * [SpatialConvolutionMap](#nn.SpatialConvolutionMap) : a 2D convolution that uses a generic connection table ;
        * [SpatialZeroPadding](#nn.SpatialZeroPadding) : padds a feature map with specified number of zeros ;
        * [SpatialSubtractiveNormalization](#nn.SpatialSubtractiveNormalization) : a spatial subtraction operation on a series of 2D inputs using
        * [SpatialBatchNormalization](#nn.SpatialBatchNormalization): mean/std normalization over the mini-batch inputs and pixels, with an optional affine transform that follows
        a kernel for computing the weighted average in a neighborhood ;
        * [SpatialUpsamplingNearest](#nn.SpatialUpSamplingNearest): A simple upsampler applied to every channel of the feature map.
        * [Volumetric Modules](#nn.VolumetricModules) apply to inputs with three-dimensional relationships (e.g. videos) :
        * [VolumetricConvolution](#nn.VolumetricConvolution) : a 3D convolution over an input video (a sequence of images) ;
        * [VolumetricFullConvolution](#nn.VolumetricFullConvolution) : a 3D full convolution over an input video (a sequence of images) ;
        * [VolumetricMaxPooling](#nn.VolumetricMaxPooling) : a 3D max-pooling operation over an input video.
        * [VolumetricAveragePooling](#nn.VolumetricAveragePooling) : a 3D average-pooling operation over an input video.
        
        
        """

   'nn.TemporalModules':
      'prefix': 'nnTemporalModules'
      'body': """
        <a name="nn.TemporalModules"></a>
        ## Temporal Modules ##
        Excluding an optional first batch dimension, temporal layers expect a 2D Tensor as input. The
        first dimension is the number of frames in the sequence (e.g. `nInputFrame`), the last dimension
        is the number of features per frame (e.g. `inputFrameSize`). The output will normally have the same number
        of dimensions, although the size of each dimension may change. These are commonly used for processing acoustic signals or sequences of words, i.e. in Natural Language Processing.
        
        Note: The [LookupTable](#nn.LookupTable) is special in that while it does output a temporal Tensor of size `nOutputFrame x outputFrameSize`,
        its input is a 1D Tensor of indices of size `nIndices`. Again, this is excluding the option first batch dimension.
        
        
        """

   'nn.TemporalConvolution':
      'prefix': 'nnTemporalConvolution'
      'body': """
        <a name="nn.TemporalConvolution"></a>
        ## TemporalConvolution ##
        
        ```lua
        module = nn.TemporalConvolution(inputFrameSize, outputFrameSize, kW, [dW])
        ```
        
        Applies a 1D convolution over an input sequence composed of `nInputFrame` frames. The `input` tensor in
        `forward(input)` is expected to be a 2D tensor (`nInputFrame x inputFrameSize`) or a 3D tensor (`nBatchFrame x nInputFrame x inputFrameSize`).
        
        The parameters are the following:
        * `inputFrameSize`: The input frame size expected in sequences given into `forward()`.
        * `outputFrameSize`: The output frame size the convolution layer will produce.
        * `kW`: The kernel width of the convolution
        * `dW`: The step of the convolution. Default is `1`.
        
        Note that depending of the size of your kernel, several (of the last)
        frames of the sequence might be lost. It is up to the user to add proper padding frames in the input
        sequences.
        
        If the input sequence is a 2D tensor of dimension `nInputFrame x inputFrameSize`, the output sequence will be
        `nOutputFrame x outputFrameSize` where
        ```lua
        nOutputFrame = (nInputFrame - kW) / dW + 1
        ```
        
        If the input sequence is a 3D tensor of dimension `nBatchFrame x nInputFrame x inputFrameSize`, the output sequence will be
        `nBatchFrame x nOutputFrame x outputFrameSize`.
        
        The parameters of the convolution can be found in `self.weight` (Tensor of
        size `outputFrameSize x (inputFrameSize x kW) `) and `self.bias` (Tensor of
        size `outputFrameSize`). The corresponding gradients can be found in
        `self.gradWeight` and `self.gradBias`.
        
        For a 2D input, the output value of the layer can be precisely described as:
        ```lua
        output[t][i] = bias[i]
        + sum_j sum_{k=1}^kW weight[i][j][k]
        * input[dW*(t-1)+k)][j]
        ```
        
        Here is a simple example:
        
        ```lua
        inp=5;  -- dimensionality of one sequence element
        outp=1; -- number of derived features for one sequence element
        kw=1;   -- kernel only operates on one sequence element per step
        dw=1;   -- we step once and go on to the next sequence element
        
        mlp=nn.TemporalConvolution(inp,outp,kw,dw)
        
        x=torch.rand(7,inp) -- a sequence of 7 elements
        print(mlp:forward(x))
        ```
        which gives:
        ```lua
        -0.9109
        -0.9872
        -0.6808
        -0.9403
        -0.9680
        -0.6901
        -0.6387
        [torch.Tensor of dimension 7x1]
        ```
        
        This is equivalent to:
        ```lua
        weights=torch.reshape(mlp.weight,inp) -- weights applied to all
        bias= mlp.bias[1];
        for i=1,x:size(1) do -- for each sequence element
        element= x[i]; -- features of ith sequence element
        print(element:dot(weights) + bias)
        end
        ```
        which gives:
        ```lua
        -0.91094998687717
        -0.98721705771773
        -0.68075004276185
        -0.94030132495887
        -0.96798754116609
        -0.69008470895581
        -0.63871422284166
        ```
        
        
        """

   'nn.TemporalMaxPooling':
      'prefix': 'nnTemporalMaxPooling'
      'body': """
        <a name="nn.TemporalMaxPooling"></a>
        ## TemporalMaxPooling ##
        
        ```lua
        module = nn.TemporalMaxPooling(kW, [dW])
        ```
        
        Applies 1D max-pooling operation in `kW` regions by step size
        `dW` steps. Input sequence composed of `nInputFrame` frames. The `input` tensor in
        `forward(input)` is expected to be a 2D tensor (`nInputFrame x inputFrameSize`)
        or a 3D tensor (`nBatchFrame x nInputFrame x inputFrameSize`).
        
        If the input sequence is a 2D tensor of dimension `nInputFrame x inputFrameSize`, the output sequence will be
        `nOutputFrame x inputFrameSize` where
        ```lua
        nOutputFrame = (nInputFrame - kW) / dW + 1
        ```
        
        
        """

   'nn.TemporalSubSampling':
      'prefix': 'nnTemporalSubSampling'
      'body': """
        <a name="nn.TemporalSubSampling"></a>
        ## TemporalSubSampling ##
        
        ```lua
        module = nn.TemporalSubSampling(inputFrameSize, kW, [dW])
        ```
        
        Applies a 1D sub-sampling over an input sequence composed of `nInputFrame` frames. The `input` tensor in
        `forward(input)` is expected to be a 2D tensor (`nInputFrame x inputFrameSize`). The output frame size
        will be the same as the input one (`inputFrameSize`).
        
        The parameters are the following:
        * `inputFrameSize`: The input frame size expected in sequences given into `forward()`.
        * `kW`: The kernel width of the sub-sampling
        * `dW`: The step of the sub-sampling. Default is `1`.
        
        Note that depending of the size of your kernel, several (of the last)
        frames of the sequence might be lost. It is up to the user to add proper padding frames in the input
        sequences.
        
        If the input sequence is a 2D tensor `nInputFrame x inputFrameSize`, the output sequence will be
        `inputFrameSize x nOutputFrame` where
        ```lua
        nOutputFrame = (nInputFrame - kW) / dW + 1
        ```
        
        The parameters of the sub-sampling can be found in `self.weight` (Tensor of
        size `inputFrameSize`) and `self.bias` (Tensor of
        size `inputFrameSize`). The corresponding gradients can be found in
        `self.gradWeight` and `self.gradBias`.
        
        The output value of the layer can be precisely described as:
        ```lua
        output[i][t] = bias[i] + weight[i] * sum_{k=1}^kW input[i][dW*(t-1)+k)]
        ```
        
        
        """

   'nn.LookupTable':
      'prefix': 'nnLookupTable'
      'body': """
        <a name="nn.LookupTable"></a>
        ## LookupTable ##
        
        ```lua
        module = nn.LookupTable(nIndex, sizes)
        ```
        or
        ```lua
        module = nn.LookupTable(nIndex, size1, [size2], [size3], ...)
        ```
        
        This layer is a particular case of a convolution, where the width of the convolution would be `1`.
        When calling `forward(input)`, it assumes `input` is a 1D or 2D tensor filled with indices.
        If the input is a matrix, then each row is assumed to be an input sample of given batch. Indices start
        at `1` and can go up to `nIndex`. For each index, it outputs a corresponding `Tensor` of size
        specified by `sizes` (a `LongStorage`) or `size1 x size2 x...`.
        
        Given a 1D input, the output tensors are concatenated,
        generating a `n x size1 x size2 x ... x sizeN` tensor, where `n`
        is the size of a 1D `input` tensor.
        
        Again with a 1D input, when only `size1` is provided, the `forward(input)` is equivalent to
        performing the following matrix-matrix multiplication in an efficient manner:
        ```lua
        M P
        ```
        where `M` is a 2D matrix `size1 x nIndex` containing the parameters of the lookup-table and
        `P` is a 2D matrix, where each column vector `i` is a zero vector except at index `input[i]` where it is `1`.
        
        1D example:
        ```lua
        -- a lookup table containing 10 tensors of size 3
        module = nn.LookupTable(10, 3)
        
        input = torch.Tensor{1,2,1,10}
        print(module:forward(input))
        ```
        
        Outputs something like:
        ```lua
        -1.4415 -0.1001 -0.1708
        -0.6945 -0.4350  0.7977
        -1.4415 -0.1001 -0.1708
        -0.0745  1.9275  1.0915
        [torch.DoubleTensor of dimension 4x3]
        ```
        Note that the first row vector is the same as the 3rd one!
        
        Given a 2D input tensor of size `m x n`, the output is a `m x n x size1 x size2 x ... x sizeN`
        tensor, where `m` is the number of samples in
        the batch and `n` is the number of indices per sample.
        
        2D example:
        ```lua
        -- a lookup table containing 10 tensors of size 3
        module = nn.LookupTable(10, 3)
        
        -- a batch of 2 samples of 4 indices each
        input = torch.Tensor({{1,2,4,5},{4,3,2,10}})
        print(module:forward(input))
        ```
        
        Outputs something like:
        ```lua
        (1,.,.) =
        -0.0570 -1.5354  1.8555
        -0.9067  1.3392  0.6275
        1.9662  0.4645 -0.8111
        0.1103  1.7811  1.5969
        
        (2,.,.) =
        1.9662  0.4645 -0.8111
        0.0026 -1.4547 -0.5154
        -0.9067  1.3392  0.6275
        -0.0193 -0.8641  0.7396
        [torch.DoubleTensor of dimension 2x4x3]
        ```
        
        
        """

   'nn.SpatialModules':
      'prefix': 'nnSpatialModules'
      'body': """
        <a name="nn.SpatialModules"></a>
        ## Spatial Modules ##
        Excluding an optional batch dimension, spatial layers expect a 3D Tensor as input. The
        first dimension is the number of features (e.g. `frameSize`), the last two dimensions
        are spatial (e.g. `height x width`). These are commonly used for processing images.
        
        
        """

   'nn.SpatialConvolution':
      'prefix': 'nnSpatialConvolution'
      'body': """
        <a name="nn.SpatialConvolution"></a>
        ### SpatialConvolution ###
        
        ```lua
        module = nn.SpatialConvolution(nInputPlane, nOutputPlane, kW, kH, [dW], [dH], [padW], [padH])
        ```
        
        Applies a 2D convolution over an input image composed of several input planes. The `input` tensor in
        `forward(input)` is expected to be a 3D tensor (`nInputPlane x height x width`).
        
        The parameters are the following:
        * `nInputPlane`: The number of expected input planes in the image given into `forward()`.
        * `nOutputPlane`: The number of output planes the convolution layer will produce.
        * `kW`: The kernel width of the convolution
        * `kH`: The kernel height of the convolution
        * `dW`: The step of the convolution in the width dimension. Default is `1`.
        * `dH`: The step of the convolution in the height dimension. Default is `1`.
        * `padW`: The additional zeros added per width to the input planes. Default is `0`, a good number is `(kW-1)/2`.
        * `padH`: The additional zeros added per height to the input planes. Default is `padW`, a good number is `(kH-1)/2`.
        
        Note that depending of the size of your kernel, several (of the last)
        columns or rows of the input image might be lost. It is up to the user to
        add proper padding in images.
        
        If the input image is a 3D tensor `nInputPlane x height x width`, the output image size
        will be `nOutputPlane x oheight x owidth` where
        ```lua
        owidth  = floor((width  + 2*padW - kW) / dW + 1)
        oheight = floor((height + 2*padH - kH) / dH + 1)
        ```
        
        The parameters of the convolution can be found in `self.weight` (Tensor of
        size `nOutputPlane x nInputPlane x kH x kW`) and `self.bias` (Tensor of
        size `nOutputPlane`). The corresponding gradients can be found in
        `self.gradWeight` and `self.gradBias`.
        
        The output value of the layer can be precisely described as:
        ```lua
        output[i][j][k] = bias[k]
        + sum_l sum_{s=1}^kW sum_{t=1}^kH weight[s][t][l][k]
        * input[dW*(i-1)+s)][dH*(j-1)+t][l]
        ```
        
        
        
        """

   'nn.SpatialConvolutionMap':
      'prefix': 'nnSpatialConvolutionMap'
      'body': """
        <a name="nn.SpatialConvolutionMap"></a>
        ### SpatialConvolutionMap ###
        
        ```lua
        module = nn.SpatialConvolutionMap(connectionMatrix, kW, kH, [dW], [dH])
        ```
        
        This class is a generalization of
        [nn.SpatialConvolution](#nn.SpatialConvolution). It uses a generic
        connection table between input and output features. The
        [nn.SpatialConvolution](#nn.SpatialConvolution) is equivalent to
        using a [full connection table](#nn.tables.full). One can specify
        different types of connection tables.
        
        
        """

   'nn.tables.full':
      'prefix': 'nntablesfull'
      'body': """
        <a name="nn.tables.full"></a>
        #### Full Connection Table ####
        
        ```lua
        table = nn.tables.full(nin,nout)
        ```
        
        This is a precomputed table that specifies connections between every
        input and output node.
        
        
        """

   'nn.tables.onetoone':
      'prefix': 'nntablesonetoone'
      'body': """
        <a name="nn.tables.onetoone"></a>
        #### One to One Connection Table ####
        
        ```lua
        table = nn.tables.oneToOne(n)
        ```
        
        This is a precomputed table that specifies a single connection to each
        output node from corresponding input node.
        
        
        """

   'nn.tables.random':
      'prefix': 'nntablesrandom'
      'body': """
        <a name="nn.tables.random"></a>
        #### Random Connection Table ####
        
        ```lua
        table = nn.tables.random(nin,nout, nto)
        ```
        
        This table is randomly populated such that each output unit has
        `nto` incoming connections. The algorithm tries to assign uniform
        number of outgoing connections to each input node if possible.
        
        
        """

   'nn.SpatialFullConvolution':
      'prefix': 'nnSpatialFullConvolution'
      'body': """
        <a name="nn.SpatialFullConvolution"></a>
        ### SpatialFullConvolution ###
        
        ```lua
        module = nn.SpatialFullConvolution(nInputPlane, nOutputPlane, kW, kH, [dW], [dH], [padW], [padH])
        ```
        
        Applies a 2D full convolution over an input image composed of several input planes. The `input` tensor in
        `forward(input)` is expected to be a 3D or 4D tensor.
        
        Other frameworks call this operation "In-network Upsampling", "Fractionally-strided convolution", "Backwards Convolution," "Deconvolution", or "Upconvolution."
        
        The parameters are the following:
        * `nInputPlane`: The number of expected input planes in the image given into `forward()`.
        * `nOutputPlane`: The number of output planes the convolution layer will produce.
        * `kW`: The kernel width of the convolution
        * `kH`: The kernel height of the convolution
        * `dW`: The step of the convolution in the width dimension. Default is `1`.
        * `dH`: The step of the convolution in the height dimension. Default is `1`.
        * `padW`: The additional zeros added per width to the input planes. Default is `0`, a good number is `(kW-1)/2`.
        * `padH`: The additional zeros added per height to the input planes. Default is `0`, a good number is `(kH-1)/2`.
        
        If the input image is a 3D tensor `nInputPlane x height x width`, the output image size
        will be `nOutputPlane x oheight x owidth` where
        ```lua
        owidth  = (width  - 1) * dW - 2*padW + kW
        oheight = (height - 1) * dH - 2*padH + kH
        ```
        
        Further information about the full convolution can be found in the following paper: [Fully Convolutional Networks for Semantic Segmentation](http://www.cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf).
        
        
        """

   'nn.SpatialLPPooling':
      'prefix': 'nnSpatialLPPooling'
      'body': """
        <a name="nn.SpatialLPPooling"></a>
        ### SpatialLPPooling ###
        
        ```lua
        module = nn.SpatialLPPooling(nInputPlane, pnorm, kW, kH, [dW], [dH])
        ```
        
        Computes the `p` norm in a convolutional manner on a set of 2D input planes.
        
        
        """

   'nn.SpatialMaxPooling':
      'prefix': 'nnSpatialMaxPooling'
      'body': """
        <a name="nn.SpatialMaxPooling"></a>
        ### SpatialMaxPooling ###
        
        ```lua
        module = nn.SpatialMaxPooling(kW, kH [, dW, dH, padW, padH])
        ```
        
        Applies 2D max-pooling operation in `kWxkH` regions by step size
        `dWxdH` steps. The number of output features is equal to the number of
        input planes.
        
        If the input image is a 3D tensor `nInputPlane x height x width`, the output
        image size will be `nOutputPlane x oheight x owidth` where
        
        ```lua
        owidth  = op((width  + 2*padW - kW) / dW + 1)
        oheight = op((height + 2*padH - kH) / dH + 1)
        ```
        
        `op` is a rounding operator. By default, it is `floor`. It can be changed
        by calling `:ceil()` or `:floor()` methods.
        
        
        """

   'nn.SpatialFractionalMaxPooling':
      'prefix': 'nnSpatialFractionalMaxPooling'
      'body': """
        <a name="nn.SpatialFractionalMaxPooling"></a>
        ### SpatialFractionalMaxPooling ###
        
        ```lua
        module = nn.SpatialFractionalMaxPooling(kW, kH, outW, outH)
        --   the output should be the exact size (outH x outW)
        OR
        module = nn.SpatialFractionalMaxPooling(kW, kH, ratioW, ratioH)
        --   the output should be the size (floor(inH x ratioH) x floor(inW x ratioW))
        --   ratios are numbers between (0, 1) exclusive
        ```
        
        Applies 2D Fractional max-pooling operation as described in the
        paper ["Fractional Max Pooling" by Ben Graham](http://arxiv.org/abs/1412.6071) in the "pseudorandom" mode.
        
        The max-pooling operation is applied in `kWxkH` regions by a stochastic step size determined by the target output size.
        The number of output features is equal to the number of input planes.
        
        There are two constructors available.
        
        Constructor 1:
        ```lua
        module = nn.SpatialFractionalMaxPooling(kW, kH, outW, outH)
        ```
        
        Constructor 2:
        ```lua
        module = nn.SpatialFractionalMaxPooling(kW, kH, ratioW, ratioH)
        ```
        If the input image is a 3D tensor `nInputPlane x height x width`, the output
        image size will be `nOutputPlane x oheight x owidth`
        
        where
        
        ```lua
        owidth  = floor(width * ratioW)
        oheight = floor(height * ratioH)
        ```
        ratios are numbers between (0, 1) exclusive
        
        
        
        """

   'nn.SpatialAveragePooling':
      'prefix': 'nnSpatialAveragePooling'
      'body': """
        <a name="nn.SpatialAveragePooling"></a>
        ### SpatialAveragePooling ###
        
        ```lua
        module = nn.SpatialAveragePooling(kW, kH [, dW, dH, padW, padH])
        ```
        
        Applies 2D average-pooling operation in `kWxkH` regions by step size
        `dWxdH` steps. The number of output features is equal to the number of
        input planes.
        
        If the input image is a 3D tensor `nInputPlane x height x width`, the output
        image size will be `nOutputPlane x oheight x owidth` where
        
        ```lua
        owidth  = op((width  + 2*padW - kW) / dW + 1)
        oheight = op((height + 2*padH - kH) / dH + 1)
        ```
        
        `op` is a rounding operator. By default, it is `floor`. It can be changed
        by calling `:ceil()` or `:floor()` methods.
        
        By default, the output of each pooling region is divided by the number of
        elements inside the padded image (which is usually `kW*kH`, except in some
        corner cases in which it can be smaller). You can also divide by the number
        of elements inside the original non-padded image. To switch between different
        division factors, call `:setCountIncludePad()` or `:setCountExcludePad()`. If
        `padW=padH=0`, both options give the same results.
        
        
        """

   'nn.SpatialAdaptiveMaxPooling':
      'prefix': 'nnSpatialAdaptiveMaxPooling'
      'body': """
        <a name="nn.SpatialAdaptiveMaxPooling"></a>
        ### SpatialAdaptiveMaxPooling ###
        
        ```lua
        module = nn.SpatialAdaptiveMaxPooling(W, H)
        ```
        
        Applies 2D max-pooling operation in an image such that the output is of
        size `WxH`, for any input size. The number of output features is equal
        to the number of input planes.
        
        For an output of dimensions `(owidth,oheight)`, the indexes of the pooling
        region `(j,i)` in the input image of dimensions `(iwidth,iheight)` are
        given by:
        
        ```lua
        x_j_start = floor((j   /owidth)  * iwidth)
        x_j_end   = ceil(((j+1)/owidth)  * iwidth)
        
        y_i_start = floor((i   /oheight) * iheight)
        y_i_end   = ceil(((i+1)/oheight) * iheight)
        ```
        
        
        """

   'nn.SpatialMaxUnpooling':
      'prefix': 'nnSpatialMaxUnpooling'
      'body': """
        <a name="nn.SpatialMaxUnpooling"></a>
        ### SpatialMaxUnpooling ###
        
        ```lua
        module = nn.SpatialMaxUnpooling(poolingModule)
        ```
        
        Applies 2D "max-unpooling" operation using the indices previously computed 
        by the SpatialMaxPooling module `poolingModule`.
        
        When `B = poolingModule:forward(A)` is called, the indices of the maximal 
        values (corresponding to their position within each map) are stored: 
        `B[{n,k,i,j}] = A[{n,k,indices[{n,k,i}],indices[{n,k,j}]}]`. 
        If `C` is a tensor of same size as `B`, `module:updateOutput(C)` outputs a 
        tensor `D` of same size as `A` such that: 
        `D[{n,k,indices[{n,k,i}],indices[{n,k,j}]}] = C[{n,k,i,j}]`.
        
        Module inspired by:
        "Visualizing and understanding convolutional networks" (2014)
        by Matthew Zeiler, Rob Fergus
        
        
        """

   'nn.SpatialSubSampling':
      'prefix': 'nnSpatialSubSampling'
      'body': """
        <a name="nn.SpatialSubSampling"></a>
        ### SpatialSubSampling ###
        
        ```lua
        module = nn.SpatialSubSampling(nInputPlane, kW, kH, [dW], [dH])
        ```
        
        Applies a 2D sub-sampling over an input image composed of several input planes. The `input` tensor in
        `forward(input)` is expected to be a 3D tensor (`nInputPlane x height x width`). The number of output
        planes will be the same as `nInputPlane`.
        
        The parameters are the following:
        * `nInputPlane`: The number of expected input planes in the image given into `forward()`.
        * `kW`: The kernel width of the sub-sampling
        * `kH`: The kernel height of the sub-sampling
        * `dW`: The step of the sub-sampling in the width dimension. Default is `1`.
        * `dH`: The step of the sub-sampling in the height dimension. Default is `1`.
        
        Note that depending of the size of your kernel, several (of the last)
        columns or rows of the input image might be lost. It is up to the user to
        add proper padding in images.
        
        If the input image is a 3D tensor `nInputPlane x height x width`, the output image size
        will be `nInputPlane x oheight x owidth` where
        
        ```lua
        owidth  = (width  - kW) / dW + 1
        oheight = (height - kH) / dH + 1 .
        ```
        
        The parameters of the sub-sampling can be found in `self.weight` (Tensor of
        size `nInputPlane`) and `self.bias` (Tensor of size `nInputPlane`). The
        corresponding gradients can be found in `self.gradWeight` and
        `self.gradBias`.
        
        The output value of the layer can be precisely described as:
        ```lua
        output[i][j][k] = bias[k]
        + weight[k] sum_{s=1}^kW sum_{t=1}^kH input[dW*(i-1)+s)][dH*(j-1)+t][k]
        ```
        
        
        """

   'nn.SpatialUpSamplingNearest':
      'prefix': 'nnSpatialUpSamplingNearest'
      'body': """
        <a name="nn.SpatialUpSamplingNearest"></a>
        ### SpatialUpSamplingNearest ###
        
        ```lua
        module = nn.SpatialUpSamplingNearest(scale)
        ```
        
        Applies a 2D up-sampling over an input image composed of several input planes. The `input` tensor in
        `forward(input)` is expected to be a 3D or 4D tensor (i.e. for 4D: `nBatchPlane x nInputPlane x height x width`). The number of output planes will be the same.  The v dimension is assumed to be the second last dimension (i.e. for 4D it will be the 3rd dim), and the u dimension is assumed to be the last dimension.
        
        The parameters are the following:
        * `scale`: The upscale ratio.  Must be a positive integer
        
        The up-scaling method is simple nearest neighbor, ie:
        
        ```lua
        output(u,v) = input(floor((u-1)/scale)+1, floor((v-1)/scale)+1)
        ```
        
        Where `u` and `v` are index from 1 (as per lua convention).  There are no learnable parameters.
        
        
        """

   'nn.SpatialZeroPadding':
      'prefix': 'nnSpatialZeroPadding'
      'body': """
        <a name="nn.SpatialZeroPadding"></a>
        ### SpatialZeroPadding ###
        
        ```lua
        module = nn.SpatialZeroPadding(padLeft, padRight, padTop, padBottom)
        ```
        
        Each feature map of a given input is padded with specified number of
        zeros. If padding values are negative, then input is cropped.
        
        
        """

   'nn.SpatialSubtractiveNormalization':
      'prefix': 'nnSpatialSubtractiveNormalization'
      'body': """
        <a name="nn.SpatialSubtractiveNormalization"></a>
        ### SpatialSubtractiveNormalization ###
        
        ```lua
        module = nn.SpatialSubtractiveNormalization(ninputplane, kernel)
        ```
        
        Applies a spatial subtraction operation on a series of 2D inputs using
        `kernel` for computing the weighted average in a neighborhood. The
        neighborhood is defined for a local spatial region that is the size as
        kernel and across all features. For a an input image, since there is
        only one feature, the region is only spatial. For an RGB image, the
        weighted average is taken over RGB channels and a spatial region.
        
        If the `kernel` is 1D, then it will be used for constructing and seperable
        2D kernel. The operations will be much more efficient in this case.
        
        The kernel is generally chosen as a gaussian when it is believed that
        the correlation of two pixel locations decrease with increasing
        distance. On the feature dimension, a uniform average is used since
        the weighting across features is not known.
        
        For this example we use an external package
        [image](http://www.github.com/clementfarabet/lua---image/)
        
        ```lua
        require 'image'
        require 'nn'
        lena = image.rgb2y(image.lena())
        ker = torch.ones(11)
        m=nn.SpatialSubtractiveNormalization(1,ker)
        processed = m:forward(lena)
        w1=image.display(lena)
        w2=image.display(processed)
        ```
        ![](image/lena.jpg)![](image/lenap.jpg)
        
        
        """

   'nn.SpatialBatchNormalization':
      'prefix': 'nnSpatialBatchNormalization'
      'body': """
        <a name="nn.SpatialBatchNormalization"></a>
        ## SpatialBatchNormalization ##
        
        `module` = `nn.SpatialBatchNormalization(N [,eps] [, momentum] [,affine])`
        where N = number of input feature maps
        eps is a small value added to the standard-deviation to avoid divide-by-zero. Defaults to 1e-5
        `affine` is a boolean. When set to false, the learnable affine transform is disabled.  Defaults to true
        
        Implements Batch Normalization as described in the paper:
        "Batch Normalization: Accelerating Deep Network Training
        by Reducing Internal Covariate Shift"
        by Sergey Ioffe, Christian Szegedy
        
        The operation implemented is:
        ```
        y =     ( x - mean(x) )
        -------------------- * gamma + beta
        standard-deviation(x)
        ```
        where the mean and standard-deviation are calculated per feature-map over the mini-batches and pixels
        and where gamma and beta are learnable parameter vectors of size N (where N = number of feature maps).
        The learning of gamma and beta is optional.
        
        In training time, this layer keeps a running estimate of it's computed mean and std.
        The running sum is kept with a default momentup of 0.1 (unless over-ridden)
        In test time, this running mean/std is used to normalize.
        
        
        
        The module only accepts 4D inputs.
        
        ```lua
        -- with learnable parameters
        model = nn.SpatialBatchNormalization(m)
        A = torch.randn(b, m, h, w)
        C = model:forward(A)  -- C will be of size `b x m x h x w`
        
        -- without learnable parameters
        model = nn.SpatialBatchNormalization(m, nil, nil, false)
        A = torch.randn(b, m, h, w)
        C = model:forward(A)  -- C will be of size `b x m x h x w`
        ```
        
        
        """

   'nn.VolumetricModules':
      'prefix': 'nnVolumetricModules'
      'body': """
        <a name="nn.VolumetricModules"></a>
        ## Volumetric Modules ##
        Excluding an optional batch dimension, volumetric layers expect a 4D Tensor as input. The
        first dimension is the number of features (e.g. `frameSize`), the second is sequential (e.g. `time`) and the
        last two dimensions are spatial (e.g. `height x width`). These are commonly used for processing videos (sequences of images).
        
        
        """

   'nn.VolumetricConvolution':
      'prefix': 'nnVolumetricConvolution'
      'body': """
        <a name="nn.VolumetricConvolution"></a>
        ### VolumetricConvolution ###
        
        ```lua
        module = nn.VolumetricConvolution(nInputPlane, nOutputPlane, kT, kW, kH [, dT, dW, dH])
        ```
        
        Applies a 3D convolution over an input image composed of several input planes. The `input` tensor in
        `forward(input)` is expected to be a 4D tensor (`nInputPlane x time x height x width`).
        
        The parameters are the following:
        * `nInputPlane`: The number of expected input planes in the image given into `forward()`.
        * `nOutputPlane`: The number of output planes the convolution layer will produce.
        * `kT`: The kernel size of the convolution in time
        * `kW`: The kernel width of the convolution
        * `kH`: The kernel height of the convolution
        * `dT`: The step of the convolution in the time dimension. Default is `1`.
        * `dW`: The step of the convolution in the width dimension. Default is `1`.
        * `dH`: The step of the convolution in the height dimension. Default is `1`.
        
        Note that depending of the size of your kernel, several (of the last)
        columns or rows of the input image might be lost. It is up to the user to
        add proper padding in images.
        
        If the input image is a 4D tensor `nInputPlane x time x height x width`, the output image size
        will be `nOutputPlane x otime x owidth x oheight` where
        ```lua
        otime   = (time  - kT)  / dT + 1
        owidth  = (width  - kW) / dW + 1
        oheight = (height - kH) / dH + 1 .
        ```
        
        The parameters of the convolution can be found in `self.weight` (Tensor of
        size `nOutputPlane x nInputPlane x kT x kH x kW`) and `self.bias` (Tensor of
        size `nOutputPlane`). The corresponding gradients can be found in
        `self.gradWeight` and `self.gradBias`.
        
        
        """

   'nn.VolumetricFullConvolution':
      'prefix': 'nnVolumetricFullConvolution'
      'body': """
        <a name="nn.VolumetricFullConvolution"></a>
        ### VolumetricFullConvolution ###
        
        ```lua
        module = nn.VolumetricFullConvolution(nInputPlane, nOutputPlane, kT, kW, kH, [dT], [dW], [dH], [padT], [padW], [padH])
        ```
        
        Applies a 3D full convolution over an input image composed of several input planes. The `input` tensor in
        `forward(input)` is expected to be a 4D or 5D tensor.
        
        The parameters are the following:
        * `nInputPlane`: The number of expected input planes in the image given into `forward()`.
        * `nOutputPlane`: The number of output planes the convolution layer will produce.
        * `kT`: The kernel depth of the convolution
        * `kW`: The kernel width of the convolution
        * `kH`: The kernel height of the convolution
        * `dT`: The step of the convolution in the depth dimension. Default is `1`.
        * `dW`: The step of the convolution in the width dimension. Default is `1`.
        * `dH`: The step of the convolution in the height dimension. Default is `1`.
        * `padT`: The additional zeros added per depth to the input planes. Default is `0`, a good number is `(kT-1)/2`.
        * `padW`: The additional zeros added per width to the input planes. Default is `0`, a good number is `(kW-1)/2`.
        * `padH`: The additional zeros added per height to the input planes. Default is `0`, a good number is `(kH-1)/2`.
        
        If the input image is a 3D tensor `nInputPlane x depth x height x width`, the output image size
        will be `nOutputPlane x odepth x oheight x owidth` where
        ```lua
        odepth  = (depth  - 1) * dT - 2*padT + kT
        owidth  = (width  - 1) * dW - 2*padW + kW
        oheight = (height - 1) * dH - 2*padH + kH
        ```
        
        
        """

   'nn.VolumetricMaxPooling':
      'prefix': 'nnVolumetricMaxPooling'
      'body': """
        <a name="nn.VolumetricMaxPooling"></a>
        ### VolumetricMaxPooling ###
        
        ```lua
        module = nn.VolumetricMaxPooling(kT, kW, kH [, dT, dW, dH])
        ```
        
        Applies 3D max-pooling operation in `kTxkWxkH` regions by step size
        `dTxdWxdH` steps. The number of output features is equal to the number of
        input planes / dT.
        
        
        """

   'nn.VolumetricAveragePooling':
      'prefix': 'nnVolumetricAveragePooling'
      'body': """
        <a name="nn.VolumetricAveragePooling"></a>
        ### VolumetricAveragePooling ###
        
        ```lua
        module = nn.VolumetricAveragePooling(kT, kW, kH [, dT, dW, dH])
        ```
        
        Applies 3D average-pooling operation in `kTxkWxkH` regions by step size
        `dTxdWxdH` steps. The number of output features is equal to the number of
        input planes / dT.
        
        """

   'nn.Criterions':
      'prefix': 'nnCriterions'
      'body': """
        <a name="nn.Criterions"></a>
        # Criterions #
        
        [`Criterions`](#nn.Criterion) are helpful to train a neural network. Given an input and a
        target, they compute a gradient according to a given loss function.
        
        * Classification criterions:
        * [`BCECriterion`](#nn.BCECriterion): binary cross-entropy for [`Sigmoid`](transfer.md#nn.Sigmoid) (two-class version of [`ClassNLLCriterion`](#nn.ClassNLLCriterion));
        * [`ClassNLLCriterion`](#nn.ClassNLLCriterion): negative log-likelihood for [`LogSoftMax`](transfer.md#nn.LogSoftMax) (multi-class);
        * [`CrossEntropyCriterion`](#nn.CrossEntropyCriterion): combines [`LogSoftMax`](transfer.md#nn.LogSoftMax) and [`ClassNLLCriterion`](#nn.ClassNLLCriterion);
        * [`MarginCriterion`](#nn.MarginCriterion): two class margin-based loss;
        * [`MultiMarginCriterion`](#nn.MultiMarginCriterion): multi-class margin-based loss;
        * [`MultiLabelMarginCriterion`](#nn.MultiLabelMarginCriterion): multi-class multi-classification margin-based loss;
        * Regression criterions:
        * [`AbsCriterion`](#nn.AbsCriterion): measures the mean absolute value of the element-wise difference between input;
        * [`SmoothL1Criterion`](#nn.SmoothL1Criterion): a smooth version of the AbsCriterion;
        * [`MSECriterion`](#nn.MSECriterion): mean square error (a classic);
        * [`DistKLDivCriterion`](#nn.DistKLDivCriterion): KullbackLeibler divergence (for fitting continuous probability distributions);
        * Embedding criterions (measuring whether two inputs are similar or dissimilar):
        * [`HingeEmbeddingCriterion`](#nn.HingeEmbeddingCriterion): takes a distance as input;
        * [`L1HingeEmbeddingCriterion`](#nn.L1HingeEmbeddingCriterion): L1 distance between two inputs;
        * [`CosineEmbeddingCriterion`](#nn.CosineEmbeddingCriterion): cosine distance between two inputs;
        * Miscelaneus criterions:
        * [`MultiCriterion`](#nn.MultiCriterion) : a weighted sum of other criterions each applied to the same input and target;
        * [`ParallelCriterion`](#nn.ParallelCriterion) : a weighted sum of other criterions each applied to a different input and target;
        * [`MarginRankingCriterion`](#nn.MarginRankingCriterion): ranks two inputs;
        
        
        """

   'nn.Criterion':
      'prefix': 'nnCriterion'
      'body': """
        <a name="nn.Criterion"></a>
        ## Criterion ##
        
        This is an abstract class which declares methods defined in all criterions.
        This class is [serializable](https://github.com/torch/torch7/blob/master/doc/file.md#serialization-methods).
        
        
        """

   'nn.Criterion.forward':
      'prefix': 'nnCriterionforward'
      'body': """
        <a name="nn.Criterion.forward"></a>
        ### [output] forward(input, target) ###
        
        Given an `input` and a `target`, compute the loss function associated to the criterion and return the result.
        In general `input` and `target` are [`Tensor`s](https://github.com/torch/torch7/blob/master/doc/tensor.md), but some specific criterions might require some other type of object.
        
        The `output` returned should be a scalar in general.
        
        The state variable [`self.output`](#nn.Criterion.output) should be updated after a call to `forward()`.
        
        
        
        """

   'nn.Criterion.backward':
      'prefix': 'nnCriterionbackward'
      'body': """
        <a name="nn.Criterion.backward"></a>
        ### [gradInput] backward(input, target) ###
        
        Given an `input` and a `target`, compute the gradients of the loss function associated to the criterion and return the result.
        In general `input`, `target` and `gradInput` are [`Tensor`s](..:torch:tensor), but some specific criterions might require some other type of object.
        
        The state variable [`self.gradInput`](#nn.Criterion.gradInput) should be updated after a call to `backward()`.
        
        
        
        """

   'nn.Criterion.output':
      'prefix': 'nnCriterionoutput'
      'body': """
        <a name="nn.Criterion.output"></a>
        ### State variable: output ###
        
        State variable which contains the result of the last [`forward(input, target)`](#nn.Criterion.forward) call.
        
        
        
        """

   'nn.Criterion.gradInput':
      'prefix': 'nnCriteriongradInput'
      'body': """
        <a name="nn.Criterion.gradInput"></a>
        ### State variable: gradInput ###
        
        State variable which contains the result of the last [`backward(input, target)`](#nn.Criterion.backward) call.
        
        
        
        """

   'nn.AbsCriterion':
      'prefix': 'nnAbsCriterion'
      'body': """
        <a name="nn.AbsCriterion"></a>
        ## AbsCriterion ##
        
        ```lua
        criterion = nn.AbsCriterion()
        ```
        
        Creates a criterion that measures the mean absolute value of the element-wise difference between input `x` and target `y`:
        
        ```lua
        loss(x, y)  = 1/n \sum |x_i - y_i|
        ```
        
        If `x` and `y` are `d`-dimensional `Tensor`s with a total of `n` elements, the sum operation still operates over all the elements, and divides by `n`.
        
        The division by `n` can be avoided if one sets the internal variable `sizeAverage` to `false`:
        
        ```lua
        criterion = nn.AbsCriterion()
        criterion.sizeAverage = false
        ```
        
        
        
        """

   'nn.ClassNLLCriterion':
      'prefix': 'nnClassNLLCriterion'
      'body': """
        <a name="nn.ClassNLLCriterion"></a>
        ## ClassNLLCriterion ##
        
        ```lua
        criterion = nn.ClassNLLCriterion([weights])
        ```
        
        The negative log likelihood criterion. It is useful to train a classication problem with `n` classes.
        If provided, the optional argument `weights` should be a 1D `Tensor` assigning weight to each of the classes.
        This is particularly useful when you have an unbalanced training set.
        
        The `input` given through a `forward()` is expected to contain _log-probabilities_ of each class: `input` has to be a 1D `Tensor` of size `n`.
        Obtaining log-probabilities in a neural network is easily achieved by adding a [`LogSoftMax`](transfer.md#nn.LogSoftMax) layer in the last layer of your neural network.
        You may use [`CrossEntropyCriterion`](#nn.CrossEntropyCriterion) instead, if you prefer not to add an extra layer to your network.
        This criterion expects a class index (1 to the number of class) as `target` when calling [`forward(input, target`)](#nn.CriterionForward) and [`backward(input, target)`](#nn.CriterionBackward).
        
        The loss can be described as:
        
        ```lua
        loss(x, class) = -x[class]
        ```
        
        or in the case of the `weights` argument it is specified as follows:
        
        ```lua
        loss(x, class) = -weights[class] * x[class]
        ```
        
        The following is a code fragment showing how to make a gradient step given an input `x`, a desired output `y` (an integer `1` to `n`, in this case `n = 2` classes), a network `mlp` and a learning rate `learningRate`:
        
        ```lua
        function gradUpdate(mlp, x, y, learningRate)
        local criterion = nn.ClassNLLCriterion()
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        mlp:zeroGradParameters()
        local t = criterion:backward(pred, y)
        mlp:backward(x, t)
        mlp:updateParameters(learningRate)
        end
        ```
        
        
        
        """

   'nn.CrossEntropyCriterion':
      'prefix': 'nnCrossEntropyCriterion'
      'body': """
        <a name="nn.CrossEntropyCriterion"></a>
        ## CrossEntropyCriterion ##
        
        ```lua
        criterion = nn.CrossEntropyCriterion([weights])
        ```
        
        This criterion combines [`LogSoftMax`](#nn.LogSoftMax) and [`ClassNLLCriterion`](#nn.ClassNLLCriterion) in one single class.
        
        It is useful to train a classication problem with `n` classes.
        If provided, the optional argument `weights` should be a 1D `Tensor` assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.
        
        The `input` given through a `forward()` is expected to contain scores for each class: `input` has to be a 1D `Tensor` of size `n`.
        This criterion expect a class index (1 to the number of class) as `target` when calling [`forward(input, target)`](#nn.CriterionForward) and [`backward(input, target)`](#nn.CriterionBackward).
        
        The loss can be described as:
        
        ```lua
        loss(x, class) = -log(exp(x[class]) / (\sum_j exp(x[j])))
        = -x[class] + log(\sum_j exp(x[j]))
        ```
        
        or in the case of the `weights` argument being specified:
        
        ```lua
        loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))
        ```
        
        
        
        """

   'nn.DistKLDivCriterion':
      'prefix': 'nnDistKLDivCriterion'
      'body': """
        <a name="nn.DistKLDivCriterion"></a>
        ## DistKLDivCriterion ##
        
        ```lua
        criterion = nn.DistKLDivCriterion()
        ```
        
        The [KullbackLeibler divergence](http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) criterion.
        KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.
        As with ClassNLLCriterion, the `input` given through a `forward()` is expected to contain _log-probabilities_, however unlike ClassNLLCriterion, `input` is not restricted to a 1D or 2D vector (as the criterion is applied element-wise).
        
        This criterion expect a `target` `Tensor` of the same size as the `input` `Tensor` when calling [`forward(input, target)`](#nn.CriterionForward) and [`backward(input, target)`](#nn.CriterionBackward).
        
        The loss can be described as:
        
        ```lua
        loss(x, target) = \sum(target_i * (log(target_i) - x_i))
        ```
        
        
        
        """

   'nn.BCECriterion':
      'prefix': 'nnBCECriterion'
      'body': """
        <a name="nn.BCECriterion"></a>
        ## BCECriterion
        
        ```lua
        criterion = nn.BCECriterion([weights])
        ```
        
        Creates a criterion that measures the Binary Cross Entropy between the target and the output:
        
        ```lua
        loss(t, o) = - sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
        ```
        
        or in the case of the weights argument being specified:
        
        ```lua
        loss(t, o) = - sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
        ```
        
        This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets `t[i]` should be numbers between 0 and 1, for instance, the output of an [`nn.Sigmoid`](transfer.md#nn.Sigmoid) layer.
        
        
        
        """

   'nn.MarginCriterion':
      'prefix': 'nnMarginCriterion'
      'body': """
        <a name="nn.MarginCriterion"></a>
        ## MarginCriterion ##
        
        ```lua
        criterion = nn.MarginCriterion([margin])
        ```
        
        Creates a criterion that optimizes a two-class classification hinge loss (margin-based loss) between input `x` (a `Tensor` of dimension `1`) and output `y` (which is a tensor containing either `1`s or `-1`s).
        `margin`, if unspecified, is by default `1`.
        
        ```lua
        loss(x, y) = sum_i (max(0, margin - y[i]*x[i])) / x:nElement()
        ```
        
        The normalization by the number of elements in the input can be disabled by
        setting `self.sizeAverage` to `false`.
        
        ### Example
        
        ```lua
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        
        mlp = nn.Sequential()
        mlp:add(nn.Linear(5, 1))
        
        x1 = torch.rand(5)
        x1_target = torch.Tensor{1}
        x2 = torch.rand(5)
        x2_target = torch.Tensor{-1}
        criterion=nn.MarginCriterion(1)
        
        for i = 1, 1000 do
        gradUpdate(mlp, x1, x1_target, criterion, 0.01)
        gradUpdate(mlp, x2, x2_target, criterion, 0.01)
        end
        
        print(mlp:forward(x1))
        print(mlp:forward(x2))
        
        print(criterion:forward(mlp:forward(x1), x1_target))
        print(criterion:forward(mlp:forward(x2), x2_target))
        ```
        
        gives the output:
        
        ```lua
        1.0043
        [torch.Tensor of dimension 1]
        
        
        -1.0061
        [torch.Tensor of dimension 1]
        
        0
        0
        ```
        
        i.e. the mlp successfully separates the two data points such that they both have a `margin` of `1`, and hence a loss of `0`.
        
        
        
        """

   'nn.MultiMarginCriterion':
      'prefix': 'nnMultiMarginCriterion'
      'body': """
        <a name="nn.MultiMarginCriterion"></a>
        ## MultiMarginCriterion ##
        
        ```lua
        criterion = nn.MultiMarginCriterion(p)
        ```
        
        Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input `x`  (a `Tensor` of dimension 1) and output `y` (which is a target class index, `1` <= `y` <= `x:size(1)`):
        
        ```lua
        loss(x, y) = sum_i(max(0, 1 - (x[y] - x[i]))^p) / x:size(1)
        ```
        
        where `i == 1` to `x:size(1)` and `i ~= y`.
        Note that this criterion also works with 2D inputs and 1D targets.
        
        This criterion is especially useful for classification when used in conjunction with a module ending in the following output layer:
        
        ```lua
        mlp = nn.Sequential()
        mlp:add(nn.Euclidean(n, m)) -- outputs a vector of distances
        mlp:add(nn.MulConstant(-1)) -- distance to similarity
        ```
        
        
        
        """

   'nn.MultiLabelMarginCriterion':
      'prefix': 'nnMultiLabelMarginCriterion'
      'body': """
        <a name="nn.MultiLabelMarginCriterion"></a>
        ## MultiLabelMarginCriterion ##
        
        ```lua
        criterion = nn.MultiLabelMarginCriterion()
        ```
        
        Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input `x`  (a 1D `Tensor`) and output `y` (which is a 1D `Tensor` of target class indices):
        
        ```lua
        loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x:size(1)
        ```
        
        where `i == 1` to `x:size(1)`, `j == 1` to `y:size(1)`, `y[j] ~= 0`, and `i ~= y[j]` for all `i` and `j`.
        Note that this criterion also works with 2D inputs and targets.
        
        `y` and `x` must have the same size.
        The criterion only considers the first non zero `y[j]` targets.
        This allows for different samples to have variable amounts of target classes:
        
        ```lua
        criterion = nn.MultiLabelMarginCriterion()
        input = torch.randn(2, 4)
        target = torch.Tensor{{1, 3, 0, 0}, {4, 0, 0, 0}} -- zero-values are ignored
        criterion:forward(input, target)
        ```
        
        
        
        """

   'nn.MSECriterion':
      'prefix': 'nnMSECriterion'
      'body': """
        <a name="nn.MSECriterion"></a>
        ## MSECriterion ##
        
        ```lua
        criterion = nn.MSECriterion()
        ```
        
        Creates a criterion that measures the mean squared error between `n` elements in the input `x` and output `y`:
        
        ```lua
        loss(x, y) = 1/n \sum |x_i - y_i|^2 .
        ```
        
        If `x` and `y` are `d`-dimensional `Tensor`s with a total of `n` elements, the sum operation still operates over all the elements, and divides by `n`.
        The two `Tensor`s must have the same number of elements (but their sizes might be different).
        
        The division by `n` can be avoided if one sets the internal variable `sizeAverage` to `false`:
        
        ```lua
        criterion = nn.MSECriterion()
        criterion.sizeAverage = false
        ```
        
        
        
        """

   'nn.MultiCriterion':
      'prefix': 'nnMultiCriterion'
      'body': """
        <a name="nn.MultiCriterion"></a>
        ## MultiCriterion ##
        
        ```lua
        criterion = nn.MultiCriterion()
        ```
        
        This returns a Criterion which is a weighted sum of other Criterion.
        Criterions are added using the method:
        
        ```lua
        criterion:add(singleCriterion [, weight])
        ```
        
        where `weight` is a scalar (default 1). Each criterion is applied to the same `input` and `target`.
        
        Example :
        
        ```lua
        input = torch.rand(2,10)
        target = torch.IntTensor{1,8}
        nll = nn.ClassNLLCriterion()
        nll2 = nn.CrossEntropyCriterion()
        mc = nn.MultiCriterion():add(nll, 0.5):add(nll2)
        output = mc:forward(input, target)
        ```
        
        
        """

   'nn.ParallelCriterion':
      'prefix': 'nnParallelCriterion'
      'body': """
        <a name="nn.ParallelCriterion"></a>
        ## ParallelCriterion ##
        
        ```lua
        criterion = nn.ParallelCriterion([repeatTarget])
        ```
        
        This returns a Criterion which is a weighted sum of other Criterion. 
        Criterions are added using the method:
        
        ```lua
        criterion:add(singleCriterion [, weight])
        ```
        
        where `weight` is a scalar (default 1). The criterion expects an `input` and `target` table. 
        Each criterion is applied to the commensurate `input` and `target` element in the tables.
        However, if `repeatTarget=true`, the `target` is repeatedly presented to each criterion (with a different `input`).
        
        Example :
        
        ```lua
        input = {torch.rand(2,10), torch.randn(2,10)}
        target = {torch.IntTensor{1,8}, torch.randn(2,10)}
        nll = nn.ClassNLLCriterion()
        mse = nn.MSECriterion()
        pc = nn.ParallelCriterion():add(nll, 0.5):add(mse)
        output = pc:forward(input, target)
        ```
        
        
        
        """

   'nn.SmoothL1Criterion':
      'prefix': 'nnSmoothL1Criterion'
      'body': """
        <a name="nn.SmoothL1Criterion"></a>
        ## SmoothL1Criterion ##
        
        ```lua
        criterion = nn.SmoothL1Criterion()
        ```
        
        Creates a criterion that can be thought of as a smooth version of the [`AbsCriterion`](#nn.AbsCriterion). It uses a squared term if the absolute element-wise error falls below 1. It is less sensitive to outliers than the [`MSECriterion`](#nn.MSECriterion) and in some cases prevents exploding gradients (e.g. see "Fast R-CNN" paper by Ross Girshick).
        
        ```lua
         0.5 * (x_i - y_i)^2, if |x_i - y_i| < 1
        loss(x, y) = 1/n \sum 
         |x_i - y_i| - 0.5,   otherwise
        ```
        
        If `x` and `y` are `d`-dimensional `Tensor`s with a total of `n` elements, the sum operation still operates over all the elements, and divides by `n`.
        
        The division by `n` can be avoided if one sets the internal variable `sizeAverage` to `false`:
        
        ```lua
        criterion = nn.SmoothL1Criterion()
        criterion.sizeAverage = false
        ```
        
        
        
        """

   'nn.HingeEmbeddingCriterion':
      'prefix': 'nnHingeEmbeddingCriterion'
      'body': """
        <a name="nn.HingeEmbeddingCriterion"></a>
        ## HingeEmbeddingCriterion ##
        
        ```lua
        criterion = nn.HingeEmbeddingCriterion([margin])
        ```
        
        Creates a criterion that measures the loss given an input `x` which is a 1-dimensional vector and a label `y` (`1` or `-1`).
        This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.
        
        ```lua
         x_i,                  if y_i ==  1
        loss(x, y) = 1/n 
         max(0, margin - x_i), if y_i == -1
        ```
        
        If `x` and `y` are `n`-dimensional `Tensor`s, the sum operation still operates over all the elements, and divides by `n` (this can be avoided if one sets the internal variable `sizeAverage` to `false`). The `margin` has a default value of `1`, or can be set in the constructor.
        
        ### Example
        
        ```lua
        -- imagine we have one network we are interested in, it is called "p1_mlp"
        p1_mlp = nn.Sequential(); p1_mlp:add(nn.Linear(5, 2))
        
        -- But we want to push examples towards or away from each other so we make another copy
        -- of it called p2_mlp; this *shares* the same weights via the set command, but has its
        -- own set of temporary gradient storage that's why we create it again (so that the gradients
        -- of the pair don't wipe each other)
        p2_mlp = nn.Sequential(); p2_mlp:add(nn.Linear(5, 2))
        p2_mlp:get(1).weight:set(p1_mlp:get(1).weight)
        p2_mlp:get(1).bias:set(p1_mlp:get(1).bias)
        
        -- we make a parallel table that takes a pair of examples as input.
        -- They both go through the same (cloned) mlp
        prl = nn.ParallelTable()
        prl:add(p1_mlp)
        prl:add(p2_mlp)
        
        -- now we define our top level network that takes this parallel table
        -- and computes the pairwise distance betweem the pair of outputs
        mlp = nn.Sequential()
        mlp:add(prl)
        mlp:add(nn.PairwiseDistance(1))
        
        -- and a criterion for pushing together or pulling apart pairs
        crit = nn.HingeEmbeddingCriterion(1)
        
        -- lets make two example vectors
        x = torch.rand(5)
        y = torch.rand(5)
        
        
        -- Use a typical generic gradient update function
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        
        -- push the pair x and y together, notice how then the distance between them given
        -- by print(mlp:forward({x, y})[1]) gets smaller
        for i = 1, 10 do
        gradUpdate(mlp, {x, y}, 1, crit, 0.01)
        print(mlp:forward({x, y})[1])
        end
        
        -- pull apart the pair x and y, notice how then the distance between them given
        -- by print(mlp:forward({x, y})[1]) gets larger
        
        for i = 1, 10 do
        gradUpdate(mlp, {x, y}, -1, crit, 0.01)
        print(mlp:forward({x, y})[1])
        end
        ```
        
        
        
        """

   'nn.L1HingeEmbeddingCriterion':
      'prefix': 'nnL1HingeEmbeddingCriterion'
      'body': """
        <a name="nn.L1HingeEmbeddingCriterion"></a>
        ## L1HingeEmbeddingCriterion ##
        
        ```lua
        criterion = nn.L1HingeEmbeddingCriterion([margin])
        ```
        
        Creates a criterion that measures the loss given  an input `x` = `{x1, x2}`, a table of two `Tensor`s, and a label `y` (`1` or `-1`): this is used for measuring whether two inputs are similar or dissimilar, using the L1 distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.
        
        ```lua
         ||x1 - x2||_1,                  if y ==  1
        loss(x, y) = 
         max(0, margin - ||x1 - x2||_1), if y == -1
        ```
        
        The `margin` has a default value of `1`, or can be set in the constructor.
        
        
        """

   'nn.CosineEmbeddingCriterion':
      'prefix': 'nnCosineEmbeddingCriterion'
      'body': """
        <a name="nn.CosineEmbeddingCriterion"></a>
        ## CosineEmbeddingCriterion ##
        
        ```lua
        criterion = nn.CosineEmbeddingCriterion([margin])
        ```
        
        Creates a criterion that measures the loss given  an input `x` = `{x1, x2}`, a table of two `Tensor`s, and a `Tensor` label `y`  with values 1 or -1.
        This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.
        
        `margin` should be a number from `-1` to `1`, `0` to `0.5` is suggested.
        `Forward` and `Backward` have to be used alternately. If `margin` is missing, the default value is `0`.
        
        The loss function for each sample is:
        
        ```lua
         1 - cos(x1, x2),              if y ==  1
        loss(x, y) = 
         max(0, cos(x1, x2) - margin), if y == -1
        ```
        
        For batched inputs, if the internal variable `sizeAverage` is equal to `true`, the loss function averages the loss over the batch samples; if `sizeAverage` is `false`, then the loss function sums over the batch samples. By default, `sizeAverage` equals to `true`.
        
        
        """

   'nn.MarginRankingCriterion':
      'prefix': 'nnMarginRankingCriterion'
      'body': """
        <a name="nn.MarginRankingCriterion"></a>
        ## MarginRankingCriterion ##
        
        ```lua
        criterion = nn.MarginRankingCriterion(margin)
        ```
        
        Creates a criterion that measures the loss given  an input `x` = `{x1, x2}`, a table of two `Tensor`s of size 1 (they contain only scalars), and a label `y` (`1` or `-1`).
        
        If `y == 1` then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for `y == -1`.
        
        The loss function is:
        
        ```lua
        loss(x, y) = max(0, -y * (x[1] - x[2]) + margin)
        ```
        
        ### Example
        
        ```lua
        p1_mlp = nn.Linear(5, 2)
        p2_mlp = p1_mlp:clone('weight', 'bias')
        
        prl = nn.ParallelTable()
        prl:add(p1_mlp)
        prl:add(p2_mlp)
        
        mlp1 = nn.Sequential()
        mlp1:add(prl)
        mlp1:add(nn.DotProduct())
        
        mlp2 = mlp1:clone('weight', 'bias')
        
        mlpa = nn.Sequential()
        prla = nn.ParallelTable()
        prla:add(mlp1)
        prla:add(mlp2)
        mlpa:add(prla)
        
        crit = nn.MarginRankingCriterion(0.1)
        
        x=torch.randn(5)
        y=torch.randn(5)
        z=torch.randn(5)
        
        -- Use a typical generic gradient update function
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        
        for i = 1, 100 do
        gradUpdate(mlpa, {{x, y}, {x, z}}, 1, crit, 0.01)
        if true then
        o1 = mlp1:forward{x, y}[1]
        o2 = mlp2:forward{x, z}[1]
        o = crit:forward(mlpa:forward{{x, y}, {x, z}}, 1)
        print(o1, o2, o)
        end
        end
        
        print "--"
        
        for i = 1, 100 do
        gradUpdate(mlpa, {{x, y}, {x, z}}, -1, crit, 0.01)
        if true then
        o1 = mlp1:forward{x, y}[1]
        o2 = mlp2:forward{x, z}[1]
        o = crit:forward(mlpa:forward{{x, y}, {x, z}}, -1)
        print(o1, o2, o)
        end
        end
        ```
        
        """

   'torch.DiskFile.dok':
      'prefix': 'torchDiskFiledok'
      'body': """
        <a name="torch.DiskFile.dok"></a>
        # DiskFile #
        
        Parent classes: [File](file.md)
        
        A `DiskFile` is a particular `File` which is able to perform basic read/write operations
        on a file stored on disk. It implements all methods described in [File](file.md), and
        some additional methods relative to _endian_ encoding.
        
        By default, a `DiskFile` is in [ASCII](file.md#torch.File.ascii) mode. If changed to
        the [binary](file.md#torch.File.binary) mode, the default endian encoding is the native
        computer one.
        
        The file might be open in read, write, or read-write mode, depending on the parameter
        `mode` (which can take the value `"r"`, `"w"` or `"rw"` respectively) 
        given to the [torch.DiskFile(fileName, mode)](#torch.DiskFile).
        
        
        """

   'torch.DiskFile':
      'prefix': 'torchDiskFile'
      'body': """
        <a name="torch.DiskFile"></a>
        ### torch.DiskFile(fileName, [mode], [quiet]) ###
        
        _Constructor_ which opens `fileName` on disk, using the given `mode`. Valid `mode` are
        `"r"` (read), `"w"` (write) or `"rw"` (read-write). Default is read mode.
        
        If read-write mode, the file _will be created_ if it does not exists. If it
        exists, it will be positioned at the beginning of the file after opening.
        
        If (and only if) `quiet` is `true`, no error will be raised in case of
        problem opening the file: instead `nil` will be returned.
        
        The file is opened in [ASCII](file.md#torch.File.ascii) mode by default.
        
        
        """

   'torch.DiskFile.bigEndianEncoding':
      'prefix': 'torchDiskFilebigEndianEncoding'
      'body': """
        <a name="torch.DiskFile.bigEndianEncoding"></a>
        ### bigEndianEncoding() ###
        
        In [binary](file.md#torch.File.binary) mode, force encoding in _big endian_. 
        (_big end first_: decreasing numeric significance with increasing memory
        addresses)
        
        
        """

   'torch.DiskFile.isBigEndianCPU':
      'prefix': 'torchDiskFileisBigEndianCPU'
      'body': """
        <a name="torch.DiskFile.isBigEndianCPU"></a>
        ### [boolean] isBigEndianCPU() ###
        
        Returns `true` if, and only if, the computer CPU operates in _big endian_.
        _Big end first_: decreasing numeric significance with increasing
        memory addresses.
        
        
        """

   'torch.DiskFile.isLittleEndianCPU':
      'prefix': 'torchDiskFileisLittleEndianCPU'
      'body': """
        <a name="torch.DiskFile.isLittleEndianCPU"></a>
        ### [boolean] isLittleEndianCPU() ###
        
        Returns `true` if, and only if, the computer CPU operates in _little endian_.
        _Little end first_: increasing numeric significance with increasing
        memory addresses.
        
        
        """

   'torch.DiskFile.littleEndianEncoding':
      'prefix': 'torchDiskFilelittleEndianEncoding'
      'body': """
        <a name="torch.DiskFile.littleEndianEncoding"></a>
        ### littleEndianEncoding() ###
        
        In [binary](file.md#torch.File.binary) mode, force encoding in _little endian_.
        (_little end first_: increasing numeric significance with increasing memory
        addresses)
        
        
        """

   'torch.DiskFile.nativeEndianEncoding':
      'prefix': 'torchDiskFilenativeEndianEncoding'
      'body': """
        <a name="torch.DiskFile.nativeEndianEncoding"></a>
        ### nativeEndianEncoding() ###
        
        In [binary](file.md#torch.File.binary) mode, force encoding in _native endian_.
        
        
        """

   'torch.DiskFile.longSize':
      'prefix': 'torchDiskFilelongSize'
      'body': """
        <a name="torch.DiskFile.longSize"/></a>
        ### longSize([size]) ###
        
        Longs will be written and read from the file as `size` bytes long, which
        can be 0, 4 or 8. 0 means system default.
        
        """

   'torch.File.dok':
      'prefix': 'torchFiledok'
      'body': """
        <a name="torch.File.dok"></a>
        # File #
        
        This is an _abstract_ class. It defines most methods implemented by its
        child classes, like [DiskFile](diskfile.md),
        [MemoryFile](memoryfile.md) and [PipeFile](pipefile.md).
        
        Methods defined here are intended for basic read/write functionalities.
        Read/write methods might write in [ASCII](#torch.File.ascii) mode or
        [binary](#torch.File.binary) mode.
        
        In [ASCII](#torch.File.ascii) mode, numbers are converted in human readable
        format (characters). Booleans are converted into `0` (false) or `1` (true).
        In [binary](#torch.File.binary) mode, numbers and boolean are directly encoded
        as represented in a register of the computer. While not being human
        readable and less portable, the binary mode is obviously faster.
        
        In [ASCII](#torch.File.ascii) mode, if the default option
        [autoSpacing()](#torch.File.autoSpacing) is chosen, a space will be generated
        after each written number or boolean. A carriage return will also be added
        after each call to a write method. With this option, the spaces are
        supposed to exist while reading. This option can be deactivated with
        [noAutoSpacing()](#torch.File.noAutoSpacing).
        
        A `Lua` error might or might not be generated in case of read/write error
        or problem in the file. This depends on the choice made between
        [quiet()](#torch.File.quiet) and [pedantic()](#torch.File.pedantic) options. It
        is possible to query if an error occurred in the last operation by calling
        [hasError()](#torch.File.hasError).
        
        
        """

   'torch.File.read':
      'prefix': 'torchFileread'
      'body': """
        <a name="torch.File.read"></a>
        ## Read methods ##
        
        """

   'torch.File.readByte':
      'prefix': 'torchFilereadByte'
      'body': """
        <a name="torch.File.readByte"></a>
        
        """

   'torch.File.readBool':
      'prefix': 'torchFilereadBool'
      'body': """
        <a name="torch.File.readBool"></a>
        
        """

   'torch.File.readShort':
      'prefix': 'torchFilereadShort'
      'body': """
        <a name="torch.File.readShort"></a>
        
        """

   'torch.File.readChar':
      'prefix': 'torchFilereadChar'
      'body': """
        <a name="torch.File.readChar"></a>
        
        """

   'torch.File.readLong':
      'prefix': 'torchFilereadLong'
      'body': """
        <a name="torch.File.readLong"></a>
        
        """

   'torch.File.readInt':
      'prefix': 'torchFilereadInt'
      'body': """
        <a name="torch.File.readInt"></a>
        
        """

   'torch.File.readDouble':
      'prefix': 'torchFilereadDouble'
      'body': """
        <a name="torch.File.readDouble"></a>
        
        """

   'torch.File.readFloat':
      'prefix': 'torchFilereadFloat'
      'body': """
        <a name="torch.File.readFloat"></a>
        
        They are three types of reading methods:
        
        - `[number] readTYPE()`
        - `[TYPEStorage] readTYPE(n)`
        - `[number] readTYPE(TYPEStorage)`
        
        where `TYPE` can be either `Byte`, `Char`, `Short`, `Int`, `Long`, `Float` or `Double`.
        
        A convenience method also exist for boolean types: `[boolean] readBool()`. It reads
        a value on the file with `readInt()` and returns `true` if and only if this value is `1`. It is not possible
        to read storages of booleans.
        
        All these methods depends on the encoding choice: [ASCII](#torch.File.ascii)
        or [binary](#torch.File.binary) mode.  In [ASCII](#torch.File.ascii) mode, the
        option [autoSpacing()](#torch.File.autoSpacing) and
        [noAutoSpacing()](#torch.File.noAutoSpacing) have also an effect on these
        methods.
        
        If no parameter is given, one element is returned. This element is
        converted to a `Lua` number when reading.
        
        If `n` is given, `n` values of the specified type are read
        and returned in a new [Storage](storage.md) of that particular type.
        The storage size corresponds to the number of elements actually read.
        
        If a `Storage` is given, the method will attempt to read a number of elements
        equals to the size of the given storage, and fill up the storage with these elements.
        The number of elements actually read is returned.
        
        In case of read error, these methods will call the `Lua` error function using the default
        [pedantic](#torch.File.pedantic) option, or stay quiet with the [quiet](#torch.File.quiet)
        option. In the latter case, one can check if an error occurred with
        [hasError()](#torch.File.hasError).
        
        
        """

   'torch.File.write':
      'prefix': 'torchFilewrite'
      'body': """
        <a name="torch.File.write"></a>
        ## Write methods ##
        
        """

   'torch.File.writeByte':
      'prefix': 'torchFilewriteByte'
      'body': """
        <a name="torch.File.writeByte"></a>
        
        """

   'torch.File.writeBool':
      'prefix': 'torchFilewriteBool'
      'body': """
        <a name="torch.File.writeBool"></a>
        
        """

   'torch.File.writeShort':
      'prefix': 'torchFilewriteShort'
      'body': """
        <a name="torch.File.writeShort"></a>
        
        """

   'torch.File.writeChar':
      'prefix': 'torchFilewriteChar'
      'body': """
        <a name="torch.File.writeChar"></a>
        
        """

   'torch.File.writeLong':
      'prefix': 'torchFilewriteLong'
      'body': """
        <a name="torch.File.writeLong"></a>
        
        """

   'torch.File.writeInt':
      'prefix': 'torchFilewriteInt'
      'body': """
        <a name="torch.File.writeInt"></a>
        
        """

   'torch.File.writeDouble':
      'prefix': 'torchFilewriteDouble'
      'body': """
        <a name="torch.File.writeDouble"></a>
        
        """

   'torch.File.writeFloat':
      'prefix': 'torchFilewriteFloat'
      'body': """
        <a name="torch.File.writeFloat"></a>
        
        They are two types of writing methods:
        
        - `[number] writeTYPE(number)`
        - `[number] writeTYPE(TYPEStorage)`
        
        where `TYPE` can be either `Byte`, `Char`, `Short`, `Int`, `Long`, `Float` or `Double`.
        
        A convenience method also exist for boolean types: `writeBool(value)`. If `value` is `nil` or
        not `true` a it is equivalent to a `writeInt(0)` call, else to `writeInt(1)`. It is not possible
        to write storages of booleans.
        
        All these methods depends on the encoding choice: [ASCII](#torch.File.ascii)
        or [binary](#torch.File.ascii) mode.  In [ASCII](#torch.File.ascii) mode, the
        option [autoSpacing()](#torch.File.autoSpacing) and
        [noAutoSpacing()](#torch.File.noAutoSpacing) have also an effect on these
        methods.
        
        If one `Lua` number is given, this number is converted according to the
        name of the method when writing (e.g. `writeInt(3.14)` will write `3`).
        
        If a `Storage` is given, the method will attempt to write all the elements contained
        in the storage.
        
        These methods return the number of elements actually written.
        
        In case of write error, these methods will call the `Lua` error function using the default
        [pedantic](#torch.File.pedantic) option, or stay quiet with the [quiet](#torch.File.quiet)
        option. In the latter case, one can check if an error occurred with
        [hasError()](#torch.File.hasError).
        
        
        """

   'torch.File.serialization':
      'prefix': 'torchFileserialization'
      'body': """
        <a name="torch.File.serialization"></a>
        ## Serialization methods ##
        
        These methods allow the user to save any serializable objects on disk and
        reload it later in its original state. In other words, it can perform a
        _deep_ copy of an object into a given `File`.
        
        Serializable objects are `Torch` objects having a `read()` and
        `write()` method. `Lua` objects such as `table`, `number` or
        `string` or _pure Lua_ functions are also serializable.
        
        If the object to save contains several other objects (let say it is a tree
        of objects), then objects appearing several times in this tree will be
        _saved only once_. This saves disk space, speedup loading/saving and
        respect the dependencies between objects.
        
        Interestingly, if the `File` is a [MemoryFile](memoryfile.md), it allows
        the user to easily make a _clone_ of any serializable object:
        ```lua
        file = torch.MemoryFile() -- creates a file in memory
        file:writeObject(object) -- writes the object into file
        file:seek(1) -- comes back at the beginning of the file
        objectClone = file:readObject() -- gets a clone of object
        ```
        
        
        """

   'torch.File.readObject':
      'prefix': 'torchFilereadObject'
      'body': """
        <a name="torch.File.readObject"></a>
        ### readObject() ###
        
        Returns the next [serializable](#torch.File.serialization) object saved beforehand
        in the file with [writeObject()](#torch.File.writeObject).
        
        Note that objects which were [written](#torch.File.writeObject) with the same
        reference have still the same reference after loading.
        
        Example:
        ```lua
        -- creates an array which contains twice the same tensor
        array = {}
        x = torch.Tensor(1)
        table.insert(array, x)
        table.insert(array, x)
        
        -- array[1] and array[2] refer to the same address
        -- x[1] == array[1][1] == array[2][1] == 3.14
        array[1][1] = 3.14
        
        -- write the array on disk
        file = torch.DiskFile('foo.asc', 'w')
        file:writeObject(array)
        file:close() -- make sure the data is written
        
        -- reload the array
        file = torch.DiskFile('foo.asc', 'r')
        arrayNew = file:readObject()
        
        -- arrayNew[1] and arrayNew[2] refer to the same address!
        -- arrayNew[1][1] == arrayNew[2][1] == 3.14
        -- so if we do now:
        arrayNew[1][1] = 2.72
        -- arrayNew[1][1] == arrayNew[2][1] == 2.72 !
        ```
        
        
        """

   'torch.File.writeObject':
      'prefix': 'torchFilewriteObject'
      'body': """
        <a name="torch.File.writeObject"></a>
        ### writeObject(object) ###
        
        Writes `object` into the file. This object can be read later using
        [readObject()](#torch.File.readObject). Serializable objects are `Torch`
        objects having a `read()` and `write()` method. `Lua` objects such as
        `table`, `number` or `string` or pure Lua functions are also serializable.
        
        If the object has been already written in the file, only a _reference_ to
        this already saved object will be written: this saves space an speed-up
        writing; it also allows to keep the dependencies between objects intact.
        
        In returns, if one writes an object, modify its member, and write the
        object again in the same file, the modifications will not be recorded
        in the file, as only a reference to the original will be written. See
        [readObject()](#torch.File.readObject) for an example.
        
        
        """

   'torch.File.readString':
      'prefix': 'torchFilereadString'
      'body': """
        <a name="torch.File.readString"></a>
        ### [string] readString(format) ###
        
        If `format` starts with ''"*l"` then returns the next line in the `File''. The end-of-line character is skipped.
        
        If `format` starts with ''"*a"` then returns all the remaining contents of the `File''.
        
        If no data is available, then an error is raised, except if `File` is in [quiet()](#torch.File.quiet) mode where
        it then returns an empty string `''` and after that you'll be able to see that last reading failed due to end of file with your_file:[hasError()](#torch.File.hasError).
        
        Because Torch is more precise on number typing, the `Lua` format ''"*n"'' is not supported:
        instead use one of the [number read methods](#torch.File.read).
        
        
        """

   'torch.File.writeString':
      'prefix': 'torchFilewriteString'
      'body': """
        <a name="torch.File.writeString"></a>
        ### [number] writeString(str) ###
        
        Writes the string `str` in the `File`. If the string cannot be written completely an error is raised, except
        if `File` is in [quiet()](#torch.File.quiet) mode where it returns the number of character actually written.
        
        ## General Access and Control Methods ##
        
        
        """

   'torch.File.ascii':
      'prefix': 'torchFileascii'
      'body': """
        <a name="torch.File.ascii"></a>
        ### ascii() [default] ###
        
        The data read or written will be in `ASCII` mode: all numbers are converted
        to characters (human readable format) and boolean are converted to `0`
        (false) or `1` (true). The input-output format in this mode depends on the
        options [autoSpacing()](#torch.File.autoSpacing) and
        [noAutoSpacing()](#torch.File.noAutoSpacing).
        
        
        """

   'torch.File.autoSpacing':
      'prefix': 'torchFileautoSpacing'
      'body': """
        <a name="torch.File.autoSpacing"></a>
        ### autoSpacing() [default] ###
        
        In [ASCII](#torch.File.ascii) mode, write additional spaces around the elements
        written on disk: if writing a [Storage](storage.md), a space will be
        generated between each _element_ and a _return line_ after the last
        element. If only writing one element, a _return line_ will be generated
        after this element.
        
        Those spaces are supposed to exist while reading in this mode.
        
        This is the default behavior. You can de-activate this option with the
        [noAutoSpacing()](#torch.File.noAutoSpacing) method.
        
        
        """

   'torch.File.binary':
      'prefix': 'torchFilebinary'
      'body': """
        <a name="torch.File.binary"></a>
        ### binary() ###
        
        The data read or written will be in binary mode: the representation in the
        `File` is the same that the one in the computer memory/register (not human
        readable).  This mode is faster than [ASCII](#torch.File.ascii) but less
        portable.
        
        
        """

   'torch.File.clearError':
      'prefix': 'torchFileclearError'
      'body': """
        <a name="torch.File.clearError"></a>
        ### clearError() ###
        
        Clear the error.flag returned by [hasError()](#torch.File.hasError).
        
        
        """

   'torch.File.close':
      'prefix': 'torchFileclose'
      'body': """
        <a name="torch.File.close"></a>
        ### close() ###
        
        Close the file. Any subsequent operation will generate a `Lua` error.
        
        
        """

   'torch.File.noAutoSpacing':
      'prefix': 'torchFilenoAutoSpacing'
      'body': """
        <a name="torch.File.noAutoSpacing"></a>
        ### noAutoSpacing() ###
        
        In [ASCII](#torch.File.ascii) mode, do not put extra spaces between element
        written on disk. This is the contrary of the option
        [autoSpacing()](#torch.File.autoSpacing).
        
        
        """

   'torch.File.synchronize':
      'prefix': 'torchFilesynchronize'
      'body': """
        <a name="torch.File.synchronize"></a>
        ### synchronize() ###
        
        If the child class bufferize the data while writing, ensure that the data
        is actually written.
        
        
        
        """

   'torch.File.pedantic':
      'prefix': 'torchFilepedantic'
      'body': """
        <a name="torch.File.pedantic"></a>
        ### pedantic() [default] ###
        
        If this mode is chosen (which is the default), a `Lua` error will be
        generated in case of error (which will cause the program to stop).
        
        It is possible to use [quiet()](#torch.File.quiet) to avoid `Lua` error generation
        and set a flag instead.
        
        
        """

   'torch.File.position':
      'prefix': 'torchFileposition'
      'body': """
        <a name="torch.File.position"></a>
        ### [number] position() ###
        
        Returns the current position (in bytes) in the file.
        The first position is `1` (following Lua standard indexing).
        
        
        """

   'torch.File.quiet':
      'prefix': 'torchFilequiet'
      'body': """
        <a name="torch.File.quiet"></a>
        ### quiet() ###
        
        If this mode is chosen instead of [pedantic()](#torch.File.pedantic), no `Lua`
        error will be generated in case of read/write error. Instead, a flag will
        be raised, readable through [hasError()](#torch.File.hasError). This flag can
        be cleared with [clearError()](#torch.File.clearError)
        
        Checking if a file is quiet can be performed using [isQuiet()](#torch.File.isQuiet).
        
        
        """

   'torch.File.seek':
      'prefix': 'torchFileseek'
      'body': """
        <a name="torch.File.seek"></a>
        ### seek(position) ###
        
        Jump into the file at the given `position` (in byte). Might generate/raise
        an error in case of problem. The first position is `1` (following Lua standard indexing).
        
        
        """

   'torch.File.seekEnd':
      'prefix': 'torchFileseekEnd'
      'body': """
        <a name="torch.File.seekEnd"></a>
        ### seekEnd() ###
        
        Jump at the end of the file. Might generate/raise an error in case of
        problem.
        
        ## File state query ##
        
        These methods allow the user to query the state of the given `File`.
        
        
        """

   'torch.File.hasError':
      'prefix': 'torchFilehasError'
      'body': """
        <a name="torch.File.hasError"></a>
        ### [boolean] hasError() ###
        
        Returns if an error occurred since the last [clearError()](#torch.File.clearError) call, or since
        the opening of the file if `clearError()` has never been called.
        
        
        """

   'torch.File.isQuiet':
      'prefix': 'torchFileisQuiet'
      'body': """
        <a name="torch.File.isQuiet"></a>
        ### [boolean] isQuiet() ###
        
        Returns a boolean which tells if the file is in [quiet](#torch.File.quiet) mode or not.
        
        
        """

   'torch.File.isReadable':
      'prefix': 'torchFileisReadable'
      'body': """
        <a name="torch.File.isReadable"></a>
        ### [boolean] isReadable() ###
        
        Tells if one can read the file or not.
        
        
        """

   'torch.File.isWritable':
      'prefix': 'torchFileisWritable'
      'body': """
        <a name="torch.File.isWritable"></a>
        ### [boolean] isWritable() ###
        
        Tells if one can write in the file or not.
        
        
        """

   'torch.File.isAutoSpacing':
      'prefix': 'torchFileisAutoSpacing'
      'body': """
        <a name="torch.File.isAutoSpacing"></a>
        ### [boolean] isAutoSpacing() ###
        
        Return `true` if [autoSpacing](#torch.File.autoSpacing) has been chosen.
        
        
        """

   'torch.File.referenced':
      'prefix': 'torchFilereferenced'
      'body': """
        <a name="torch.File.referenced"></a>
        ### referenced(ref) ###
        
        Sets the referenced property of the File to `ref`. `ref` has to be `true`
        or `false`.
        
        By default `ref` is true, which means that a File object keeps track of
        objects written (using [writeObject](#torch.File.writeObject) method) or
        read (using [readObject](#torch.File.readObject) method). Objects with the
        same address will be written or read only once, meaning that this approach
        preserves shared memory structured.
        
        Keeping track of references has a cost: every object which is serialized in
        the file is kept alive (even if one discards the object after
        writing/reading) as File needs to track their pointer. This is not always a
        desirable behavior, especially when dealing with large data structures.
        
        Another typical example when does not want reference tracking is when
        one needs to push the same tensor repeatedly into a file but every time
        changing its contents: calling `referenced(false)` ensures desired
        behaviour.
        
        
        """

   'torch.File.isReferenced':
      'prefix': 'torchFileisReferenced'
      'body': """
        <a name="torch.File.isReferenced"></a>
        ### isReferenced() ###
        
        Return the state set by [referenced](#torch.File.referenced).
        
        """

   'torch.reference.dok':
      'prefix': 'torchreferencedok'
      'body': """
        <a name="torch.reference.dok"></a>
        # Torch Package Reference Manual #
        
        [![Join the chat at https://gitter.im/torch/torch7](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/torch/torch7?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
        [![Build Status](https://travis-ci.org/torch/torch7.svg)](https://travis-ci.org/torch/torch7)
        
        __Torch__ is the main package in [Torch7](http://torch.ch) where data
        structures for multi-dimensional tensors and mathematical operations
        over these are defined. Additionally, it provides many utilities for
        accessing files, serializing objects of arbitrary types and other
        useful utilities.
        
        
        """

   'torch.reference.dok':
      'prefix': 'torchreferencedok'
      'body': """
        <a name="torch.reference.dok"></a>
        ## Torch Packages ##
        
        * Tensor Library
        * [Tensor](tensor.md) defines the _all powerful_ tensor object that provides multi-dimensional numerical arrays with type templating.
        * [Mathematical operations](maths.md) that are defined for the tensor object types.
        * [Storage](storage.md) defines a simple storage interface that controls the underlying storage for any tensor object.
        * File I/O Interface Library
        * [File](file.md) is an abstract interface for common file operations.
        * [Disk File](diskfile.md) defines operations on files stored on disk.
        * [Memory File](memoryfile.md) defines operations on stored in RAM.
        * [Pipe File](pipefile.md) defines operations for using piped commands.
        * [High-Level File operations](serialization.md) defines higher-level serialization functions.
        * Useful Utilities
        * [Timer](timer.md) provides functionality for _measuring time_.
        * [Tester](tester.md) is a generic tester framework.
        * [CmdLine](cmdline.md) is a command line argument parsing utility.
        * [Random](random.md) defines a random number generator package with various distributions.
        * Finally useful [utility](utility.md) functions are provided for easy handling of torch tensor types and class inheritance.
        
        
        """

   'nn.dok':
      'prefix': 'nndok'
      'body': """
        <a name="nn.dok"></a>
        # Neural Network Package #
        
        This package provides an easy and modular way to build and train simple or complex neural networks using [Torch](https://github.com/torch/torch7/blob/master/README.md):
        
        * Modules are the bricks used to build neural networks. Each are themselves neural networks, but can be combined with other networks using containers to create complex neural networks:
        * [Module](module.md#nn.Module) : abstract class inherited by all modules;
        * [Containers](containers.md#nn.Containers) : container classes like [Sequential](containers.md#nn.Sequential), [Parallel](containers.md#nn.Parallel) and [Concat](containers.md#nn.Concat);
        * [Transfer functions](transfer.md#nn.transfer.dok) : non-linear functions like [Tanh](transfer.md#nn.Tanh) and [Sigmoid](transfer.md#nn.Sigmoid);
        * [Simple layers](simple.md#nn.simplelayers.dok) : like [Linear](simple.md#nn.Linear), [Mean](simple.md#nn.Mean), [Max](simple.md#nn.Max) and [Reshape](simple.md#nn.Reshape); 
        * [Table layers](table.md#nn.TableLayers) : layers for manipulating tables like [SplitTable](table.md#nn.SplitTable), [ConcatTable](table.md#nn.ConcatTable) and [JoinTable](table.md#nn.JoinTable);
        * [Convolution layers](convolution.md#nn.convlayers.dok) : [Temporal](convolution.md#nn.TemporalModules),  [Spatial](convolution.md#nn.SpatialModules) and [Volumetric](convolution.md#nn.VolumetricModules) convolutions ; 
        * Criterions compute a gradient according to a given loss function given an input and a target:
        * [Criterions](criterion.md#nn.Criterions) : a list of all criterions, including [Criterion](criterion.md#nn.Criterion), the abstract class;
        * [MSECriterion](criterion.md#nn.MSECriterion) : the Mean Squared Error criterion used for regression; 
        * [ClassNLLCriterion](criterion.md#nn.ClassNLLCriterion) : the Negative Log Likelihood criterion used for classification;
        * Additional documentation :
        * [Overview](overview.md#nn.overview.dok) of the package essentials including modules, containers and training;
        * [Training](training.md#nn.traningneuralnet.dok) : how to train a neural network using [StochasticGradient](training.md#nn.StochasticGradient);
        * [Testing](testing.md) : how to test your modules.
        * [Experimental Modules](https://github.com/clementfarabet/lua---nnx/blob/master/README.md) : a package containing experimental modules and criteria.
        
        
        """

   'torch.maths.dok':
      'prefix': 'torchmathsdok'
      'body': """
        <a name="torch.maths.dok"></a>
        # Math Functions #
        
        Torch provides Matlab-like functions for manipulating [`Tensor`](tensor.md) objects.  Functions fall into several types of categories:
        
        * [Constructors](#torch.construction.dok) like [`zeros`](#torch.zeros), [`ones`](#torch.ones);
        * Extractors like [`diag`](#torch.diag)  and [`triu`](#torch.triu);
        * [Element-wise](#torch.elementwise.dok) mathematical operations like [`abs`](#torch.abs) and [`pow`](#torch.pow);
        * [BLAS](#torch.basicoperations.dok) operations;
        * [Column or row-wise operations](#torch.columnwise.dok) like [`sum`](#torch.sum) and [`max`](#torch.max);
        * [Matrix-wide operations](#torch.matrixwide.dok) like [`trace`](#torch.trace) and [`norm`](#torch.norm);
        * [Convolution and cross-correlation](#torch.conv.dok) operations like [`conv2`](#torch.conv2);
        * [Basic linear algebra operations](#torch.linalg.dok) like [`eig`](#torch.eig);
        * [Logical operations](#torch.logical.dok) on `Tensor`s.
        
        By default, all operations allocate a new `Tensor` to return the result.
        However, all functions also support passing the target `Tensor`(s) as the first argument(s), in which case the target `Tensor`(s) will be resized accordingly and filled with result.
        This property is especially useful when one wants have tight control over when memory is allocated.
        
        The *Torch* package adopts the same concept, so that calling a function directly on the `Tensor` itself using an object-oriented syntax is equivalent to passing the `Tensor` as the optional resulting `Tensor`.
        The following two calls are equivalent.
        
        ```lua
        torch.log(x, x)
        x:log()
        ```
        
        Similarly, `torch.conv2` function can be used in the following manner.
        
        ```lua
        > x = torch.rand(100, 100)
        > k = torch.rand(10, 10)
        > res1 = torch.conv2(x, k)   -- case 1
        
        > res2 = torch.Tensor()
        > torch.conv2(res2, x, k)     -- case 2
        
        > res2:dist(res1)
        0
        ```
        
        The advantage of second case is, same `res2` `Tensor` can be used successively in a loop without any new allocation.
        
        ```lua
        -- no new memory allocations...
        > for i = 1, 100 do
        torch.conv2(res2, x, k)
        end
        
        > res2:dist(res1)
        0
        ```
        
        
        """

   'torch.construction.dok':
      'prefix': 'torchconstructiondok'
      'body': """
        <a name="torch.construction.dok"></a>
        ## Construction or extraction functions ##
        
        
        """

   'torch.cat':
      'prefix': 'torchcat'
      'body': """
        <a name="torch.cat"></a>
        ### [res] torch.cat( [res,] x_1, x_2, [dimension] ) ###
        ### [res] torch.cat( [res,] {x_1, x_2, ...}, [dimension] ) ###
        
        """

   'torch.cat':
      'prefix': 'torchcat'
      'body': """
        <a name="torch.cat"></a>
        `x = torch.cat(x_1, x_2, [dimension])` returns a `Tensor` `x` which is the concatenation of `Tensor`s `x_1` and `x_2` along dimension `dimension`.
        
        If `dimension` is not specified it is the last dimension.
        
        The other dimensions of `x_1` and `x_2` have to be equal.
        
        Also supports arrays with arbitrary numbers of `Tensor`s as inputs.
        
        Examples:
        ```lua
        > torch.cat(torch.ones(3), torch.zeros(2))
        1
        1
        1
        0
        0
        [torch.Tensor of dimension 5]
        
        > torch.cat(torch.ones(3, 2), torch.zeros(2, 2), 1)
        1  1
        1  1
        1  1
        0  0
        0  0
        [torch.DoubleTensor of dimension 5x2]
        
        > torch.cat(torch.ones(2, 2), torch.zeros(2, 2), 1)
        1  1
        1  1
        0  0
        0  0
        [torch.DoubleTensor of dimension 4x2]
        
        > torch.cat(torch.ones(2, 2), torch.zeros(2, 2), 2)
        1  1  0  0
        1  1  0  0
        [torch.DoubleTensor of dimension 2x4]
        
        > torch.cat(torch.cat(torch.ones(2, 2), torch.zeros(2, 2), 1), torch.rand(3, 2), 1)
        1.0000  1.0000
        1.0000  1.0000
        0.0000  0.0000
        0.0000  0.0000
        0.3227  0.0493
        0.9161  0.1086
        0.2206  0.7449
        [torch.DoubleTensor of dimension 7x2]
        
        > torch.cat({torch.ones(2, 2), torch.zeros(2, 2), torch.rand(3, 2)}, 1)
        1.0000  1.0000
        1.0000  1.0000
        0.0000  0.0000
        0.0000  0.0000
        0.3227  0.0493
        0.9161  0.1086
        0.2206  0.7449
        [torch.DoubleTensor of dimension 7x2]
        
        ```
        
        
        
        """

   'torch.diag':
      'prefix': 'torchdiag'
      'body': """
        <a name="torch.diag"></a>
        ### [res] torch.diag([res,] x [,k]) ###
        
        """

   'torch.diag':
      'prefix': 'torchdiag'
      'body': """
        <a name="torch.diag"></a>
        
        `y = torch.diag(x)` when `x` is of dimension 1 returns a diagonal matrix with diagonal elements constructed from `x`.
        
        `y = torch.diag(x)` when `x` is of dimension 2 returns a `Tensor` of dimension 1 with elements constructed from the diagonal of `x`.
        
        `y = torch.diag(x, k)` returns the k-th diagonal of `x`, where `k = 0` is the main diagonal, `k > 0` is above the main diagonal and `k < 0` is below the main diagonal.
        
        
        """

   'torch.eye':
      'prefix': 'torcheye'
      'body': """
        <a name="torch.eye"></a>
        ### [res] torch.eye([res,] n [,m]) ###
        
        """

   'torch.eye':
      'prefix': 'torcheye'
      'body': """
        <a name="torch.eye"></a>
        
        `y = torch.eye(n)` returns the `n  n` identity matrix.
        
        `y = torch.eye(n, m)` returns an `n  m` identity matrix with ones on the diagonal and zeros elsewhere.
        
        
        
        """

   'torch.histc':
      'prefix': 'torchhistc'
      'body': """
        <a name="torch.histc"></a>
        ### [res] torch.histc([res,] x [,nbins, min_value, max_value]) ###
        
        """

   'torch.histc':
      'prefix': 'torchhistc'
      'body': """
        <a name="torch.histc"></a>
        
        `y = torch.histc(x)` returns the histogram of the elements in `x`.
        By default the elements are sorted into 100 equally spaced bins between the minimum and maximum values of `x`.
        
        `y = torch.histc(x, n)` same as above with `n` bins.
        
        `y = torch.histc(x, n, min, max)` same as above with `n` bins and `[min, max]` as elements range.
        
        
        
        """

   'torch.linspace':
      'prefix': 'torchlinspace'
      'body': """
        <a name="torch.linspace"></a>
        ### [res] torch.linspace([res,] x1, x2, [,n]) ###
        
        """

   'torch.linspace':
      'prefix': 'torchlinspace'
      'body': """
        <a name="torch.linspace"></a>
        
        `y = torch.linspace(x1, x2)` returns a one-dimensional `Tensor` of size 100 equally spaced points between `x1` and `x2`.
        
        `y = torch.linspace(x1, x2, n)` returns a one-dimensional `Tensor` of `n` equally spaced points between `x1` and `x2`.
        
        
        
        """

   'torch.logspace':
      'prefix': 'torchlogspace'
      'body': """
        <a name="torch.logspace"></a>
        ### [res] torch.logspace([res,] x1, x2, [,n]) ###
        
        """

   'torch.logspace':
      'prefix': 'torchlogspace'
      'body': """
        <a name="torch.logspace"></a>
        
        `y = torch.logspace(x1, x2)` returns a one-dimensional `Tensor` of `100` logarithmically eqally spaced points between `10^x1` and `10^x2`.
        
        `y = torch.logspace(x1, x2, n)` returns a one-dimensional `Tensor` of `n` logarithmically equally spaced points between `10^x1` and `10^x2`.
        
        
        """

   'torch.multinomial':
      'prefix': 'torchmultinomial'
      'body': """
        <a name="torch.multinomial"></a>
        ### [res] torch.multinomial([res,], p, n, [,replacement]) ###
        
        """

   'torch.multinomial':
      'prefix': 'torchmultinomial'
      'body': """
        <a name="torch.multinomial"></a>
        
        `y = torch.multinomial(p, n)` returns a `Tensor` `y` where each row contains `n` indices sampled from the [multinomial probability distribution](http://en.wikipedia.org/wiki/Multinomial_distribution) located in the corresponding row of `Tensor` `p`.
        
        The rows of `p` do not need to sum to one (in which case we use the values as weights), but must be non-negative and have a non-zero sum.
        Indices are ordered from left to right according to when each was sampled (first samples are placed in first column).
        
        If `p` is a vector, `y` is a vector size `n`.
        
        If `p` is a m-rows matrix, `y` is an `m  n` matrix.
        
        If `replacement` is `true`, samples are drawn **with replacement**.
        If not, they are drawn **without replacement**, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.
        This implies the constraint that `n` must be lower than `p` length (or number of columns of `p` if it is a matrix).
        
        The default value for `replacement` is `false`.
        
        
        ```lua
        p = torch.Tensor{1, 1, 0.5, 0}
        a = torch.multinomial(p, 10000, true)
        
        > a
        ...
        [torch.LongTensor of dimension 10000]
        
        > for i = 1, 4 do print(a:eq(i):sum()) end
        3967
        4016
        2017
        0
        ```
        
        Note: If you use the function with a given result `Tensor`, i.e. of the function prototype: `torch.multinomial(res, p, n [, replacement])` then you will have to call it slightly differently as:
        
        ```lua
        p.multinomial(res, p, n, replacement) -- p.multinomial instead of torch.multinomial
        ```
        
        This is due to the fact that the result here is of a `LongTensor` type, and we do not define a `torch.multinomial` over long `Tensor`s.
        
        
        """

   'torch.ones':
      'prefix': 'torchones'
      'body': """
        <a name="torch.ones"></a>
        ### [res] torch.ones([res,] m [,n...]) ###
        
        """

   'torch.ones':
      'prefix': 'torchones'
      'body': """
        <a name="torch.ones"></a>
        
        `y = torch.ones(n)` returns a one-dimensional `Tensor` of size `n` filled with ones.
        
        `y = torch.ones(m, n)` returns a `m  n` `Tensor` filled with ones.
        
        For more than `4` dimensions, you can use a storage as argument: `y = torch.ones(torch.LongStorage{m, n, k, l, o})`.
        
        
        
        """

   'torch.rand':
      'prefix': 'torchrand'
      'body': """
        <a name="torch.rand"></a>
        ### [res] torch.rand([res,] m [,n...]) ###
        
        """

   'torch.rand':
      'prefix': 'torchrand'
      'body': """
        <a name="torch.rand"></a>
        
        `y = torch.rand(n)` returns a one-dimensional `Tensor` of size `n` filled with random numbers from a uniform distribution on the interval `[0, 1)`.
        
        `y = torch.rand(m, n)` returns a `m  n` `Tensor` of random numbers from a uniform distribution on the interval `[0, 1)`.
        
        For more than 4 dimensions, you can use a storage as argument: `y = torch.rand(torch.LongStorage{m, n, k, l, o})`.
        
        
        
        """

   'torch.randn':
      'prefix': 'torchrandn'
      'body': """
        <a name="torch.randn"></a>
        ### [res] torch.randn([res,] m [,n...]) ###
        
        """

   'torch.randn':
      'prefix': 'torchrandn'
      'body': """
        <a name="torch.randn"></a>
        
        `y = torch.randn(n)` returns a one-dimensional `Tensor` of size `n` filled with random numbers from a normal distribution with mean zero and variance one.
        
        `y = torch.randn(m, n)` returns a `m  n` `Tensor` of random numbers from a normal distribution with mean zero and variance one.
        
        For more than 4 dimensions, you can use a storage as argument: `y = torch.randn(torch.LongStorage{m, n, k, l, o})`.
        
        
        
        """

   'torch.range':
      'prefix': 'torchrange'
      'body': """
        <a name="torch.range"></a>
        ### [res] torch.range([res,] x, y [,step]) ###
        
        """

   'torch.range':
      'prefix': 'torchrange'
      'body': """
        <a name="torch.range"></a>
        
        `y = torch.range(x, y)` returns a `Tensor` of size `floor((y - x) / step) + 1` with values from `x` to `y` with step `step` (default to 1).
        
        ```lua
        > torch.range(2, 5)
        2
        3
        4
        5
        [torch.Tensor of dimension 4]
        
        > torch.range(2, 5, 1.2)
        2.0000
        3.2000
        4.4000
        [torch.DoubleTensor of dimension 3]
        ```
        
        
        
        """

   'torch.randperm':
      'prefix': 'torchrandperm'
      'body': """
        <a name="torch.randperm"></a>
        ### [res] torch.randperm([res,] n) ###
        
        """

   'torch.randperm':
      'prefix': 'torchrandperm'
      'body': """
        <a name="torch.randperm"></a>
        
        `y = torch.randperm(n)` returns a random permutation of integers from 1 to `n`.
        
        
        
        """

   'torch.reshape':
      'prefix': 'torchreshape'
      'body': """
        <a name="torch.reshape"></a>
        ### [res] torch.reshape([res,] x, m [,n...]) ###
        
        """

   'torch.reshape':
      'prefix': 'torchreshape'
      'body': """
        <a name="torch.reshape"></a>
        
        `y = torch.reshape(x, m, n)` returns a new `m  n` `Tensor` y whose elements are taken rowwise from `x`, which must have `m * n` elements. The elements are copied into the new `Tensor`.
        
        For more than 4 dimensions, you can use a storage: `y = torch.reshape(x, torch.LongStorage{m, n, k, l, o})`.
        
        
        
        """

   'torch.tril':
      'prefix': 'torchtril'
      'body': """
        <a name="torch.tril"></a>
        ### [res] torch.tril([res,] x [,k]) ###
        
        """

   'torch.tril':
      'prefix': 'torchtril'
      'body': """
        <a name="torch.tril"></a>
        
        `y = torch.tril(x)` returns the lower triangular part of `x`, the other elements of `y` are set to 0.
        
        `torch.tril(x, k)` returns the elements on and below the k-th diagonal of `x` as non-zero.
        `k = 0` is the main diagonal, `k > 0` is above the main diagonal and `k < 0` is below the main diagonal.
        
        
        
        """

   'torch.triu':
      'prefix': 'torchtriu'
      'body': """
        <a name="torch.triu"></a>
        ### [res] torch.triu([res,] x, [,k]) ###
        
        """

   'torch.triu':
      'prefix': 'torchtriu'
      'body': """
        <a name="torch.triu"></a>
        
        `y = torch.triu(x)` returns the upper triangular part of `x`, the other elements of `y` are set to 0.
        
        `torch.triu(x, k)` returns the elements on and above the k-th diagonal of `x` as non-zero.
        `k = 0` is the main diagonal, `k > 0` is above the main diagonal and `k < 0` is below the main diagonal.
        
        
        
        """

   'torch.zeros':
      'prefix': 'torchzeros'
      'body': """
        <a name="torch.zeros"></a>
        ### [res] torch.zeros([res,] x) ###
        
        """

   'torch.zeros':
      'prefix': 'torchzeros'
      'body': """
        <a name="torch.zeros"></a>
        
        `y = torch.zeros(n)` returns a one-dimensional `Tensor` of size n filled with zeros.
        
        `y = torch.zeros(m, n)` returns a `m  n` `Tensor` filled with zeros.
        
        For more than 4 dimensions, you can use a storage: `y = torch.zeros(torch.LongStorage{m, n, k, l, o})`.
        
        
        
        """

   'torch.elementwise.dok':
      'prefix': 'torchelementwisedok'
      'body': """
        <a name="torch.elementwise.dok"></a>
        ## Element-wise Mathematical Operations ##
        
        
        """

   'torch.abs':
      'prefix': 'torchabs'
      'body': """
        <a name="torch.abs"></a>
        ### [res] torch.abs([res,] x) ###
        
        """

   'torch.abs':
      'prefix': 'torchabs'
      'body': """
        <a name="torch.abs"></a>
        
        `y = torch.abs(x)` returns a new `Tensor` with the absolute values of the elements of `x`.
        
        `x:abs()` replaces all elements in-place with the absolute values of the elements of `x`.
        
        
        
        """

   'torch.sign':
      'prefix': 'torchsign'
      'body': """
        <a name="torch.sign"></a>
        ### [res] torch.sign([res,] x) ###
        
        """

   'torch.sign':
      'prefix': 'torchsign'
      'body': """
        <a name="torch.sign"></a>
        
        `y = torch.sign(x)` returns a new `Tensor` with the sign (`+/- 1`) of the elements of `x`.
        
        `x:sign()` replaces all elements in-place with the sign of the elements of `x`.
        
        
        
        """

   'torch.acos':
      'prefix': 'torchacos'
      'body': """
        <a name="torch.acos"></a>
        ### [res] torch.acos([res,] x) ###
        
        """

   'torch.acos':
      'prefix': 'torchacos'
      'body': """
        <a name="torch.acos"></a>
        
        `y = torch.acos(x)` returns a new `Tensor` with the arcosine of the elements of `x`.
        
        `x:acos()` replaces all elements in-place with the arcosine of the elements of `x`.
        
        
        
        """

   'torch.asin':
      'prefix': 'torchasin'
      'body': """
        <a name="torch.asin"></a>
        ### [res] torch.asin([res,] x) ###
        
        """

   'torch.asin':
      'prefix': 'torchasin'
      'body': """
        <a name="torch.asin"></a>
        
        `y = torch.asin(x)` returns a new `Tensor` with the arcsine  of the elements of `x`.
        
        `x:asin()` replaces all elements in-place with the arcsine  of the elements of `x`.
        
        
        
        """

   'torch.atan':
      'prefix': 'torchatan'
      'body': """
        <a name="torch.atan"></a>
        ### [res] torch.atan([res,] x) ###
        
        """

   'torch.atan':
      'prefix': 'torchatan'
      'body': """
        <a name="torch.atan"></a>
        
        `y = torch.atan(x)` returns a new `Tensor` with the arctangent of the elements of `x`.
        
        `x:atan()` replaces all elements in-place with the arctangent of the elements of `x`.
        
        
        
        """

   'torch.ceil':
      'prefix': 'torchceil'
      'body': """
        <a name="torch.ceil"></a>
        ### [res] torch.ceil([res,] x) ###
        
        """

   'torch.ceil':
      'prefix': 'torchceil'
      'body': """
        <a name="torch.ceil"></a>
        
        `y = torch.ceil(x)` returns a new `Tensor` with the values of the elements of `x` rounded up to the nearest integers.
        
        `x:ceil()` replaces all elements in-place with the values of the elements of `x` rounded up to the nearest integers.
        
        
        
        """

   'torch.cos':
      'prefix': 'torchcos'
      'body': """
        <a name="torch.cos"></a>
        ### [res] torch.cos([res,] x) ###
        
        """

   'torch.cos':
      'prefix': 'torchcos'
      'body': """
        <a name="torch.cos"></a>
        
        `y = torch.cos(x)` returns a new `Tensor` with the cosine of the elements of `x`.
        
        `x:cos()` replaces all elements in-place with the cosine of the elements of `x`.
        
        
        
        """

   'torch.cosh':
      'prefix': 'torchcosh'
      'body': """
        <a name="torch.cosh"></a>
        ### [res] torch.cosh([res,] x) ###
        
        """

   'torch.cosh':
      'prefix': 'torchcosh'
      'body': """
        <a name="torch.cosh"></a>
        
        `y = torch.cosh(x)` returns a new `Tensor` with the hyberbolic cosine of the elements of `x`.
        
        `x:cosh()` replaces all elements in-place with the hyberbolic cosine of the elements of `x`.
        
        
        
        """

   'torch.exp':
      'prefix': 'torchexp'
      'body': """
        <a name="torch.exp"></a>
        ### [res] torch.exp([res,] x) ###
        
        """

   'torch.exp':
      'prefix': 'torchexp'
      'body': """
        <a name="torch.exp"></a>
        
        `y = torch.exp(x)` returns, for each element in `x`,  *e* (*Neper number*, the base of natural logarithms) raised to the power of the element in `x`.
        
        `x:exp()` returns, for each element in `x`,  *e* raised to the power of the element in `x`.
        
        
        
        """

   'torch.floor':
      'prefix': 'torchfloor'
      'body': """
        <a name="torch.floor"></a>
        ### [res] torch.floor([res,] x) ###
        
        """

   'torch.floor':
      'prefix': 'torchfloor'
      'body': """
        <a name="torch.floor"></a>
        
        `y = torch.floor(x)` returns a new `Tensor` with the values of the elements of `x` rounded down to the nearest integers.
        
        `x:floor()` replaces all elements in-place with the values of the elements of `x` rounded down to the nearest integers.
        
        
        
        """

   'torch.log':
      'prefix': 'torchlog'
      'body': """
        <a name="torch.log"></a>
        ### [res] torch.log([res,] x) ###
        
        """

   'torch.log':
      'prefix': 'torchlog'
      'body': """
        <a name="torch.log"></a>
        
        `y = torch.log(x)` returns a new `Tensor` with the natural logarithm of the elements of `x`.
        
        `x:log()` replaces all elements in-place with the natural logarithm of the elements of `x`.
        
        
        
        """

   'torch.log1p':
      'prefix': 'torchlog1p'
      'body': """
        <a name="torch.log1p"></a>
        ### [res] torch.log1p([res,] x) ###
        
        """

   'torch.log1p':
      'prefix': 'torchlog1p'
      'body': """
        <a name="torch.log1p"></a>
        
        `y = torch.log1p(x)` returns a new `Tensor` with the natural logarithm of the elements of `x + 1`.
        
        `x:log1p()` replaces all elements in-place with the natural logarithm of the elements of `x + 1`.
        This function is more accurate than [`log`](#torch.log) for small values of `x`.
        
        
        
        """

   'x:neg':
      'prefix': 'x:neg'
      'body': """
        <a name="x:neg"></a>
        ### x:neg() ###
        
        """

   'x:neg':
      'prefix': 'x:neg'
      'body': """
        <a name="x:neg"></a>
        
        `x:neg()` replaces all elements in-place with the sign-reversed values of the elements of `x`.
        
        
        
        """

   'torch.pow':
      'prefix': 'torchpow'
      'body': """
        <a name="torch.pow"></a>
        ### [res] torch.pow([res,] x, n) ###
        
        """

   'torch.pow':
      'prefix': 'torchpow'
      'body': """
        <a name="torch.pow"></a>
        
        Let `x` be a `Tensor` and `n` a number.
        
        `y = torch.pow(x, n)` returns a new `Tensor` with the elements of `x` to the power of `n`.
        
        `y = torch.pow(n, x)` returns, a new `Tensor` with `n` to the power of the elements of `x`.
        
        `x:pow(n)` replaces all elements in-place with the elements of `x` to the power of `n`.
        
        
        
        """

   'torch.round':
      'prefix': 'torchround'
      'body': """
        <a name="torch.round"></a>
        ### [res] torch.round([res,] x) ###
        
        """

   'torch.round':
      'prefix': 'torchround'
      'body': """
        <a name="torch.round"></a>
        
        `y = torch.round(x)` returns a new `Tensor` with the values of the elements of `x` rounded to the nearest integers.
        
        `x:round()` replaces all elements in-place with the values of the elements of `x` rounded to the nearest integers.
        
        
        
        """

   'torch.sin':
      'prefix': 'torchsin'
      'body': """
        <a name="torch.sin"></a>
        ### [res] torch.sin([res,] x) ###
        
        """

   'torch.sin':
      'prefix': 'torchsin'
      'body': """
        <a name="torch.sin"></a>
        
        `y = torch.sin(x)` returns a new `Tensor` with the sine  of the elements of `x`.
        
        `x:sin()` replaces all elements in-place with the sine  of the elements of `x`.
        
        
        
        """

   'torch.sinh':
      'prefix': 'torchsinh'
      'body': """
        <a name="torch.sinh"></a>
        ### [res] torch.sinh([res,] x) ###
        
        """

   'torch.sinh':
      'prefix': 'torchsinh'
      'body': """
        <a name="torch.sinh"></a>
        
        `y = torch.sinh(x)` returns a new `Tensor` with the hyperbolic sine of the elements of `x`.
        
        `x:sinh()` replaces all elements in-place with the hyperbolic sine of the elements of `x`.
        
        
        
        """

   'torch.sqrt':
      'prefix': 'torchsqrt'
      'body': """
        <a name="torch.sqrt"></a>
        ### [res] torch.sqrt([res,] x) ###
        
        """

   'torch.sqrt':
      'prefix': 'torchsqrt'
      'body': """
        <a name="torch.sqrt"></a>
        
        `y = torch.sqrt(x)` returns a new `Tensor` with the square root of the elements of `x`.
        
        `x:sqrt()` replaces all elements in-place with the square root of the elements of `x`.
        
        
        
        """

   'torch.tan':
      'prefix': 'torchtan'
      'body': """
        <a name="torch.tan"></a>
        ### [res] torch.tan([res,] x) ###
        
        """

   'torch.tan':
      'prefix': 'torchtan'
      'body': """
        <a name="torch.tan"></a>
        
        `y = torch.tan(x)` returns a new `Tensor` with the tangent of the elements of `x`.
        
        `x:tan()` replaces all elements in-place with the tangent of the elements of `x`.
        
        
        
        """

   'torch.tanh':
      'prefix': 'torchtanh'
      'body': """
        <a name="torch.tanh"></a>
        ### [res] torch.tanh([res,] x) ###
        
        """

   'torch.tanh':
      'prefix': 'torchtanh'
      'body': """
        <a name="torch.tanh"></a>
        
        `y = torch.tanh(x)` returns a new `Tensor` with the hyperbolic tangent of the elements of `x`.
        
        `x:tanh()` replaces all elements in-place with the hyperbolic tangent of the elements of `x`.
        
        
        
        """

   'torch.basicoperations.dok':
      'prefix': 'torchbasicoperationsdok'
      'body': """
        <a name="torch.basicoperations.dok"></a>
        ## Basic operations ##
        
        In this section, we explain basic mathematical operations for `Tensor`s.
        
        
        
        """

   'torch.add':
      'prefix': 'torchadd'
      'body': """
        <a name="torch.add"></a>
        ### [res] torch.add([res,] tensor, value) ###
        
        """

   'torch.add':
      'prefix': 'torchadd'
      'body': """
        <a name="torch.add"></a>
        
        Add the given value to all elements in the `Tensor`.
        
        `y = torch.add(x, value)` returns a new `Tensor`.
        
        `x:add(value)` add `value` to all elements in place.
        
        
        
        """

   'torch.add':
      'prefix': 'torchadd'
      'body': """
        <a name="torch.add"></a>
        ### [res] torch.add([res,] tensor1, tensor2) ###
        
        """

   'torch.add':
      'prefix': 'torchadd'
      'body': """
        <a name="torch.add"></a>
        
        Add `tensor1` to `tensor2` and put result into `res`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > x:add(y)
        > x
        5  5
        5  5
        [torch.Tensor of dimension 2x2]
        ```
        
        `y = torch.add(a, b)` returns a new `Tensor`.
        
        `torch.add(y, a, b)` puts `a + b` in `y`.
        
        `a:add(b)` accumulates all elements of `b` into `a`.
        
        `y:add(a, b)` puts `a + b` in `y`.
        
        
        
        """

   'torch.add':
      'prefix': 'torchadd'
      'body': """
        <a name="torch.add"></a>
        ### [res] torch.add([res,] tensor1, value, tensor2) ###
        
        """

   'torch.add':
      'prefix': 'torchadd'
      'body': """
        <a name="torch.add"></a>
        
        Multiply elements of `tensor2` by the scalar `value` and add it to `tensor1`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > x:add(2, y)
        > x
        8  8
        8  8
        [torch.Tensor of dimension 2x2]
        ```
        
        `x:add(value, y)` multiply-accumulates values of `y` into `x`.
        
        `z:add(x, value, y)` puts the result of `x + value * y` in `z`.
        
        `torch.add(x, value, y)` returns a new `Tensor` `x + value * y`.
        
        `torch.add(z, x, value, y)` puts the result of `x + value * y` in `z`.
        
        
        
        """

   'x:csub':
      'prefix': 'x:csub'
      'body': """
        <a name="x:csub"></a>
        ### tensor:csub(value) ###
        
        """

   'x:csub':
      'prefix': 'x:csub'
      'body': """
        <a name="x:csub"></a>
        
        Subtracts the given value from all elements in the `Tensor`, in place.
        
        
        
        """

   'x:csub':
      'prefix': 'x:csub'
      'body': """
        <a name="x:csub"></a>
        ### tensor1:csub(tensor2) ###
        
        """

   'x:csub':
      'prefix': 'x:csub'
      'body': """
        <a name="x:csub"></a>
        
        Subtracts `tensor2` from `tensor1`, in place.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(8)
        > y = torch.Tensor(4):fill(3)
        > x:csub(y)
        > x
        5  5
        5  5
        [torch.Tensor of dimension 2x2]
        ```
        
        `a:csub(b)` put `a - b` into `a`.
        
        
        
        """

   'torch.mul':
      'prefix': 'torchmul'
      'body': """
        <a name="torch.mul"></a>
        ### [res] torch.mul([res,] tensor1, value) ###
        
        """

   'torch.mul':
      'prefix': 'torchmul'
      'body': """
        <a name="torch.mul"></a>
        
        Multiply all elements in the `Tensor` by the given `value`.
        
        `z = torch.mul(x, 2)` will return a new `Tensor` with the result of `x * 2`.
        
        `torch.mul(z, x, 2)` will put the result of `x * 2` in `z`.
        
        `x:mul(2)` will multiply all elements of `x` with `2` in-place.
        
        `z:mul(x, 2)` will put the result of `x * 2` in `z`.
        
        
        
        """

   'torch.clamp':
      'prefix': 'torchclamp'
      'body': """
        <a name="torch.clamp"></a>
        ### [res] torch.clamp([res,] tensor, min_value, max_value) ###
        
        """

   'torch.mul':
      'prefix': 'torchmul'
      'body': """
        <a name="torch.mul"></a>
        
        Clamp all elements in the `Tensor` into the range `[min_value, max_value]`.  ie:
        
        ```
         min_value, if x_i < min_value
        y_i =  x_i,       if min_value  x_i  max_value
         max_value, if x_i > max_value
        ```
        
        `z = torch.clamp(x, 0, 1)` will return a new `Tensor` with the result of `x` bounded between `0` and `1`.
        
        `torch.clamp(z, x, 0, 1)` will put the result in `z`.
        
        `x:clamp(0, 1)` will perform the clamp operation in place (putting the result in `x`).
        
        `z:clamp(x, 0, 1)` will put the result in `z`.
        
        
        
        """

   'torch.cmul':
      'prefix': 'torchcmul'
      'body': """
        <a name="torch.cmul"></a>
        ### [res] torch.cmul([res,] tensor1, tensor2) ###
        
        """

   'torch.cmul':
      'prefix': 'torchcmul'
      'body': """
        <a name="torch.cmul"></a>
        
        Element-wise multiplication of `tensor1` by `tensor2`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > x:cmul(y)
        > = x
        6  6
        6  6
        [torch.Tensor of dimension 2x2]
        ```
        
        `z = torch.cmul(x, y)` returns a new `Tensor`.
        
        `torch.cmul(z, x, y)` puts the result in `z`.
        
        `y:cmul(x)` multiplies all elements of `y` with corresponding elements of `x`.
        
        `z:cmul(x, y)` puts the result in `z`.
        
        
        
        """

   'torch.cpow':
      'prefix': 'torchcpow'
      'body': """
        <a name="torch.cpow"></a>
        ### [res] torch.cpow([res,] tensor1, tensor2) ###
        
        """

   'torch.cpow':
      'prefix': 'torchcpow'
      'body': """
        <a name="torch.cpow"></a>
        
        Element-wise power operation, taking the elements of `tensor1` to the powers given by elements of `tensor2`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > x:cpow(y)
        > x
        8  8
        8  8
        [torch.Tensor of dimension 2x2]
        ```
        
        `z = torch.cpow(x, y)` returns a new `Tensor`.
        
        `torch.cpow(z, x, y)` puts the result in `z`.
        
        `y:cpow(x)` takes all elements of `y` to the powers given by the corresponding elements of `x`.
        
        `z:cpow(x, y)` puts the result in `z`.
        
        
        
        """

   'torch.addcmul':
      'prefix': 'torchaddcmul'
      'body': """
        <a name="torch.addcmul"></a>
        ### [res] torch.addcmul([res,] x [,value], tensor1, tensor2) ###
        
        """

   'torch.addcmul':
      'prefix': 'torchaddcmul'
      'body': """
        <a name="torch.addcmul"></a>
        
        Performs the element-wise multiplication of `tensor1` by `tensor2`, multiply the result by the scalar `value` (1 if not present) and add it to `x`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > z = torch.Tensor(2, 2):fill(5)
        > x:addcmul(2, y, z)
        > x
        32  32
        32  32
        [torch.Tensor of dimension 2x2]
        ```
        
        `z:addcmul(value, x, y)` accumulates the result in `z`.
        
        `torch.addcmul(z, value, x, y)` returns a new `Tensor` with the result.
        
        `torch.addcmul(z, z, value, x, y)` puts the result in `z`.
        
        
        
        """

   'torch.div':
      'prefix': 'torchdiv'
      'body': """
        <a name="torch.div"></a>
        ### [res] torch.div([res,] tensor, value) ###
        
        """

   'torch.div':
      'prefix': 'torchdiv'
      'body': """
        <a name="torch.div"></a>
        
        Divide all elements in the `Tensor` by the given `value`.
        
        `z = torch.div(x, 2)` will return a new `Tensor` with the result of `x / 2`.
        
        `torch.div(z, x, 2)` will put the result of `x / 2` in `z`.
        
        `x:div(2)` will divide all elements of `x` with `2` in-place.
        
        `z:div(x, 2)` with put the result of `x / 2` in `z`.
        
        
        
        """

   'torch.cdiv':
      'prefix': 'torchcdiv'
      'body': """
        <a name="torch.cdiv"></a>
        ### [res] torch.cdiv([res,] tensor1, tensor2) ###
        
        """

   'torch.cdiv':
      'prefix': 'torchcdiv'
      'body': """
        <a name="torch.cdiv"></a>
        
        Performs the element-wise division of `tensor1` by `tensor2`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(1)
        > y = torch.range(1, 4)
        > x:cdiv(y)
        > x
        1.0000  0.5000
        0.3333  0.2500
        [torch.Tensor of dimension 2x2]
        ```
        
        `z = torch.cdiv(x, y)` returns a new `Tensor`.
        
        `torch.cdiv(z, x, y)` puts the result in `z`.
        
        `y:cdiv(x)` divides all elements of `y` with corresponding elements of `x`.
        
        `z:cdiv(x, y)` puts the result in `z`.
        
        
        
        """

   'torch.addcdiv':
      'prefix': 'torchaddcdiv'
      'body': """
        <a name="torch.addcdiv"></a>
        ### [res] torch.addcdiv([res,] x [,value], tensor1, tensor2) ###
        
        """

   'torch.addcdiv':
      'prefix': 'torchaddcdiv'
      'body': """
        <a name="torch.addcdiv"></a>
        
        Performs the element-wise division of `tensor1` by `tensor2`, multiply the result by the scalar `value` and add it to `x`.
        The number of elements must match, but sizes do not matter.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(1)
        > y = torch.range(1, 4)
        > z = torch.Tensor(2, 2):fill(5)
        > x:addcdiv(2, y, z)
        > x
        1.4000  1.8000
        2.2000  2.6000
        [torch.Tensor of dimension 2x2]
        ```
        
        `z:addcdiv(value, x, y)` accumulates the result in `z`.
        
        `torch.addcdiv(z, value, x, y)` returns a new `Tensor` with the result.
        
        `torch.addcdiv(z, z, value, x, y)` puts the result in `z`.
        
        
        
        """

   'torch.dot':
      'prefix': 'torchdot'
      'body': """
        <a name="torch.dot"></a>
        ### [number] torch.dot(tensor1, tensor2) ###
        
        """

   'torch.dot':
      'prefix': 'torchdot'
      'body': """
        <a name="torch.dot"></a>
        
        Performs the dot product between `tensor1` and `tensor2`.
        The number of elements must match: both `Tensor`s are seen as a 1D vector.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > x:dot(y)
        24
        ```
        
        `torch.dot(x, y)` returns dot product of `x` and `y`.
        `x:dot(y)` returns dot product of `x` and `y`.
        
        
        
        """

   'torch.addmv':
      'prefix': 'torchaddmv'
      'body': """
        <a name="torch.addmv"></a>
        ### [res] torch.addmv([res,] [beta,] [v1,] vec1, [v2,] mat, vec2) ###
        
        """

   'torch.addmv':
      'prefix': 'torchaddmv'
      'body': """
        <a name="torch.addmv"></a>
        
        Performs a matrix-vector multiplication between `mat` (2D `Tensor`) and `vec2` (1D `Tensor`) and add it to `vec1`.
        
        Optional values `v1` and `v2` are scalars that multiply `vec1` and `vec2` respectively.
        
        Optional value `beta` is  a scalar that scales the result `Tensor`, before accumulating the result into the `Tensor`.
        Defaults to `1.0`.
        
        In other words,
        
        ```
        res = (beta * res) + (v1 * vec1) + (v2 * (mat * vec2))
        ```
        
        Sizes must respect the matrix-multiplication operation: if `mat` is a `n  m` matrix, `vec2` must be vector of size `m` and `vec1` must be a vector of size `n`.
        
        ```lua
        > x = torch.Tensor(3):fill(0)
        > M = torch.Tensor(3, 2):fill(3)
        > y = torch.Tensor(2):fill(2)
        > x:addmv(M, y)
        > x
        12
        12
        12
        [torch.Tensor of dimension 3]
        ```
        
        `torch.addmv(x, y, z)` returns a new `Tensor` with the result.
        
        `torch.addmv(r, x, y, z)` puts the result in `r`.
        
        `x:addmv(y, z)` accumulates `y * z` into `x`.
        
        `r:addmv(x, y, z)` puts the result of `x + y * z` into `r`.
        
        
        
        """

   'torch.addr':
      'prefix': 'torchaddr'
      'body': """
        <a name="torch.addr"></a>
        ### [res] torch.addr([res,] [v1,] mat, [v2,] vec1, vec2) ###
        
        """

   'torch.addr':
      'prefix': 'torchaddr'
      'body': """
        <a name="torch.addr"></a>
        
        Performs the outer-product between `vec1` (1D `Tensor`) and `vec2` (1D `Tensor`).
        
        Optional values `v1` and `v2` are scalars that multiply `mat` and `vec1 [out] vec2` respectively.
        
        In other words,
        
        ```
        res_ij = (v1 * mat_ij) + (v2 * vec1_i * vec2_j)
        ```
        
        If `vec1` is a vector of size `n` and `vec2` is a vector of size `m`, then `mat` must be a matrix of size `n  m`.
        
        ```lua
        > x = torch.range(1, 3)
        > y = torch.range(1, 2)
        > M = torch.Tensor(3, 2):zero()
        > M:addr(x, y)
        1  2         --     |0 0|     |1 2|
        2  4         -- = 1*|0 0| + 1*|2 4|
        3  6         --     |0 0|     |3 6|
        [torch.DoubleTensor of size 3x2]
        -- default values of v1 and v2 are 1.
        
        > M:addr(2, 1, x, y)
        3   6        --     |1 2|     |1 2|
        6  12        -- = 2*|2 4| + 1*|2 4|
        9  18        --     |3 6|     |3 6|
        [torch.DoubleTensor of size 3x2]
        
        > A = torch.range(1, 6):resize(3, 2)
        > A
        1  2
        3  4
        5  6
        [torch.DoubleTensor of size 3x2]
        > M:addr(2, A, 1, x, y)
        3   6        --   |1 2|     |1 2|
        8  12        -- 2*|3 4| + 1*|2 4|
        13  18        --   |5 6|     |3 6|
        [torch.DoubleTensor of size 3x2]
        ```
        
        `torch.addr(M, x, y)` returns the result in a new `Tensor`.
        
        `torch.addr(r, M, x, y)` puts the result in `r`.
        
        `M:addr(x, y)` puts the result in `M`.
        
        `r:addr(M, x, y)` puts the result in `r`.
        
        
        
        """

   'torch.addmm':
      'prefix': 'torchaddmm'
      'body': """
        <a name="torch.addmm"></a>
        ### [res] torch.addmm([res,] [beta,] [v1,] M [v2,] mat1, mat2) ###
        
        """

   'torch.addmm':
      'prefix': 'torchaddmm'
      'body': """
        <a name="torch.addmm"></a>
        
        Performs a matrix-matrix multiplication between `mat1` (2D `Tensor`) and `mat2` (2D `Tensor`).
        
        Optional values `v1` and `v2` are scalars that multiply `M` and `mat1 * mat2` respectively.
        
        Optional value `beta` is  a scalar that scales the result `Tensor`, before accumulating the result into the `Tensor`.
        Defaults to `1.0`.
        
        In other words,
        
        ```
        res = (res * beta) + (v1 * M) + (v2 * mat1 * mat2)
        ```
        
        If `mat1` is a `n  m` matrix, `mat2` a `m  p` matrix, `M` must be a `n  p` matrix.
        
        `torch.addmm(M, mat1, mat2)` returns the result in a new `Tensor`.
        
        `torch.addmm(r, M, mat1, mat2)` puts the result in `r`.
        
        `M:addmm(mat1, mat2)` puts the result in `M`.
        
        `r:addmm(M, mat1, mat2)` puts the result in `r`.
        
        
        
        """

   'torch.addbmm':
      'prefix': 'torchaddbmm'
      'body': """
        <a name="torch.addbmm"></a>
        ### [res] torch.addbmm([res,] [v1,] M [v2,] batch1, batch2) ###
        
        """

   'torch.addbmm':
      'prefix': 'torchaddbmm'
      'body': """
        <a name="torch.addbmm"></a>
        
        Batch matrix matrix product of matrices stored in `batch1` and `batch2`, with a reduced add step (all matrix multiplications get accumulated in a single place).
        
        `batch1` and `batch2` must be 3D `Tensor`s each containing the same number of matrices.
        If `batch1` is a `b  n  m` `Tensor`, `batch2` a `b  m  p` `Tensor`, res will be a `n  p` `Tensor`.
        
        In other words,
        
        ```
        res = (v1 * M) + (v2 * sum(batch1_i * batch2_i, i = 1, b))
        ```
        
        `torch.addbmm(M, x, y)` puts the result in a new `Tensor`.
        
        `M:addbmm(x, y)` puts the result in `M`, resizing `M` if necessary.
        
        `M:addbmm(beta, M2, alpha, x, y)` puts the result in `M`, resizing `M` if necessary.
        
        
        
        """

   'torch.baddbmm':
      'prefix': 'torchbaddbmm'
      'body': """
        <a name="torch.baddbmm"></a>
        ### [res] torch.baddbmm([res,] [v1,] M [v2,] batch1, batch2) ###
        
        """

   'torch.baddbmm':
      'prefix': 'torchbaddbmm'
      'body': """
        <a name="torch.baddbmm"></a>
        
        Batch matrix matrix product of matrices stored in `batch1` and `batch2`, with batch add.
        
        `batch1` and `batch2` must be 3D `Tensor`s each containing the same number of matrices.
        If `batch1` is a `b  n  m` `Tensor`, `batch2` a `b  m  p` `Tensor`, res will be a `b  n  p` `Tensor`.
        
        In other words,
        
        ```
        res_i = (v1 * M_i) + (v2 * batch1_i * batch2_i)
        ```
        
        `torch.baddbmm(M, x, y)` puts the result in a new `Tensor`.
        
        `M:baddbmm(x, y)` puts the result in `M`, resizing `M` if necessary.
        
        `M:baddbmm(beta, M2, alpha, x, y)` puts the result in `M`, resizing `M` if necessary.
        
        
        
        """

   'torch.mv':
      'prefix': 'torchmv'
      'body': """
        <a name="torch.mv"></a>
        ### [res] torch.mv([res,] mat, vec) ###
        
        """

   'torch.mv':
      'prefix': 'torchmv'
      'body': """
        <a name="torch.mv"></a>
        
        Matrix vector product of `mat` and `vec`.
        Sizes must respect the matrix-multiplication operation: if `mat` is a `n  m` matrix, `vec` must be vector of size `m` and `res` must be a vector of size `n`.
        
        `torch.mv(x, y)` puts the result in a new `Tensor`.
        
        `torch.mv(M, x, y)` puts the result in `M`.
        
        `M:mv(x, y)` puts the result in `M`.
        
        
        
        """

   'torch.mm':
      'prefix': 'torchmm'
      'body': """
        <a name="torch.mm"></a>
        ### [res] torch.mm([res,] mat1, mat2) ###
        
        """

   'torch.mm':
      'prefix': 'torchmm'
      'body': """
        <a name="torch.mm"></a>
        
        Matrix matrix product of `mat1` and `mat2`.
        If `mat1` is a `n  m` matrix, `mat2` a `m  p` matrix, `res` must be a `n  p` matrix.
        
        `torch.mm(x, y)` puts the result in a new `Tensor`.
        
        `torch.mm(M, x, y)` puts the result in `M`.
        
        `M:mm(x, y)` puts the result in `M`.
        
        
        
        """

   'torch.bmm':
      'prefix': 'torchbmm'
      'body': """
        <a name="torch.bmm"></a>
        ### [res] torch.bmm([res,] batch1, batch2) ###
        
        """

   'torch.bmm':
      'prefix': 'torchbmm'
      'body': """
        <a name="torch.bmm"></a>
        
        Batch matrix matrix product of matrices stored in `batch1` and `batch2`.
        `batch1` and `batch2` must be 3D `Tensor`s each containing the same number of matrices.
        If `batch1` is a `b  n  m` `Tensor`, `batch2` a `b  m  p` `Tensor`, `res` will be a `b  n  p` `Tensor`.
        
        `torch.bmm(x, y)` puts the result in a new `Tensor`.
        
        `torch.bmm(M, x, y)` puts the result in `M`, resizing `M` if necessary.
        
        `M:bmm(x, y)` puts the result in `M`, resizing `M` if necessary.
        
        
        
        """

   'torch.ger':
      'prefix': 'torchger'
      'body': """
        <a name="torch.ger"></a>
        ### [res] torch.ger([res,] vec1, vec2) ###
        
        """

   'torch.ger':
      'prefix': 'torchger'
      'body': """
        <a name="torch.ger"></a>
        
        Outer product of `vec1` and `vec2`.
        If `vec1` is a vector of size `n` and `vec2` is a vector of size `m`, then `res` must be a matrix of size `n  m`.
        
        `torch.ger(x, y)` puts the result in a new `Tensor`.
        
        `torch.ger(M, x, y)` puts the result in `M`.
        
        `M:ger(x, y)` puts the result in `M`.
        
        
        ## Overloaded operators ##
        
        It is possible to use basic mathematic operators like `+`, `-`, `/` and `*` with `Tensor`s.
        These operators are provided as a convenience.
        While they might be handy, they create and return a new `Tensor` containing the results.
        They are thus not as fast as the operations available in the [previous section](#torch.BasicOperations.dok).
        
        Another important point to note is that these operators are only overloaded when the first operand is a `Tensor`.
        For example, this will NOT work:
        
        ```lua
        > x = 5 + torch.rand(3)
        ```
        
        
        ### Addition and subtraction ###
        
        You can add a `Tensor` to another one with the `+` operator.
        Subtraction is done with `-`.
        The number of elements in the `Tensor`s must match, but the sizes do not matter.
        The size of the returned `Tensor` will be the size of the first `Tensor`.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > y = torch.Tensor(4):fill(3)
        > x + y
        5  5
        5  5
        [torch.Tensor of dimension 2x2]
        
        > y - x
        1
        1
        1
        1
        [torch.Tensor of dimension 4]
        ```
        
        A scalar might also be added or subtracted to a `Tensor`.
        The scalar needs to be on the right of the operator.
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > x + 3
        5  5
        5  5
        [torch.Tensor of dimension 2x2]
        ```
        
        
        ### Negation ###
        
        A `Tensor` can be negated with the `-` operator placed in front:
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > -x
        -2 -2
        -2 -2
        [torch.Tensor of dimension 2x2]
        ```
        
        
        ### Multiplication ###
        
        Multiplication between two `Tensor`s is supported with the `*` operators.
        The result of the multiplication depends on the sizes of the `Tensor`s.
        
        - 1D and 1D: Returns the dot product between the two `Tensor`s (scalar).
        - 2D and 1D: Returns the matrix-vector operation between the two `Tensor`s (1D `Tensor`).
        - 2D and 2D: Returns the matrix-matrix operation between the two `Tensor`s (2D `Tensor`).
        
        Sizes must be relevant for the corresponding operation.
        
        A `Tensor` might also be multiplied by a scalar.
        The scalar might be on the right or left of the operator.
        
        Examples:
        
        ```lua
        > M = torch.Tensor(2, 2):fill(2)
        > N = torch.Tensor(2, 4):fill(3)
        > x = torch.Tensor(2):fill(4)
        > y = torch.Tensor(2):fill(5)
        > x * y -- dot product
        40
        
        > M * x --- matrix-vector
        16
        16
        [torch.Tensor of dimension 2]
        
        > M * N -- matrix-matrix
        12  12  12  12
        12  12  12  12
        [torch.Tensor of dimension 2x4]
        ```
        
        
        ### Division ###
        
        Only the division of a `Tensor` by a scalar is supported with the operator `/`.
        
        Example:
        
        ```lua
        > x = torch.Tensor(2, 2):fill(2)
        > x/3
        0.6667  0.6667
        0.6667  0.6667
        [torch.Tensor of dimension 2x2]
        ```
        
        
        
        """

   'torch.columnwise.dok':
      'prefix': 'torchcolumnwisedok'
      'body': """
        <a name="torch.columnwise.dok"></a>
        ## Column or row-wise operations  (dimension-wise operations) ##
        
        
        
        """

   'torch.cross':
      'prefix': 'torchcross'
      'body': """
        <a name="torch.cross"></a>
        ### [res] torch.cross([res,] a, b [,n]) ###
        
        `y = torch.cross(a, b)` returns the cross product of `a` and `b` along the first dimension of length 3.
        
        `y = torch.cross(a, b, n)`  returns the cross product of vectors in dimension `n` of `a` and `b`.
        
        `a` and `b` must have the same size, and both `a:size(n)` and `b:size(n)` must be 3.
        
        
        
        """

   'torch.cumprod':
      'prefix': 'torchcumprod'
      'body': """
        <a name="torch.cumprod"></a>
        ### [res] torch.cumprod([res,] x [,dim]) ###
        
        `y = torch.cumprod(x)` returns the cumulative product of the elements of `x`, performing the operation over the last dimension.
        
        `y = torch.cumprod(x, n)` returns the cumulative product of the elements of `x`, performing the operation over dimension `n`.
        
        ```lua
        -- 1. cumulative product for a vector
        > A = torch.range(1, 5)
        > A
        1
        2
        3
        4
        5
        [torch.DoubleTensor of size 5]
        
        > B = torch.cumprod(A)
        > B
        1     -- B(1) = A(1) = 1
        2     -- B(2) = A(1)*A(2) = 1*2 = 2
        6     -- B(3) = A(1)*A(2)*A(3) = 1*2*3 = 6
        24     -- B(4) = A(1)*A(2)*A(3)*A(4) = 1*2*3*4 = 24
        120     -- B(5) = A(1)*A(2)*A(3)*A(4)*A(5) =1*2*3*4*5 = 120
        [torch.DoubleTensor of size 5]
        
        -- 2. cumulative product for a matrix
        > A = torch.LongTensor{{1, 4, 7}, {2, 5, 8}, {3, 6, 9}}
        > A
        1  4  7
        2  5  8
        3  6  9
        [torch.LongTensor of size 3x3]
        
        > B = torch.cumprod(A)
        > B
        1    4    7
        2   20   56
        6  120  504
        [torch.LongTensor of size 3x3]
        
        -- Why?
        -- B(1, 1) = A(1, 1) = 1
        -- B(2, 1) = A(1, 1)*A(2, 1) = 1*2 = 2
        -- B(3, 1) = A(1, 1)*A(2, 1)*A(3, 1) = 1*2*3 = 6
        -- B(1, 2) = A(1, 2) = 4
        -- B(2, 2) = A(1, 2)*A(2, 2) = 4*5 = 20
        -- B(3, 2) = A(1, 2)*A(2, 2)*A(3, 2) = 4*5*6 = 120
        -- B(1, 3) = A(1, 3) = 7
        -- B(2, 3) = A(1, 3)*A(2, 3) = 7*8 = 56
        -- B(3, 3) = A(1, 3)*A(2, 3)*A(3, 3) = 7*8*9 = 504
        
        -- 3. cumulative product along 2-dim
        > B = torch.cumprod(A, 2)
        > B
        1    4   28
        2   10   80
        3   18  162
        [torch.LongTensor of size 3x3]
        
        -- Why?
        -- B(1, 1) = A(1, 1) = 1
        -- B(1, 2) = A(1, 1)*A(1, 2) = 1*4 = 4
        -- B(1, 3) = A(1, 1)*A(1, 2)*A(1, 3) = 1*4*7 = 28
        -- B(2, 1) = A(2, 1) = 2
        -- B(2, 2) = A(2, 1)*A(2, 2) = 2*5 = 10
        -- B(2, 3) = A(2, 1)*A(2, 2)*A(2, 3) = 2*5*8 = 80
        -- B(3, 1) = A(3, 1) = 3
        -- B(3, 2) = A(3, 1)*A(2, 3) = 3*6 = 18
        -- B(3, 3) = A(3, 1)*A(2, 3)*A(3, 3) = 3*6*9 = 162
        ```
        
        
        
        """

   'torch.cumsum':
      'prefix': 'torchcumsum'
      'body': """
        <a name="torch.cumsum"></a>
        ### [res] torch.cumsum([res,] x [,dim]) ###
        
        `y = torch.cumsum(x)` returns the cumulative sum of the elements of `x`, performing the operation over the first dimension.
        
        `y = torch.cumsum(x, n)` returns the cumulative sum of the elements of `x`, performing the operation over dimension `n`.
        
        
        
        """

   'torch.max':
      'prefix': 'torchmax'
      'body': """
        <a name="torch.max"></a>
        ### torch.max([resval, resind,] x [,dim]) ###
        
        `y = torch.max(x)` returns the single largest element of `x`.
        
        `y, i = torch.max(x, 1)` returns the largest element in each column (across rows) of `x`, and a `Tensor` `i` of their corresponding indices in `x`.
        
        `y, i = torch.max(x, 2)` performs the max operation for each row.
        
        `y, i = torch.max(x, n)` performs the max operation over the dimension `n`.
        
        ```lua
        > x = torch.randn(3, 3)
        > x
        1.1994 -0.6290  0.6888
        -0.0038 -0.0908 -0.2075
        0.3437 -0.9948  0.1216
        [torch.DoubleTensor of size 3x3]
        
        > torch.max(x)
        1.1993977428735
        
        > torch.max(x, 1)
        1.1994 -0.0908  0.6888
        [torch.DoubleTensor of size 1x3]
        
        1  2  1
        [torch.LongTensor of size 1x3]
        
        > torch.max(x, 2)
        1.1994
        -0.0038
        0.3437
        [torch.DoubleTensor of size 3x1]
        
        1
        1
        1
        [torch.LongTensor of size 3x1]
        ```
        
        
        
        """

   'torch.mean':
      'prefix': 'torchmean'
      'body': """
        <a name="torch.mean"></a>
        ### [res] torch.mean([res,] x [,dim]) ###
        
        `y = torch.mean(x)` returns the mean of all elements of `x`.
        
        `y = torch.mean(x, 1)` returns a `Tensor` `y` of the mean of the elements in each column of `x`.
        
        `y = torch.mean(x, 2)` performs the mean operation for each row.
        
        `y = torch.mean(x, n)` performs the mean operation over the dimension `n`.
        
        
        
        """

   'torch.min':
      'prefix': 'torchmin'
      'body': """
        <a name="torch.min"></a>
        ### torch.min([resval, resind,] x [,dim]) ###
        
        `y = torch.min(x)` returns the single smallest element of `x`.
        
        `y, i = torch.min(x, 1)` returns the smallest element in each column (across rows) of `x`, and a `Tensor` `i` of their corresponding indices in `x`.
        
        `y, i = torch.min(x, 2)` performs the min operation for each row.
        
        `y, i = torch.min(x, n)` performs the min operation over the dimension `n`.
        
        
        
        """

   'torch.cmax':
      'prefix': 'torchcmax'
      'body': """
        <a name="torch.cmax"></a>
        ### [res] torch.cmax([res,] tensor1, tensor2) ###
        
        Compute the maximum of each pair of values in `tensor1` and `tensor2`.
        
        `c = torch.cmax(a, b)` returns a new `Tensor` containing the element-wise maximum of `a` and `b`.
        
        `a:cmax(b)` stores the element-wise maximum of `a` and `b` in `a`.
        
        `c:cmax(a, b)` stores the element-wise maximum of `a` and `b` in `c`.
        
        ```lua
        > a = torch.Tensor{1, 2, 3}
        > b = torch.Tensor{3, 2, 1}
        > torch.cmax(a, b)
        3
        2
        3
        [torch.DoubleTensor of size 3]
        ```
        
        
        
        """

   'torch.cmax':
      'prefix': 'torchcmax'
      'body': """
        <a name="torch.cmax"></a>
        ### [res] torch.cmax([res,] tensor, value) ###
        
        Compute the maximum between each value in `tensor` and `value`.
        
        `c = torch.cmax(a, v)` returns a new `Tensor` containing the maxima of each element in `a` and `v`.
        
        `a:cmax(v)` stores the maxima of each element in `a` and `v` in `a`.
        
        `c:cmax(a, v)` stores the maxima of each element in `a` and `v` in `c`.
        
        ```lua
        > a = torch.Tensor{1, 2, 3}
        > torch.cmax(a, 2)
        2
        2
        3
        [torch.DoubleTensor of size 3]
        ```
        
        
        
        """

   'torch.cmin':
      'prefix': 'torchcmin'
      'body': """
        <a name="torch.cmin"></a>
        ### [res] torch.cmin([res,] tensor1, tensor2) ###
        
        Compute the minimum of each pair of values in `tensor1` and `tensor2`.
        
        `c = torch.cmin(a, b)` returns a new `Tensor` containing the element-wise minimum of `a` and `b`.
        
        `a:cmin(b)` stores the element-wise minimum of `a` and `b` in `a`.
        
        `c:cmin(a, b)` stores the element-wise minimum of `a` and `b` in `c`.
        
        ```lua
        > a = torch.Tensor{1, 2, 3}
        > b = torch.Tensor{3, 2, 1}
        > torch.cmin(a, b)
        1
        2
        1
        [torch.DoubleTensor of size 3]
        ```
        
        
        
        """

   'torch.cmin':
      'prefix': 'torchcmin'
      'body': """
        <a name="torch.cmin"></a>
        ### [res] torch.cmin([res,] tensor, value) ###
        
        Compute the minimum between each value in `tensor` and `value`.
        
        `c = torch.cmin(a, v)` returns a new `Tensor` containing the minima of each element in `a` and `v`.
        
        `a:cmin(v)` stores the minima of each element in `a` and `v` in `a`.
        
        `c:cmin(a, v)` stores the minima of each element in `a` and `v` in `c`.
        
        ```lua
        > a = torch.Tensor{1, 2, 3}
        > torch.cmin(a, 2)
        1
        2
        2
        [torch.DoubleTensor of size 3]
        ```
        
        
        
        """

   'torch.median':
      'prefix': 'torchmedian'
      'body': """
        <a name="torch.median"></a>
        ### torch.median([resval, resind,] x [,dim]) ###
        
        `y = torch.median(x)` performs the median operation over the last dimension of `x` (one-before-middle in the case of an even number of elements).
        
        `y, i = torch.median(x, 1)` returns the median element in each column (across rows) of `x`, and a `Tensor` `i` of their corresponding indices in `x`.
        
        `y, i = torch.median(x, 2)` performs the median operation for each row.
        
        `y, i = torch.median(x, n)` performs the median operation over the dimension `n`.
        
        ```lua
        > x = torch.randn(3, 3)
        > x
        0.7860  0.7687 -0.9362
        0.0411  0.5407 -0.3616
        -0.0129 -0.2499 -0.5786
        [torch.DoubleTensor of size 3x3]
        
        > y, i = torch.median(x)
        > y
        0.7687
        0.0411
        -0.2499
        [torch.DoubleTensor of size 3x1]
        
        > i
        2
        1
        2
        [torch.LongTensor of size 3x1]
        
        > y, i = torch.median(x, 1)
        > y
        0.0411  0.5407 -0.5786
        [torch.DoubleTensor of size 1x3]
        
        > i
        2  2  3
        [torch.LongTensor of size 1x3]
        
        > y, i = torch.median(x, 2)
        > y
        0.7687
        0.0411
        -0.2499
        [torch.DoubleTensor of size 3x1]
        
        > i
        2
        1
        2
        [torch.LongTensor of size 3x1]
        ```
        
        
        
        """

   'torch.mode':
      'prefix': 'torchmode'
      'body': """
        <a name="torch.mode"></a>
        ### torch.mode([resval, resind,] x [,dim]) ###
        
        `y = torch.mode(x)` returns the most frequent element of `x` over its last dimension.
        
        `y, i = torch.mode(x, 1)` returns the mode element in each column (across rows) of `x`, and a `Tensor` `i` of their corresponding indices in `x`.
        
        `y, i = torch.mode(x, 2)` performs the mode operation for each row.
        
        `y, i = torch.mode(x, n)` performs the mode operation over the dimension `n`.
        
        
        
        """

   'torch.kthvalue':
      'prefix': 'torchkthvalue'
      'body': """
        <a name="torch.kthvalue"></a>
        ### torch.kthvalue([resval, resind,] x, k [,dim]) ###
        
        `y = torch.kthvalue(x, k)` returns the `k`-th smallest element of `x` over its last dimension.
        
        `y, i = torch.kthvalue(x, k, 1)` returns the `k`-th smallest element in each column (across rows) of `x`, and a `Tensor` `i` of their corresponding indices in `x`.
        
        `y, i = torch.kthvalue(x, k, 2)` performs the `k`-th value operation for each row.
        
        `y, i = torch.kthvalue(x, k, n)` performs the `k`-th value operation over the dimension `n`.
        
        
        
        """

   'torch.prod':
      'prefix': 'torchprod'
      'body': """
        <a name="torch.prod"></a>
        ### [res] torch.prod([res,] x [,n]) ###
        
        `y = torch.prod(x)` returns the product of all elements in `x`.
        
        `y = torch.prod(x, n)` returns a `Tensor` `y` whom size in dimension `n` is 1 and where elements are the product of elements of `x` with respect to dimension `n`.
        
        ```lua
        > a = torch.Tensor{{{1, 2}, {3, 4}}, {{5, 6}, {7, 8}}}
        > a
        (1,.,.) =
        1  2
        3  4
        
        (2,.,.) =
        5  6
        7  8
        [torch.DoubleTensor of dimension 2x2x2]
        
        > torch.prod(a, 1)
        (1,.,.) =
        5  12
        21  32
        [torch.DoubleTensor of dimension 1x2x2]
        
        > torch.prod(a, 2)
        (1,.,.) =
        3   8
        
        (2,.,.) =
        35  48
        [torch.DoubleTensor of size 2x1x2]
        
        > torch.prod(a, 3)
        (1,.,.) =
        2
        12
        
        (2,.,.) =
        30
        56
        [torch.DoubleTensor of size 2x2x1]
        ```
        
        
        
        """

   'torch.sort':
      'prefix': 'torchsort'
      'body': """
        <a name="torch.sort"></a>
        ### torch.sort([resval, resind,] x [,d] [,flag]) ###
        
        `y, i = torch.sort(x)` returns a `Tensor` `y` where all entries are sorted along the last dimension, in **ascending** order.
        It also returns a `Tensor` `i` that provides the corresponding indices from `x`.
        
        `y, i = torch.sort(x, d)` performs the sort operation along a specific dimension `d`.
        
        `y, i = torch.sort(x)` is therefore equivalent to `y, i = torch.sort(x, x:dim())`
        
        `y, i = torch.sort(x, d, true)` performs the sort operation along a specific dimension `d`, in **descending** order.
        
        ```lua
        > x = torch.randn(3, 3)
        > x
        -1.2470 -0.4288 -0.5337
        0.8836 -0.1622  0.9604
        0.6297  0.2397  0.0746
        [torch.DoubleTensor of size 3x3]
        
        > torch.sort(x)
        -1.2470 -0.5337 -0.4288
        -0.1622  0.8836  0.9604
        0.0746  0.2397  0.6297
        [torch.DoubleTensor of size 3x3]
        
        1  3  2
        2  1  3
        3  2  1
        [torch.LongTensor of size 3x3]
        ```
        
        
        
        """

   'torch.std':
      'prefix': 'torchstd'
      'body': """
        <a name="torch.std"></a>
        ### [res] torch.std([res,] x, [,dim] [,flag]) ###
        
        `y = torch.std(x)` returns the standard deviation of the elements of `x`.
        
        `y = torch.std(x, dim)` performs the `std` operation over the dimension `dim`.
        
        `y = torch.std(x, dim, false)` performs the `std` operation normalizing by `n-1` (this is the default).
        
        `y = torch.std(x, dim, true)` performs the `std` operation normalizing by `n` instead of `n-1`.
        
        
        
        """

   'torch.sum':
      'prefix': 'torchsum'
      'body': """
        <a name="torch.sum"></a>
        ### [res] torch.sum([res,] x) ###
        
        `y = torch.sum(x)` returns the sum of the elements of `x`.
        
        `y = torch.sum(x, 2)` performs the sum operation for each row.
        
        `y = torch.sum(x, n)` performs the sum operation over the dimension `n`.
        
        
        
        """

   'torch.var':
      'prefix': 'torchvar'
      'body': """
        <a name="torch.var"></a>
        ### [res] torch.var([res,] x [,dim] [,flag]) ###
        
        `y = torch.var(x)` returns the variance of the elements of `x`.
        
        `y = torch.var(x, dim)` performs the `var` operation over the dimension `dim`.
        
        `y = torch.var(x, dim, false)` performs the `var` operation normalizing by `n-1` (this is the default).
        
        `y = torch.var(x, dim, true)` performs the `var` operation normalizing by `n` instead of `n-1`.
        
        
        
        """

   'torch.matrixwide.dok':
      'prefix': 'torchmatrixwidedok'
      'body': """
        <a name="torch.matrixwide.dok"></a>
        ## Matrix-wide operations  (`Tensor`-wide operations) ##
        
        Note that many of the operations in [dimension-wise operations](#torch.columnwise.dok) can also be used as matrix-wide operations, by just omitting the `dim` parameter.
        
        
        
        """

   'torch.norm':
      'prefix': 'torchnorm'
      'body': """
        <a name="torch.norm"></a>
        ### torch.norm(x [,p] [,dim]) ###
        
        `y = torch.norm(x)` returns the `2`-norm of the `Tensor` `x`.
        
        `y = torch.norm(x, p)` returns the `p`-norm of the `Tensor` `x`.
        
        `y = torch.norm(x, p, dim)` returns the `p`-norms of the `Tensor` `x` computed over the dimension `dim`.
        
        
        
        """

   'torch.renorm':
      'prefix': 'torchrenorm'
      'body': """
        <a name="torch.renorm"></a>
        ### torch.renorm([res], x, p, dim, maxnorm) ###
        
        Renormalizes the sub-`Tensor`s along dimension `dim` such that they exceed norm `maxnorm`.
        
        `y = torch.renorm(x, p, dim, maxnorm)` returns a version of `x` with `p`-norms lower than `maxnorm` over non-`dim` dimensions.
        The `dim` argument is not to be confused with the argument of the same name in function [`norm`](#torch.norm).
        In this case, the `p`-norm is measured for each `i`-th sub-`Tensor` `x:select(dim, i)`.
        This function is equivalent to (but faster than) the following:
        
        ```lua
        function renorm(matrix, value, dim, maxnorm)
        local m1 = matrix:transpose(dim, 1):contiguous()
        -- collapse non-dim dimensions:
        m2 = m1:reshape(m1:size(1), m1:nElement()/m1:size(1))
        local norms = m2:norm(value, 2)
        -- clip
        local new_norms = norms:clone()
        new_norms[torch.gt(norms, maxnorm)] = maxnorm
        new_norms:cdiv(norms:add(1e-7))
        -- renormalize
        m1:cmul(new_norms:expandAs(m1))
        return m1:transpose(dim, 1)
        end
        ```
        
        `x:renorm(p, dim, maxnorm)` returns the equivalent of `x:copy(torch.renorm(x, p, dim, maxnorm))`.
        
        Note: this function is particularly useful as a regularizer for constraining the norm of parameter `Tensor`s.
        See [Hinton et al. 2012, p. 2](http://arxiv.org/pdf/1207.0580.pdf).
        
        
        
        """

   'torch.dist':
      'prefix': 'torchdist'
      'body': """
        <a name="torch.dist"></a>
        ### torch.dist(x, y) ###
        
        `y = torch.dist(x, y)` returns the `2`-norm of `x - y`.
        
        `y = torch.dist(x, y, p)` returns the `p`-norm of `x - y`.
        
        
        
        """

   'torch.numel':
      'prefix': 'torchnumel'
      'body': """
        <a name="torch.numel"></a>
        ### torch.numel(x) ###
        
        `y = torch.numel(x)` returns the count of the number of elements in the matrix `x`.
        
        
        
        """

   'torch.trace':
      'prefix': 'torchtrace'
      'body': """
        <a name="torch.trace"></a>
        ### torch.trace(x) ###
        
        `y = torch.trace(x)` returns the trace (sum of the diagonal elements) of a matrix `x`.
        This is equal to the sum of the eigenvalues of `x`.
        The returned value `y` is a number, not a `Tensor`.
        
        
        
        """

   'torch.conv.dok':
      'prefix': 'torchconvdok'
      'body': """
        <a name="torch.conv.dok"></a>
        ## Convolution Operations ##
        
        These functions implement convolution or cross-correlation of an input image (or set of input images) with a kernel (or set of kernels).
        The convolution function in Torch can handle different types of input/kernel dimensions and produces corresponding outputs.
        The general form of operations always remain the same.
        
        
        
        """

   'torch.conv2':
      'prefix': 'torchconv2'
      'body': """
        <a name="torch.conv2"></a>
        ### [res] torch.conv2([res,] x, k, [, 'F' or 'V']) ###
        
        """

   'torch.conv2':
      'prefix': 'torchconv2'
      'body': """
        <a name="torch.conv2"></a>
        
        This function computes 2 dimensional convolutions between `x` and `k`.
        These operations are similar to BLAS operations when number of dimensions of input and kernel are reduced by `2`.
        
        * `x`  and `k` are 2D: convolution of a single image with a single kernel (2D output). This operation is similar to multiplication of two scalars.
        * `x` (`p  m  n`)  and `k` (`p  ki  kj`) are 3D: convolution of each input slice with corresponding kernel (3D output).
        * `x` (`p  m  n`) 3D, `k` (`q  p  ki  kj`) 4D: convolution of all input slices with the corresponding slice of kernel. Output is 3D (`q  m  n`). This operation is similar to matrix vector product of matrix `k` and vector `x`.
        
        The last argument controls if the convolution is a full (`'F'`) or valid (`'V'`) convolution.
        The default is **valid** convolution.
        
        ```lua
        x = torch.rand(100, 100)
        k = torch.rand(10, 10)
        c = torch.conv2(x, k)
        > c:size()
        91
        91
        [torch.LongStorage of size 2]
        
        c = torch.conv2(x, k, 'F')
        > c:size()
        109
        109
        [torch.LongStorage of size 2]
        ```
        
        
        
        """

   'torch.xcorr2':
      'prefix': 'torchxcorr2'
      'body': """
        <a name="torch.xcorr2"></a>
        ### [res] torch.xcorr2([res,] x, k, [, 'F' or 'V']) ###
        
        """

   'torch.xcorr2':
      'prefix': 'torchxcorr2'
      'body': """
        <a name="torch.xcorr2"></a>
        
        This function operates with same options and input/output configurations as [`torch.conv2`](#torch.conv2), but performs cross-correlation of the input with the kernel `k`.
        
        
        
        """

   'torch.conv3':
      'prefix': 'torchconv3'
      'body': """
        <a name="torch.conv3"></a>
        ### [res] torch.conv3([res,] x, k, [, 'F' or 'V']) ###
        
        """

   'torch.conv3':
      'prefix': 'torchconv3'
      'body': """
        <a name="torch.conv3"></a>
        
        This function computes 3 dimensional convolutions between `x` and `k`.
        These operations are similar to BLAS operations when number of dimensions of input and kernel are reduced by `3`.
        
        * `x`  and `k` are 3D: convolution of a single image with a single kernel (3D output). This operation is similar to multiplication of two scalars.
        * `x` (`p  m  n  o`)  and `k` (`p  ki  kj  kk`) are 4D: convolution of each input slice with corresponding kernel (4D output).
        * `x` (`p  m  n  o`) 4D, `k` (`q  p  ki  kj  kk`) 5D: convolution of all input slices with the corresponding slice of kernel. Output is 4D `q  m  n  o`. This operation is similar to matrix vector product of matrix `k` and vector `x`.
        
        The last argument controls if the convolution is a full (`'F'`) or valid (`'V'`) convolution.
        The default is **valid** convolution.
        
        ```lua
        x = torch.rand(100, 100, 100)
        k = torch.rand(10, 10, 10)
        c = torch.conv3(x, k)
        > c:size()
        91
        91
        91
        [torch.LongStorage of size 3]
        
        c = torch.conv3(x, k, 'F')
        > c:size()
        109
        109
        109
        [torch.LongStorage of size 3]
        
        ```
        
        
        
        """

   'torch.xcorr3':
      'prefix': 'torchxcorr3'
      'body': """
        <a name="torch.xcorr3"></a>
        ### [res] torch.xcorr3([res,] x, k, [, 'F' or 'V']) ###
        
        """

   'torch.xcorr3':
      'prefix': 'torchxcorr3'
      'body': """
        <a name="torch.xcorr3"></a>
        
        This function operates with same options and input/output configurations as [`torch.conv3`](#torch.conv3), but performs cross-correlation of the input with the kernel `k`.
        
        
        
        """

   'torch.linalg.dok':
      'prefix': 'torchlinalgdok'
      'body': """
        <a name="torch.linalg.dok"></a>
        ## Eigenvalues, SVD, Linear System Solution ##
        
        Functions in this section are implemented with an interface to [LAPACK](https://en.wikipedia.org/wiki/LAPACK) libraries.
        If LAPACK libraries are not found during compilation step, then these functions will not be available.
        
        
        
        """

   'torch.gesv':
      'prefix': 'torchgesv'
      'body': """
        <a name="torch.gesv"></a>
        ### [x, lu] torch.gesv([resb, resa,] B, A) ###
        
        `X, LU = torch.gesv(B, A)` returns the solution of `AX = B` and `LU` contains `L` and `U` factors for `LU` factorization of `A`.
        
        `A` has to be a square and non-singular matrix (2D `Tensor`).
        `A` and `LU` are `m  m`, `X` is `m  k` and `B` is `m  k`.
        
        If `resb` and `resa` are given, then they will be used for temporary storage and returning the result.
        
        * `resa` will contain `L` and `U` factors for `LU` factorization of `A`.
        * `resb` will contain the solution `X`.
        
        Note: Irrespective of the original strides, the returned matrices `resb` and `resa` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > a = torch.Tensor({{6.80, -2.11,  5.66,  5.97,  8.23},
        {-6.05, -3.30,  5.36, -4.44,  1.08},
        {-0.45,  2.58, -2.70,  0.27,  9.04},
        {8.32,  2.71,  4.35,  -7.17,  2.14},
        {-9.67, -5.14, -7.26,  6.08, -6.87}}):t()
        
        > b = torch.Tensor({{4.02,  6.19, -8.22, -7.57, -3.03},
        {-1.56,  4.00, -8.67,  1.75,  2.86},
        {9.81, -4.09, -4.57, -8.61,  8.99}}):t()
        
        > b
        4.0200 -1.5600  9.8100
        6.1900  4.0000 -4.0900
        -8.2200 -8.6700 -4.5700
        -7.5700  1.7500 -8.6100
        -3.0300  2.8600  8.9900
        [torch.DoubleTensor of dimension 5x3]
        
        > a
        6.8000 -6.0500 -0.4500  8.3200 -9.6700
        -2.1100 -3.3000  2.5800  2.7100 -5.1400
        5.6600  5.3600 -2.7000  4.3500 -7.2600
        5.9700 -4.4400  0.2700 -7.1700  6.0800
        8.2300  1.0800  9.0400  2.1400 -6.8700
        [torch.DoubleTensor of dimension 5x5]
        
        
        > x = torch.gesv(b, a)
        > x
        -0.8007 -0.3896  0.9555
        -0.6952 -0.5544  0.2207
        0.5939  0.8422  1.9006
        1.3217 -0.1038  5.3577
        0.5658  0.1057  4.0406
        [torch.DoubleTensor of dimension 5x3]
        
        > b:dist(a * x)
        1.1682163181673e-14
        ```
        
        
        
        """

   'torch.trtrs':
      'prefix': 'torchtrtrs'
      'body': """
        <a name="torch.trtrs"></a>
        ### [x] torch.trtrs([resb, resa,] b, a [, 'U' or 'L'] [, 'N' or 'T'] [, 'N' or 'U']) ###
        
        `X = torch.trtrs(B, A)` returns the solution of `AX = B` where `A` is upper-triangular.
        
        `A` has to be a square, triangular, non-singular matrix (2D `Tensor`).
        `A` and `resa` are `m  m`, `X` and `B` are `m  k`.
        (To be very precise: `A` does not have to be triangular and non-singular, rather only its upper or lower triangle will be taken into account and that part has to be non-singular.)
        
        The function has several options:
        
        * `uplo` (`'U'` or `'L'`) specifies whether `A` is upper or lower triangular; the default value is `'U'`.
        * `trans` (`'N'` or `'T`') specifies the system of equations: `'N'` for `A * X = B` (no transpose), or `'T'` for `A^T * X = B` (transpose); the default value is `'N'`.
        * `diag` (`'N'` or `'U'`) `'U'` specifies that `A` is unit triangular, i.e., it has ones on its diagonal; `'N'` specifies that `A` is not (necessarily) unit triangular; the default value is `'N'`.
        
        If `resb` and `resa` are given, then they will be used for temporary storage and returning the result.
        `resb` will contain the solution `X`.
        
        Note: Irrespective of the original strides, the returned matrices `resb` and `resa` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > a = torch.Tensor({{6.80, -2.11,  5.66,  5.97,  8.23},
        {0, -3.30,  5.36, -4.44,  1.08},
        {0,  0, -2.70,  0.27,  9.04},
        {0,  0,  0,  -7.17,  2.14},
        {0,  0,  0,  0, -6.87}})
        
        > b = torch.Tensor({{4.02,  6.19, -8.22, -7.57, -3.03},
        {-1.56,  4.00, -8.67,  1.75,  2.86},
        {9.81, -4.09, -4.57, -8.61,  8.99}}):t()
        
        > b
        4.0200 -1.5600  9.8100
        6.1900  4.0000 -4.0900
        -8.2200 -8.6700 -4.5700
        -7.5700  1.7500 -8.6100
        -3.0300  2.8600  8.9900
        [torch.DoubleTensor of dimension 5x3]
        
        > a
        6.8000 -2.1100  5.6600  5.9700  8.2300
        0.0000 -3.3000  5.3600 -4.4400  1.0800
        0.0000  0.0000 -2.7000  0.2700  9.0400
        0.0000  0.0000  0.0000 -7.1700  2.1400
        0.0000  0.0000  0.0000  0.0000 -6.8700
        [torch.DoubleTensor of dimension 5x5]
        
        > x = torch.trtrs(b, a)
        > x
        -3.5416 -0.2514  3.0847
        4.2072  2.0391 -4.5146
        4.6399  1.7804 -2.6077
        1.1874 -0.3683  0.8103
        0.4410 -0.4163 -1.3086
        [torch.DoubleTensor of size 5x3]
        
        > b:dist(a*x)
        4.1895292266754e-15
        ```
        
        
        
        """

   'torch.potrf':
      'prefix': 'torchpotrf'
      'body': """
        <a name="torch.potrf"></a>
        ### torch.potrf([res,] A [, 'U' or 'L'] ) ###
        
        Cholesky Decomposition of 2D `Tensor` `A`.
        The matrix `A` has to be a positive-definite and either symmetric or complex Hermitian.
        
        The factorization has the form
        
        A = U**T * U,   if UPLO = 'U', or
        A = L  * L**T,  if UPLO = 'L',
        
        where `U` is an upper triangular matrix and `L` is lower triangular.
        
        The optional character `uplo` = {'U', 'L'} specifies whether the upper or lower triangulardecomposition should be returned. By default, `uplo` = 'U'.
        
        `U = torch.potrf(A, 'U')` returns the upper triangular Cholesky decomposition of `A`.
        
        `L = torch.potrf(A, 'L')` returns the lower triangular Cholesky decomposition of `A`.
        
        If `Tensor` `res` is provided, the resulting decomposition will be stored therein.
        
        ```lua
        > A = torch.Tensor({
        {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
        {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
        {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
        {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
        {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})
        
        > chol = torch.potrf(A)
        > chol
        1.1272  0.8846  0.4390  0.1232  0.2112
        0.0000  0.4626  0.6200 -0.0874 -0.1453
        0.0000  0.0000  0.7525  0.0419  0.0738
        0.0000  0.0000  0.0000  0.0491  0.2199
        0.0000  0.0000  0.0000  0.0000  0.5255
        [torch.DoubleTensor of size 5x5]
        
        > torch.potrf(chol, A, 'L')
        > chol
        1.1272  0.0000  0.0000  0.0000  0.0000
        0.8846  0.4626  0.0000  0.0000  0.0000
        0.4390  0.6200  0.7525  0.0000  0.0000
        0.1232 -0.0874  0.0419  0.0491  0.0000
        0.2112 -0.1453  0.0738  0.2199  0.5255
        [torch.DoubleTensor of size 5x5]
        ```
        
        
        """

   'torch.pstrf':
      'prefix': 'torchpstrf'
      'body': """
        <a name="torch.pstrf"></a>
        ### torch.pstrf([res, piv, ] A [, 'U' or 'L'] ) ###
        
        Cholesky factorization with complete pivoting of a real symmetric positive semidefinite 2D `Tensor` `A`.
        The matrix `A` has to be a positive semi-definite and symmetric. The factorization has the form
        
        P**T * A * P = U**T * U ,  if UPLO = 'U',
        P**T * A * P = L  * L**T,  if UPLO = 'L',
        
        where `U` is an upper triangular matrix and `L` is lower triangular, and
        `P` is stored as the vector `piv`. More specifically, `piv` is such that the nonzero entries are `P[piv[k], k] = 1`.
        
        The optional character argument `uplo` = {'U', 'L'} specifies whether the upper or lower triangular decomposition should be returned. By default, `uplo` = 'U'.
        
        `U, piv = torch.sdtrf(A, 'U')` returns the upper triangular Cholesky decomposition of `A`
        
        `L, piv = torch.potrf(A, 'L')` returns the lower triangular Cholesky decomposition of `A`.
        
        If tensors `res` and `piv` (an `IntTensor`) are provided, the resulting decomposition will be stored therein.
        
        ```lua
        > A = torch.Tensor({
        {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
        {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
        {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
        {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
        {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})
        
        > U, piv = torch.pstrf(A)
        > U
        1.1272  0.4390  0.2112  0.8846  0.1232
        0.0000  0.9750 -0.0354  0.2942 -0.0233
        0.0000  0.0000  0.5915 -0.0961  0.0435
        0.0000  0.0000  0.0000  0.3439 -0.0854
        0.0000  0.0000  0.0000  0.0000  0.0456
        [torch.DoubleTensor of size 5x5]
        
        > piv
        1
        3
        5
        2
        4
        [torch.IntTensor of size 5]
        
        > Ap = U:t() * U
        > Ap
        1.2705  0.4948  0.2381  0.9971  0.1389
        0.4948  1.1434  0.0582  0.6752  0.0314
        0.2381  0.0582  0.3957  0.1196  0.0526
        0.9971  0.6752  0.1196  0.9966  0.0686
        0.1389  0.0314  0.0526  0.0686  0.0270
        [torch.DoubleTensor of size 5x5]
        
        > -- Permute rows and columns
        > Ap:indexCopy(1, piv:long(), Ap:clone())
        > Ap:indexCopy(2, piv:long(), Ap:clone())
        > (Ap - A):norm()
        1.5731560566382e-16
        ```
        
        
        """

   'torch.potrs':
      'prefix': 'torchpotrs'
      'body': """
        <a name="torch.potrs"></a>
        ### torch.potrs([res,] B, chol [, 'U' or 'L'] ) ###
        
        Returns the solution to linear system `AX = B` using the Cholesky decomposition `chol` of 2D `Tensor` `A`.
        
        Square matrix `chol` should be triangular; and, righthand side matrix `B` should be of full rank.
        
        Optional character `uplo` = {'U', 'L'} specifies matrix `chol` as either upper or lower triangular; and, by default, equals 'U'.
        
        If `Tensor` `res` is provided, the resulting decomposition will be stored therein.
        
        ```lua
        > A = torch.Tensor({
        {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
        {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
        {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
        {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
        {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})
        
        > B = torch.Tensor({
        {0.6219,  0.3439,  0.0431},
        {0.5642,  0.1756,  0.0153},
        {0.2334,  0.8594,  0.4103},
        {0.7556,  0.1966,  0.9637},
        {0.1420,  0.7185,  0.7476}})
        
        > chol = torch.potrf(A)
        > chol
        1.1272  0.8846  0.4390  0.1232  0.2112
        0.0000  0.4626  0.6200 -0.0874 -0.1453
        0.0000  0.0000  0.7525  0.0419  0.0738
        0.0000  0.0000  0.0000  0.0491  0.2199
        0.0000  0.0000  0.0000  0.0000  0.5255
        [torch.DoubleTensor of size 5x5]
        
        > solve = torch.potrs(B, chol)
        > solve
        12.1945   61.8622   92.6882
        -11.1782  -97.0303 -138.4874
        -15.3442  -76.6562 -116.8218
        6.1930   13.5238   25.2056
        29.9678  251.7346  360.2301
        [torch.DoubleTensor of size 5x3]
        
        > A*solve
        0.6219  0.3439  0.0431
        0.5642  0.1756  0.0153
        0.2334  0.8594  0.4103
        0.7556  0.1966  0.9637
        0.1420  0.7185  0.7476
        [torch.DoubleTensor of size 5x3]
        
        > B:dist(A*solve)
        4.6783066076306e-14
        ```
        
        
        
        """

   'torch.potri':
      'prefix': 'torchpotri'
      'body': """
        <a name="torch.potri"></a>
        ### torch.potri([res,] chol [, 'U' or 'L'] ) ###
        
        Returns the inverse of 2D `Tensor` `A` given its Cholesky decomposition `chol`.
        
        Square matrix `chol` should be triangular.
        
        Optional character `uplo` = {'U', 'L'} specifies matrix `chol` as either upper or lower triangular; and, by default, equals 'U'.
        
        If `Tensor` `res` is provided, the resulting inverse will be stored therein.
        
        ```lua
        > A = torch.Tensor({
        {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
        {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
        {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
        {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
        {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})
        
        > chol = torch.potrf(A)
        > chol
        1.1272  0.8846  0.4390  0.1232  0.2112
        0.0000  0.4626  0.6200 -0.0874 -0.1453
        0.0000  0.0000  0.7525  0.0419  0.0738
        0.0000  0.0000  0.0000  0.0491  0.2199
        0.0000  0.0000  0.0000  0.0000  0.5255
        [torch.DoubleTensor of size 5x5]
        
        > inv = torch.potri(chol)
        > inv
        42.2781  -39.0824    8.3019 -133.4998    2.8980
        -39.0824   38.1222   -8.7468  119.4247   -2.5944
        8.3019   -8.7468    3.1104  -25.1405    0.5327
        -133.4998  119.4247  -25.1405  480.7511  -15.9747
        2.8980   -2.5944    0.5327  -15.9747    3.6127
        [torch.DoubleTensor of size 5x5]
        
        > inv:dist(torch.inverse(A))
        2.8525852877633e-12
        ```
        
        
        
        """

   'torch.gels':
      'prefix': 'torchgels'
      'body': """
        <a name="torch.gels"></a>
        ### torch.gels([resb, resa,] b, a) ###
        
        Solution of least squares and least norm problems for a full rank `m  n` matrix `A`.
        
        * If `n  m`, then solve `||AX-B||_F`.
        * If `n > m` , then solve `min ||X||_F` s.t. `AX = B`.
        
        On return, first `n` rows of `x` matrix contains the solution and the rest contains residual information.
        Square root of sum squares of elements of each column of `x` starting at row `n + 1` is the residual for corresponding column.
        
        Note: Irrespective of the original strides, the returned matrices `resb` and `resa` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > a = torch.Tensor({{ 1.44, -9.96, -7.55,  8.34,  7.08, -5.45},
        {-7.84, -0.28,  3.24,  8.09,  2.52, -5.70},
        {-4.39, -3.24,  6.27,  5.28,  0.74, -1.19},
        {4.53,  3.83, -6.64,  2.06, -2.47,  4.70}}):t()
        
        > b = torch.Tensor({{8.58,  8.26,  8.48, -5.28,  5.72,  8.93},
        {9.35, -4.43, -0.70, -0.26, -7.36, -2.52}}):t()
        
        > a
        1.4400 -7.8400 -4.3900  4.5300
        -9.9600 -0.2800 -3.2400  3.8300
        -7.5500  3.2400  6.2700 -6.6400
        8.3400  8.0900  5.2800  2.0600
        7.0800  2.5200  0.7400 -2.4700
        -5.4500 -5.7000 -1.1900  4.7000
        [torch.DoubleTensor of dimension 6x4]
        
        > b
        8.5800  9.3500
        8.2600 -4.4300
        8.4800 -0.7000
        -5.2800 -0.2600
        5.7200 -7.3600
        8.9300 -2.5200
        [torch.DoubleTensor of dimension 6x2]
        
        > x = torch.gels(b, a)
        > x
        -0.4506   0.2497
        -0.8492  -0.9020
        0.7066   0.6323
        0.1289   0.1351
        13.1193  -7.4922
        -4.8214  -7.1361
        [torch.DoubleTensor of dimension 6x2]
        
        > b:dist(a*x:narrow(1, 1, 4))
        17.390200628863
        
        > math.sqrt(x:narrow(1, 5, 2):pow(2):sumall())
        17.390200628863
        ```
        
        
        
        """

   'torch.symeig':
      'prefix': 'torchsymeig'
      'body': """
        <a name="torch.symeig"></a>
        ### torch.symeig([rese, resv,] a [, 'N' or 'V'] [, 'U' or 'L']) ###
        
        `e, V = torch.symeig(A)` returns eigenvalues and eigenvectors of a symmetric real matrix `A`.
        
        `A` and `V` are `m  m` matrices and `e` is a `m` dimensional vector.
        
        This function calculates all eigenvalues (and vectors) of `A` such that `A = V' diag(e) V`.
        
        Third argument defines computation of eigenvectors or eigenvalues only.
        If it is `'N'`, only eigenvalues are computed.
        If it is `'V'`, both eigenvalues and eigenvectors are computed.
        
        Since the input matrix `A` is supposed to be symmetric, only upper triangular portion is used by default.
        If the 4th argument is `'L'`, then lower triangular portion is used.
        
        Note: Irrespective of the original strides, the returned matrix `V` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > a = torch.Tensor({{ 1.96,  0.00,  0.00,  0.00,  0.00},
        {-6.49,  3.80,  0.00,  0.00,  0.00},
        {-0.47, -6.39,  4.17,  0.00,  0.00},
        {-7.20,  1.50, -1.51,  5.70,  0.00},
        {-0.65, -6.34,  2.67,  1.80, -7.10}}):t()
        
        > a
        1.9600 -6.4900 -0.4700 -7.2000 -0.6500
        0.0000  3.8000 -6.3900  1.5000 -6.3400
        0.0000  0.0000  4.1700 -1.5100  2.6700
        0.0000  0.0000  0.0000  5.7000  1.8000
        0.0000  0.0000  0.0000  0.0000 -7.1000
        [torch.DoubleTensor of dimension 5x5]
        
        > e = torch.symeig(a)
        > e
        -11.0656
        -6.2287
        0.8640
        8.8655
        16.0948
        [torch.DoubleTensor of dimension 5]
        
        > e, v = torch.symeig(a, 'V')
        > e
        -11.0656
        -6.2287
        0.8640
        8.8655
        16.0948
        [torch.DoubleTensor of dimension 5]
        
        > v
        -0.2981 -0.6075  0.4026 -0.3745  0.4896
        -0.5078 -0.2880 -0.4066 -0.3572 -0.6053
        -0.0816 -0.3843 -0.6600  0.5008  0.3991
        -0.0036 -0.4467  0.4553  0.6204 -0.4564
        -0.8041  0.4480  0.1725  0.3108  0.1622
        [torch.DoubleTensor of dimension 5x5]
        
        > v*torch.diag(e)*v:t()
        1.9600 -6.4900 -0.4700 -7.2000 -0.6500
        -6.4900  3.8000 -6.3900  1.5000 -6.3400
        -0.4700 -6.3900  4.1700 -1.5100  2.6700
        -7.2000  1.5000 -1.5100  5.7000  1.8000
        -0.6500 -6.3400  2.6700  1.8000 -7.1000
        [torch.DoubleTensor of dimension 5x5]
        
        > a:dist(torch.triu(v*torch.diag(e)*v:t()))
        1.0219480822443e-14
        ```
        
        
        
        """

   'torch.eig':
      'prefix': 'torcheig'
      'body': """
        <a name="torch.eig"></a>
        ### torch.eig([rese, resv,] a [, 'N' or 'V']) ###
        
        `e, V = torch.eig(A)` returns eigenvalues and eigenvectors of a general real square matrix `A`.
        
        `A` and `V` are `m  m` matrices and `e` is a `m` dimensional vector.
        
        This function calculates all right eigenvalues (and vectors) of `A` such that `A = V' diag(e) V`.
        
        Third argument defines computation of eigenvectors or eigenvalues only.
        If it is `'N'`, only eigenvalues are computed.
        If it is `'V'`, both eigenvalues and eigenvectors are computed.
        
        The eigen values returned follow [LAPACK convention](https://software.intel.com/sites/products/documentation/hpc/mkl/mklman/GUID-16EB5901-5644-4DA6-A332-A052309010C4.htm) and are returned as complex (real/imaginary) pairs of numbers (`2 * m` dimensional `Tensor`).
        
        Note: Irrespective of the original strides, the returned matrix `V` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > a = torch.Tensor({{ 1.96,  0.00,  0.00,  0.00,  0.00},
        {-6.49,  3.80,  0.00,  0.00,  0.00},
        {-0.47, -6.39,  4.17,  0.00,  0.00},
        {-7.20,  1.50, -1.51,  5.70,  0.00},
        {-0.65, -6.34,  2.67,  1.80, -7.10}}):t()
        
        > a
        1.9600 -6.4900 -0.4700 -7.2000 -0.6500
        0.0000  3.8000 -6.3900  1.5000 -6.3400
        0.0000  0.0000  4.1700 -1.5100  2.6700
        0.0000  0.0000  0.0000  5.7000  1.8000
        0.0000  0.0000  0.0000  0.0000 -7.1000
        [torch.DoubleTensor of dimension 5x5]
        
        > b = a + torch.triu(a, 1):t()
        > b
        
        1.9600 -6.4900 -0.4700 -7.2000 -0.6500
        -6.4900  3.8000 -6.3900  1.5000 -6.3400
        -0.4700 -6.3900  4.1700 -1.5100  2.6700
        -7.2000  1.5000 -1.5100  5.7000  1.8000
        -0.6500 -6.3400  2.6700  1.8000 -7.1000
        [torch.DoubleTensor of dimension 5x5]
        
        > e = torch.eig(b)
        > e
        16.0948   0.0000
        -11.0656   0.0000
        -6.2287   0.0000
        0.8640   0.0000
        8.8655   0.0000
        [torch.DoubleTensor of dimension 5x2]
        
        > e, v = torch.eig(b, 'V')
        > e
        16.0948   0.0000
        -11.0656   0.0000
        -6.2287   0.0000
        0.8640   0.0000
        8.8655   0.0000
        [torch.DoubleTensor of dimension 5x2]
        
        > v
        -0.4896  0.2981 -0.6075 -0.4026 -0.3745
        0.6053  0.5078 -0.2880  0.4066 -0.3572
        -0.3991  0.0816 -0.3843  0.6600  0.5008
        0.4564  0.0036 -0.4467 -0.4553  0.6204
        -0.1622  0.8041  0.4480 -0.1725  0.3108
        [torch.DoubleTensor of dimension 5x5]
        
        > v * torch.diag(e:select(2, 1))*v:t()
        1.9600 -6.4900 -0.4700 -7.2000 -0.6500
        -6.4900  3.8000 -6.3900  1.5000 -6.3400
        -0.4700 -6.3900  4.1700 -1.5100  2.6700
        -7.2000  1.5000 -1.5100  5.7000  1.8000
        -0.6500 -6.3400  2.6700  1.8000 -7.1000
        [torch.DoubleTensor of dimension 5x5]
        
        > b:dist(v * torch.diag(e:select(2, 1)) * v:t())
        3.5423944346685e-14
        ```
        
        
        
        """

   'torch.svd':
      'prefix': 'torchsvd'
      'body': """
        <a name="torch.svd"></a>
        ### torch.svd([resu, ress, resv,] a [, 'S' or 'A']) ###
        
        `U, S, V = torch.svd(A)` returns the singular value decomposition of a real matrix `A` of size `n  m` such that `A = USV'*`.
        
        `U` is `n  n`, `S` is `n  m` and `V` is `m  m`.
        
        The last argument, if it is string, represents the number of singular values to be computed.
        `'S'` stands for *some* and `'A'` stands for *all*.
        
        Note: Irrespective of the original strides, the returned matrix `U` will be transposed, i.e. with strides `1, n` instead of `n, 1`.
        
        ```lua
        > a = torch.Tensor({{8.79,  6.11, -9.15,  9.57, -3.49,  9.84},
        {9.93,  6.91, -7.93,  1.64,  4.02,  0.15},
        {9.83,  5.04,  4.86,  8.83,  9.80, -8.99},
        {5.45, -0.27,  4.85,  0.74, 10.00, -6.02},
        {3.16,  7.98,  3.01,  5.80,  4.27, -5.31}}):t()
        
        > a
        8.7900   9.9300   9.8300   5.4500   3.1600
        6.1100   6.9100   5.0400  -0.2700   7.9800
        -9.1500  -7.9300   4.8600   4.8500   3.0100
        9.5700   1.6400   8.8300   0.7400   5.8000
        -3.4900   4.0200   9.8000  10.0000   4.2700
        9.8400   0.1500  -8.9900  -6.0200  -5.3100
        
        > u, s, v = torch.svd(a)
        > u
        -0.5911  0.2632  0.3554  0.3143  0.2299
        -0.3976  0.2438 -0.2224 -0.7535 -0.3636
        -0.0335 -0.6003 -0.4508  0.2334 -0.3055
        -0.4297  0.2362 -0.6859  0.3319  0.1649
        -0.4697 -0.3509  0.3874  0.1587 -0.5183
        0.2934  0.5763 -0.0209  0.3791 -0.6526
        [torch.DoubleTensor of dimension 6x5]
        
        > s
        27.4687
        22.6432
        8.5584
        5.9857
        2.0149
        [torch.DoubleTensor of dimension 5]
        
        > v
        -0.2514  0.8148 -0.2606  0.3967 -0.2180
        -0.3968  0.3587  0.7008 -0.4507  0.1402
        -0.6922 -0.2489 -0.2208  0.2513  0.5891
        -0.3662 -0.3686  0.3859  0.4342 -0.6265
        -0.4076 -0.0980 -0.4933 -0.6227 -0.4396
        [torch.DoubleTensor of dimension 5x5]
        
        > u * torch.diag(s) * v:t()
        8.7900   9.9300   9.8300   5.4500   3.1600
        6.1100   6.9100   5.0400  -0.2700   7.9800
        -9.1500  -7.9300   4.8600   4.8500   3.0100
        9.5700   1.6400   8.8300   0.7400   5.8000
        -3.4900   4.0200   9.8000  10.0000   4.2700
        9.8400   0.1500  -8.9900  -6.0200  -5.3100
        [torch.DoubleTensor of dimension 6x5]
        
        > a:dist(u * torch.diag(s) * v:t())
        2.8923773593204e-14
        ```
        
        
        
        """

   'torch.inverse':
      'prefix': 'torchinverse'
      'body': """
        <a name="torch.inverse"></a>
        ### torch.inverse([res,] x) ###
        
        Computes the inverse of square matrix `x`.
        
        `torch.inverse(x)` returns the result as a new matrix.
        
        `torch.inverse(y, x)` puts the result in `y`.
        
        Note: Irrespective of the original strides, the returned matrix `y` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > x = torch.rand(10, 10)
        > y = torch.inverse(x)
        > z = x * y
        > z
        1.0000 -0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000
        0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000
        0.0000 -0.0000  1.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000
        0.0000 -0.0000 -0.0000  1.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000
        0.0000 -0.0000  0.0000 -0.0000  1.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000
        0.0000 -0.0000  0.0000 -0.0000  0.0000  1.0000  0.0000 -0.0000 -0.0000  0.0000
        0.0000 -0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000 -0.0000  0.0000  0.0000
        0.0000 -0.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000
        0.0000 -0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  1.0000  0.0000
        0.0000 -0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000  0.0000  1.0000
        [torch.DoubleTensor of dimension 10x10]
        
        > torch.max(torch.abs(z - torch.eye(10))) -- Max nonzero
        2.3092638912203e-14
        ```
        
        
        
        """

   'torch.qr':
      'prefix': 'torchqr'
      'body': """
        <a name="torch.qr"></a>
        ### torch.qr([q, r], x) ###
        
        Compute a QR decomposition of the matrix `x`: matrices `q` and `r` such that `x = q * r`, with `q` orthogonal and `r` upper triangular.
        This returns the thin (reduced) QR factorization.
        
        `torch.qr(x)` returns the Q and R components as new matrices.
        
        `torch.qr(q, r, x)` stores them in existing `Tensor`s `q` and `r`.
        
        Note that precision may be lost if the magnitudes of the elements of `x` are large.
        
        Note also that, while it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.
        
        Note: Irrespective of the original strides, the returned matrix `q` will be transposed, i.e. with strides `1, m` instead of `m, 1`.
        
        ```lua
        > a = torch.Tensor{{12, -51, 4}, {6, 167, -68}, {-4, 24, -41}}
        > a
        12  -51    4
        6  167  -68
        -4   24  -41
        [torch.DoubleTensor of dimension 3x3]
        
        > q, r = torch.qr(a)
        > q
        -0.8571  0.3943  0.3314
        -0.4286 -0.9029 -0.0343
        0.2857 -0.1714  0.9429
        [torch.DoubleTensor of dimension 3x3]
        
        > r
        -14.0000  -21.0000   14.0000
        0.0000 -175.0000   70.0000
        0.0000    0.0000  -35.0000
        [torch.DoubleTensor of dimension 3x3]
        
        > (q * r):round()
        12  -51    4
        6  167  -68
        -4   24  -41
        [torch.DoubleTensor of dimension 3x3]
        
        > (q:t() * q):round()
        1  0  0
        0  1  0
        0  0  1
        [torch.DoubleTensor of dimension 3x3]
        ```
        
        
        
        """

   'torch.geqrf':
      'prefix': 'torchgeqrf'
      'body': """
        <a name="torch.geqrf"></a>
        ### torch.geqrf([m, tau], a) ###
        
        This is a low-level function for calling LAPACK directly.
        You'll generally want to use `torch.qr()` instead.
        
        Computes a QR decomposition of `a`, but without constructing Q and R as explicit separate matrices.
        Rather, this directly calls the underlying LAPACK function `?geqrf` which produces a sequence of 'elementary reflectors'.
        See [LAPACK documentation](https://software.intel.com/en-us/node/521004) for further details.
        
        
        
        """

   'torch.orgqr':
      'prefix': 'torchorgqr'
      'body': """
        <a name="torch.orgqr"></a>
        ### torch.orgqr([q], m, tau) ###
        
        This is a low-level function for calling LAPACK directly.
        You'll generally want to use `torch.qr()` instead.
        
        Constructs a Q matrix from a sequence of elementary reflectors, such as that given by `torch.geqrf`.
        See [LAPACK documentation](https://software.intel.com/en-us/node/521010) for further details.
        
        
        
        """

   'torch.ormqr':
      'prefix': 'torchormqr'
      'body': """
        <a name="torch.ormqr"></a>
        ### torch.ormqr([res], m, tau, mat [, 'L' or 'R'] [, 'N' or 'T']) ###
        
        Multiply a matrix with `Q` as defined by the elementary reflectors and scalar factors returned by `geqrf`.
        This is a low-level function for calling LAPACK directly.
        You'll generally want to use `torch.qr()` instead.
        
        * `side` (`'L'` or `'R'`) specifies whether `mat` should be left-multiplied, `mat * Q`, or right-multiplied, `Q * mat`.
        * `trans` (`'N'` or `'T`') specifies whether `Q` should be transposed before being multiplied.
        
        See [LAPACK documentation](https://software.intel.com/en-us/node/521011) for further details.
        
        
        
        """

   'torch.logical.dok':
      'prefix': 'torchlogicaldok'
      'body': """
        <a name="torch.logical.dok"></a>
        ## Logical Operations on `Tensor`s ##
        
        These functions implement logical comparison operators that take a `Tensor` as input and another `Tensor` or a number as the comparison target.
        They return a `ByteTensor` in which each element is `0` or `1` indicating if the comparison for the corresponding element was `false` or `true` respectively.
        
        
        
        """

   'torch.lt':
      'prefix': 'torchlt'
      'body': """
        <a name="torch.lt"></a>
        ### torch.lt(a, b) ###
        
        Implements `<` operator comparing each element in `a` with `b` (if `b` is a number) or each element in `a` with corresponding element in `b`.
        
        
        
        """

   'torch.lt':
      'prefix': 'torchlt'
      'body': """
        <a name="torch.lt"></a>
        ### torch.le(a, b) ###
        
        Implements `<=` operator comparing each element in `a` with `b` (if `b` is a number) or each element in `a` with corresponding element in `b`.
        
        
        
        """

   'torch.lt':
      'prefix': 'torchlt'
      'body': """
        <a name="torch.lt"></a>
        ### torch.gt(a, b) ###
        
        Implements `>` operator comparing each element in `a` with `b` (if `b` is a number) or each element in `a` with corresponding element in `b`.
        
        
        
        """

   'torch.lt':
      'prefix': 'torchlt'
      'body': """
        <a name="torch.lt"></a>
        ### torch.ge(a, b) ###
        
        Implements `>=` operator comparing each element in `a` with `b` (if `b` is a number) or each element in `a` with corresponding element in `b`.
        
        
        
        """

   'torch.lt':
      'prefix': 'torchlt'
      'body': """
        <a name="torch.lt"></a>
        ### torch.eq(a, b) ###
        
        Implements `==` operator comparing each element in `a` with `b` (if `b` is a number) or each element in `a` with corresponding element in `b`.
        
        
        
        """

   'torch.lt':
      'prefix': 'torchlt'
      'body': """
        <a name="torch.lt"></a>
        ### torch.ne(a, b) ###
        
        Implements `~=` operator comparing each element in `a` with `b` (if `b` is a number) or each element in `a` with corresponding element in `b`.
        
        
        ### torch.all(a) ###
        ### torch.any(a) ###
        
        Additionally, `any` and `all` logically sum a `ByteTensor` returning `true` if any or all elements are logically true respectively.
        Note that logically true here is meant in the C sense (zero is `false`, non-zero is `true`) such as the output of the `Tensor` element-wise logical operations.
        
        ```lua
        > a = torch.rand(10)
        > b = torch.rand(10)
        > a
        0.5694
        0.5264
        0.3041
        0.4159
        0.1677
        0.7964
        0.0257
        0.2093
        0.6564
        0.0740
        [torch.DoubleTensor of dimension 10]
        
        > b
        0.2950
        0.4867
        0.9133
        0.1291
        0.1811
        0.3921
        0.7750
        0.3259
        0.2263
        0.1737
        [torch.DoubleTensor of dimension 10]
        
        > torch.lt(a, b)
        0
        0
        1
        0
        1
        0
        1
        1
        0
        1
        [torch.ByteTensor of dimension 10]
        
        > torch.eq(a, b)
        0
        0
        0
        0
        0
        0
        0
        0
        0
        0
        [torch.ByteTensor of dimension 10]
        
        > torch.ne(a, b)
        1
        1
        1
        1
        1
        1
        1
        1
        1
        1
        [torch.ByteTensor of dimension 10]
        
        > torch.gt(a, b)
        1
        1
        0
        1
        0
        1
        0
        0
        1
        0
        [torch.ByteTensor of dimension 10]
        
        > a[torch.gt(a, b)] = 10
        > a
        10.0000
        10.0000
        0.3041
        10.0000
        0.1677
        10.0000
        0.0257
        0.2093
        10.0000
        0.0740
        [torch.DoubleTensor of dimension 10]
        
        > a[torch.gt(a, 1)] = -1
        > a
        -1.0000
        -1.0000
        0.3041
        -1.0000
        0.1677
        -1.0000
        0.0257
        0.2093
        -1.0000
        0.0740
        [torch.DoubleTensor of dimension 10]
        
        > a = torch.ones(3):byte()
        > torch.all(a)
        true
        
        > a[2] = 0
        > torch.all(a)
        false
        
        > torch.any(a)
        true
        
        > a:zero()
        > torch.any(a)
        false
        ```
        
        """

   'torch.MemoryFile.dok':
      'prefix': 'torchMemoryFiledok'
      'body': """
        <a name="torch.MemoryFile.dok"></a>
        # MemoryFile #
        
        Parent classes: [File](file.md)
        
        A `MemoryFile` is a particular `File` which is able to perform basic
        read/write operations on a buffer in `RAM`. It implements all methods
        described in [File](file.md).
        
        The data of the `File` is contained into a `NULL` terminated
        [CharStorage](storage.md).
        
        
        """

   'torch.MemoryFile':
      'prefix': 'torchMemoryFile'
      'body': """
        <a name="torch.MemoryFile"></a>
        ### torch.MemoryFile([mode]) ###
        
        _Constructor_ which returns a new `MemoryFile` object using `mode`. Valid
        `mode` are `"r"` (read), `"w"` (write) or `"rw"` (read-write). Default is `"rw"`.
        
        
        
        """

   'torch.MemoryFile':
      'prefix': 'torchMemoryFile'
      'body': """
        <a name="torch.MemoryFile"></a>
        ### torch.MemoryFile(storage, mode) ###
        
        _Constructor_ which returns a new `MemoryFile` object, using the given
        [storage](storage.md) (which must be a `CharStorage`) and `mode`. Valid
        `mode` are `"r"` (read), `"w"` (write) or `"rw"` (read-write). The last character
        in this storage _must_ be `NULL` or an error will be generated. This allows
        to read existing memory. If used for writing, note that the `storage` might
        be resized by this class if needed.
        
        
        """

   'torch.MemoryFile.storage':
      'prefix': 'torchMemoryFilestorage'
      'body': """
        <a name="torch.MemoryFile.storage"></a>
        ### [CharStorage] storage() ###
        
        Returns the [storage](storage.md) which contains all the data of the
        `File` (note: this is _not_ a copy, but a _reference_ on this storage). The
        size of the storage is the size of the data in the `File`, plus one, the
        last character being `NULL`.
        
        
        """

   'torch.MemoryFile.longSize':
      'prefix': 'torchMemoryFilelongSize'
      'body': """
        <a name="torch.MemoryFile.longSize"/></a>
        ### longSize([size]) ###
        
        Longs will be written and read from the file as `size` bytes long, which
        can be 0, 4 or 8. 0 means system default.
        
        """

   'nn.Module':
      'prefix': 'nnModule'
      'body': """
        <a name="nn.Module"></a>
        ## Module ##
        
        `Module` is an abstract class which defines fundamental methods necessary
        for a training a neural network. Modules are [serializable](https://github.com/torch/torch7/blob/master/doc/serialization.md#serialization).
        
        Modules contain two states variables: [output](#output) and
        [gradInput](#gradinput).
        
        
        """

   'nn.Module.forward':
      'prefix': 'nnModuleforward'
      'body': """
        <a name="nn.Module.forward"></a>
        ### [output] forward(input) ###
        
        Takes an `input` object, and computes the corresponding `output` of the
        module. In general `input` and `output` are
        [Tensors](https://github.com/torch/torch7/blob/master/doc/tensor.md). However, some special sub-classes
        like [table layers](table.md#nn.TableLayers) might expect something else. Please,
        refer to each module specification for further information.
        
        After a `forward()`, the [ouput](#output) state variable should
        have been updated to the new value.
        
        It is not advised to override this function. Instead, one should
        implement [updateOutput(input)](#nn.Module.updateOutput)
        function. The forward module in the abstract parent class
        [Module](#nn.Module) will call `updateOutput(input)`.
        
        
        """

   'nn.Module.backward':
      'prefix': 'nnModulebackward'
      'body': """
        <a name="nn.Module.backward"></a>
        ### [gradInput] backward(input, gradOutput) ###
        
        Performs a _backpropagation step_ through the module, with respect to the
        given `input`.  In general this method makes the assumption
        [forward(input)](#nn.Module.forward) has been called before, _with the same input_.
        This is necessary for optimization reasons. If you do not respect
        this rule, `backward()` will compute incorrect gradients.
        
        In general `input` and `gradOutput`  and `gradInput` are
        [Tensors](https://github.com/torch/torch7/blob/master/doc/tensor.md). However, some special sub-classes
        like [table layers](table.md#nn.TableLayers) might expect something else. Please,
        refer to each module specification for further information.
        
        A _backpropagation step_ consist in computing two kind of gradients
        at `input` given `gradOutput` (gradients with respect to the
        output of the module).  This function simply performs this task using
        two function calls:
        
        - A function call to [updateGradInput(input, gradOutput)](#nn.Module.updateGradInput).
        - A function call to [accGradParameters(input,gradOutput,scale)](#nn.Module.accGradParameters).
        
        It is not advised to override this function call in custom classes. It
        is better to override
        [updateGradInput(input, gradOutput)](#nn.Module.updateGradInput) and
        [accGradParameters(input, gradOutput,scale)](#nn.Module.accGradParameters)
        functions.
        
        
        """

   'nn.Module.updateOutput':
      'prefix': 'nnModuleupdateOutput'
      'body': """
        <a name="nn.Module.updateOutput"></a>
        ### updateOutput(input) ###
        
        Computes the output using the current parameter set of the class and
        input. This function returns the result which is stored in the
        [output](#output) field.
        
        
        """

   'nn.Module.updateGradInput':
      'prefix': 'nnModuleupdateGradInput'
      'body': """
        <a name="nn.Module.updateGradInput"></a>
        ### updateGradInput(input, gradOutput) ###
        
        Computing the gradient of the module with respect to its own
        input. This is returned in `gradInput`. Also, the
        [gradInput](#gradinput) state variable is updated
        accordingly.
        
        
        """

   'nn.Module.accGradParameters':
      'prefix': 'nnModuleaccGradParameters'
      'body': """
        <a name="nn.Module.accGradParameters"></a>
        ### accGradParameters(input, gradOutput, scale) ###
        
        Computing the gradient of the module with respect to its
        own parameters. Many modules do not perform this step as they do not
        have any parameters. The state variable name for the parameters is
        module dependent. The module is expected to _accumulate_ the
        gradients with respect to the parameters in some variable.
        
        `scale` is a scale factor that is multiplied with the gradParameters before being accumulated.
        
        Zeroing this accumulation is achieved with
        [zeroGradParameters()](#nn.Module.zeroGradParameters) and updating
        the parameters according to this accumulation is done with
        [updateParameters()](#nn.Module.updateParameters).
        
        
        """

   'nn.Module.zeroGradParameters':
      'prefix': 'nnModulezeroGradParameters'
      'body': """
        <a name="nn.Module.zeroGradParameters"></a>
        ### zeroGradParameters() ###
        
        If the module has parameters, this will zero the accumulation of the
        gradients with respect to these parameters, accumulated through
        [accGradParameters(input, gradOutput,scale)](#nn.Module.accGradParameters)
        calls. Otherwise, it does nothing.
        
        
        """

   'nn.Module.updateParameters':
      'prefix': 'nnModuleupdateParameters'
      'body': """
        <a name="nn.Module.updateParameters"></a>
        ### updateParameters(learningRate) ###
        
        If the module has parameters, this will update these parameters, according
        to the accumulation of the gradients with respect to these parameters,
        accumulated through [backward()](#nn.Module.backward) calls.
        
        The update is basically:
        ```lua
        parameters = parameters - learningRate * gradients_wrt_parameters
        ```
        If the module does not have parameters, it does nothing.
        
        
        """

   'nn.Module.accUpdateGradParameters':
      'prefix': 'nnModuleaccUpdateGradParameters'
      'body': """
        <a name="nn.Module.accUpdateGradParameters"></a>
        ### accUpdateGradParameters(input, gradOutput, learningRate) ###
        
        This is a convenience module that performs two functions at
        once. Calculates and accumulates the gradients with respect to the
        weights after multiplying with negative of the learning rate
        `learningRate`. Performing these two operations at once is more
        performance efficient and it might be advantageous in certain
        situations.
        
        Keep in mind that, this function uses a simple trick to achieve its
        goal and it might not be valid for a custom module.
        
        Also note that compared to accGradParameters(), the gradients are not retained 
        for future use. 
        
        ```lua
        function Module:accUpdateGradParameters(input, gradOutput, lr)
        local gradWeight = self.gradWeight
        local gradBias = self.gradBias
        self.gradWeight = self.weight
        self.gradBias = self.bias
        self:accGradParameters(input, gradOutput, -lr)
        self.gradWeight = gradWeight
        self.gradBias = gradBias
        end
        ```
        
        As it can be seen, the gradients are accumulated directly into
        weights. This assumption may not be true for a module that computes a
        nonlinear operation.
        
        
        """

   'nn.Module.share':
      'prefix': 'nnModuleshare'
      'body': """
        <a name="nn.Module.share"></a>
        ### share(mlp,s1,s2,...,sn) ###
        
        This function modifies the parameters of the module named
        `s1`,..`sn` (if they exist) so that they are shared with (pointers
        to) the parameters with the same names in the given module `mlp`.
        
        The parameters have to be Tensors. This function is typically used if
        you want to have modules that share the same weights or biases.
        
        Note that this function if called on a [Container](containers.md#nn.Containers)
        module will share the same parameters for all the contained modules as
        well.
        
        Example:
        ```lua
        
        -- make an mlp
        mlp1=nn.Sequential(); 
        mlp1:add(nn.Linear(100,10));
        
        -- make a second mlp
        mlp2=nn.Sequential(); 
        mlp2:add(nn.Linear(100,10)); 
        
        -- the second mlp shares the bias of the first
        mlp2:share(mlp1,'bias');
        
        -- we change the bias of the first
        mlp1:get(1).bias[1]=99;
        
        -- and see that the second one's bias has also changed..
        print(mlp2:get(1).bias[1])
        
        ```
        
        
        """

   'nn.Module.clone':
      'prefix': 'nnModuleclone'
      'body': """
        <a name="nn.Module.clone"></a>
        ### clone(mlp,...) ###
        
        Creates a deep copy of (i.e. not just a pointer to) the module,
        including the current state of its parameters (e.g. weight, biases
        etc., if any).
        
        If arguments are provided to the `clone(...)` function it also calls
        [share(...)](#nn.Module.share) with those arguments on the cloned
        module after creating it, hence making a deep copy of this module with
        some shared parameters.
        
        Example:
        ```lua
        -- make an mlp
        mlp1=nn.Sequential(); 
        mlp1:add(nn.Linear(100,10));
        
        -- make a copy that shares the weights and biases
        mlp2=mlp1:clone('weight','bias');
        
        -- we change the bias of the first mlp
        mlp1:get(1).bias[1]=99;
        
        -- and see that the second one's bias has also changed..
        print(mlp2:get(1).bias[1])
        
        ```
        
        
        """

   'nn.Module.type':
      'prefix': 'nnModuletype'
      'body': """
        <a name="nn.Module.type"></a>
        ### type(type[, tensorCache]) ###
        
        This function converts all the parameters of a module to the given
        `type`. The `type` can be one of the types defined for
        [torch.Tensor](https://github.com/torch/torch7/blob/master/doc/tensor.md).
        
        If tensors (or their storages) are shared between multiple modules in a 
        network, this sharing will be preserved after type is called.
        
        To preserve sharing between multiple modules and/or tensors, use
        `nn.utils.recursiveType`:
        
        ```lua
        -- make an mlp
        mlp1=nn.Sequential(); 
        mlp1:add(nn.Linear(100,10));
        
        -- make a second mlp
        mlp2=nn.Sequential(); 
        mlp2:add(nn.Linear(100,10)); 
        
        -- the second mlp shares the bias of the first
        mlp2:share(mlp1,'bias');
        
        -- mlp1 and mlp2 will be converted to float, and will share bias
        -- note: tensors can be provided as inputs as well as modules
        nn.utils.recursiveType({mlp1, mlp2}, 'torch.FloatTensor')
        ```
        
        
        """

   'nn.Module.float':
      'prefix': 'nnModulefloat'
      'body': """
        <a name="nn.Module.float"></a>
        ### float() ###
        
        Convenience method for calling [module:type('torch.FloatTensor')](#nn.Module.type)
        
        
        """

   'nn.Module.double':
      'prefix': 'nnModuledouble'
      'body': """
        <a name="nn.Module.double"></a>
        ### double() ###
        
        Convenience method for calling [module:type('torch.DoubleTensor')](#nn.Module.type)
        
        
        """

   'nn.Module.cuda':
      'prefix': 'nnModulecuda'
      'body': """
        <a name="nn.Module.cuda"></a>
        ### cuda() ###
        
        Convenience method for calling [module:type('torch.CudaTensor')](#nn.Module.type)
        
        
        """

   'nn.statevars.dok':
      'prefix': 'nnstatevarsdok'
      'body': """
        <a name="nn.statevars.dok"></a>
        ### State Variables ###
        
        These state variables are useful objects if one wants to check the guts of
        a `Module`. The object pointer is _never_ supposed to change. However, its
        contents (including its size if it is a Tensor) are supposed to change.
        
        In general state variables are
        [Tensors](https://github.com/torch/torch7/blob/master/doc/tensor.md). 
        However, some special sub-classes
        like [table layers](table.md#nn.TableLayers) contain something else. Please,
        refer to each module specification for further information.
        
        
        """

   'nn.Module.output':
      'prefix': 'nnModuleoutput'
      'body': """
        <a name="nn.Module.output"></a>
        #### output ####
        
        This contains the output of the module, computed with the last call of
        [forward(input)](#nn.Module.forward).
        
        
        """

   'nn.Module.gradInput':
      'prefix': 'nnModulegradInput'
      'body': """
        <a name="nn.Module.gradInput"></a>
        #### gradInput ####
        
        This contains the gradients with respect to the inputs of the module, computed with the last call of
        [updateGradInput(input, gradOutput)](#nn.Module.updateGradInput). 
        
        ### Parameters and gradients w.r.t parameters ###
        
        Some modules contain parameters (the ones that we actually want to
        train!). The name of these parameters, and gradients w.r.t these parameters
        are module dependent.
        
        
        """

   'nn.Module.parameters':
      'prefix': 'nnModuleparameters'
      'body': """
        <a name="nn.Module.parameters"></a>
        ### [{weights}, {gradWeights}] parameters() ###
        
        This function should returns two tables. One for the learnable
        parameters `{weights}` and another for the gradients of the energy
        wrt to the learnable parameters `{gradWeights}`.
        
        Custom modules should override this function if they use learnable
        parameters that are stored in tensors.
        
        
        """

   'nn.Module.getParameters':
      'prefix': 'nnModulegetParameters'
      'body': """
        <a name="nn.Module.getParameters"></a>
        ### [flatParameters, flatGradParameters] getParameters() ###
        
        This function returns two tensors. One for the flattened learnable
        parameters `flatParameters` and another for the gradients of the energy
        wrt to the learnable parameters `flatGradParameters`.
        
        Custom modules should not override this function. They should instead override [parameters(...)](#nn.Module.parameters) which is, in turn, called by the present function.
        
        This function will go over all the weights and gradWeights and make them view into a single tensor (one for weights and one for gradWeights). Since the storage of every weight and gradWeight is changed, this function should be called only once on a given network.
        
        
        """

   'nn.Module.training':
      'prefix': 'nnModuletraining'
      'body': """
        <a name="nn.Module.training"></a>
        ### training() ###
        This sets the mode of the Module (or sub-modules) to `train=true`. This is useful for modules like [Dropout](simple.md#nn.Dropout) that have a different behaviour during training vs evaluation.
        
        
        """

   'nn.Module.evaluate':
      'prefix': 'nnModuleevaluate'
      'body': """
        <a name="nn.Module.evaluate"></a>
        ### evaluate() ###
        This sets the mode of the Module (or sub-modules) to `train=false`. This is useful for modules like [Dropout](simple.md#nn.Dropout) that have a different behaviour during training vs evaluation.
        
        
        """

   'nn.Module.findModules':
      'prefix': 'nnModulefindModules'
      'body': """
        <a name="nn.Module.findModules"></a>
        ### findModules(typename) ###
        Find all instances of modules in the network of a certain `typename`.  It returns a flattened list of the matching nodes, as well as a flattened list of the container modules for each matching node.
        
        Modules that do not have a parent container (ie, a top level nn.Sequential for instance) will return their `self` as the container.
        
        This function is very helpful for navigating complicated nested networks.  For example, a didactic example might be; if you wanted to print the output size of all `nn.SpatialConvolution` instances:
        
        ```lua
        -- Construct a multi-resolution convolution network (with 2 resolutions):
        model = nn.ParallelTable()
        conv_bank1 = nn.Sequential()
        conv_bank1:add(nn.SpatialConvolution(3,16,5,5))
        conv_bank1:add(nn.Threshold())
        model:add(conv_bank1)
        conv_bank2 = nn.Sequential()
        conv_bank2:add(nn.SpatialConvolution(3,16,5,5))
        conv_bank2:add(nn.Threshold())
        model:add(conv_bank2)
        -- FPROP a multi-resolution sample
        input = {torch.rand(3,128,128), torch.rand(3,64,64)}
        model:forward(input)
        -- Print the size of the Threshold outputs
        conv_nodes = model:findModules('nn.SpatialConvolution')
        for i = 1, #conv_nodes do
        print(conv_nodes[i].output:size())
        end
        ```
        
        Another use might be to replace all nodes of a certain `typename` with another.  For instance, if we wanted to replace all `nn.Threshold` with `nn.Tanh` in the model above:
        
        ```lua
        threshold_nodes, container_nodes = model:findModules('nn.Threshold')
        for i = 1, #threshold_nodes do
        -- Search the container for the current threshold node
        for j = 1, #(container_nodes[i].modules) do
        if container_nodes[i].modules[j] == threshold_nodes[i] then
        -- Replace with a new instance
        container_nodes[i].modules[j] = nn.Tanh()
        end
        end
        end
        ```
        
        
        """

   'nn.Module.listModules':
      'prefix': 'nnModulelistModules'
      'body': """
        <a name="nn.Module.listModules"></a>
        ### listModules() ###
        
        List all Modules instances in a network. Returns a flattened list of modules, 
        including container modules (which will be listed first), self, and any other 
        component modules. 
        
        For example :
        ```lua
        mlp = nn.Sequential()
        mlp:add(nn.Linear(10,20))
        mlp:add(nn.Tanh())
        mlp2 = nn.Parallel()
        mlp2:add(mlp)
        mlp2:add(nn.ReLU())
        for i,module in ipairs(mlp2:listModules()) do
        print(module)
        end
        ```
        
        Which will result in the following output :
        
        ```lua
        nn.Parallel {
        input
        |`-> (1): nn.Sequential {
        |      [input -> (1) -> (2) -> output]
        |      (1): nn.Linear(10 -> 20)
        |      (2): nn.Tanh
        |    }
        |`-> (2): nn.ReLU
        ... -> output
        }
        nn.Sequential {
        [input -> (1) -> (2) -> output]
        (1): nn.Linear(10 -> 20)
        (2): nn.Tanh
        }
        nn.Linear(10 -> 20)
        nn.Tanh
        nn.ReLU
        ```
        
        """

   'nn.overview.dok':
      'prefix': 'nnoverviewdok'
      'body': """
        <a name="nn.overview.dok"></a>
        # Overview #
        
        Each module of a network is composed of [Modules](module.md#nn.Modules) and there
        are several sub-classes of `Module` available: container classes like
        [Sequential](containers.md#nn.Sequential), [Parallel](containers.md#nn.Parallel) and
        [Concat](containers.md#nn.Concat) , which can contain simple layers like
        [Linear](simple.md#nn.Linear), [Mean](simple.md#nn.Mean), [Max](simple.md#nn.Max) and
        [Reshape](simple.md#nn.Reshape), as well as [convolutional layers](convolution.md), and [transfer
        functions](transfer.md) like [Tanh](transfer.md#nn.Tanh).
        
        Loss functions are implemented as sub-classes of
        [Criterion](criterion.md#nn.Criterions). They are helpful to train neural network on
        classical tasks.  Common criterions are the Mean Squared Error
        criterion implemented in [MSECriterion](criterion.md#nn.MSECriterion) and the
        cross-entropy criterion implemented in
        [ClassNLLCriterion](criterion.md#nn.ClassNLLCriterion).
        
        Finally, the [StochasticGradient](training.md#nn.StochasticGradient) class provides a
        high level way to train the neural network of choice, even though it is
        easy with a simple for loop to [train a neural network yourself](training.md#nn.DoItYourself).
        
        ## Detailed Overview ##
        This section provides a detailed overview of the neural network package. First the omnipresent [Module](#nn.overview.module) is examined, followed by some examples for [combining modules](#nn.overview.plugandplay) together. The last part explores facilities for [training a neural network](#nn.overview.training), and finally some caveats while training networks with [shared parameters](#nn.overview.sharedparams).
        
        
        """

   'nn.overview.module':
      'prefix': 'nnoverviewmodule'
      'body': """
        <a name="nn.overview.module"></a>
        ### Module ###
        
        A neural network is called a [Module](module.md#nn.Module) (or simply
        _module_ in this documentation) in Torch. `Module` is an abstract
        class which defines four main methods:
        
        * [forward(input)](module.md#nn.Module.forward) which computes the output of the module given the `input` [Tensor](https://github.com/torch/torch7/blob/master/doc/tensor.md).
        * [backward(input, gradOutput)](module.md#nn.Module.backward) which computes the gradients of the module with respect to its own parameters, and its own inputs.
        * [zeroGradParameters()](module.md#nn.Module.zeroGradParameters) which zeroes the gradient with respect to the parameters of the module.
        * [updateParameters(learningRate)](module.md#nn.Module.updateParameters) which updates the parameters after one has computed the gradients with `backward()`
        
        It also declares two members:
        
        * [output](module.md#nn.Module.output) which is the output returned by `forward()`.
        * [gradInput](module.md#nn.Module.gradInput) which contains the gradients with respect to the input of the module, computed in a `backward()`.
        
        Two other perhaps less used but handy methods are also defined:
        
        * [share(mlp,s1,s2,...,sn)](module.md#nn.Module.share) which makes this module share the parameters s1,..sn of the module `mlp`. This is useful if you want to have modules that share the same weights.
        * [clone(...)](module.md#nn.Module.clone) which produces a deep copy of (i.e. not just a pointer to) this Module, including the current state of its parameters (if any).
        
        Some important remarks:
        
        * `output` contains only valid values after a [forward(input)](module.md#nn.Module.forward).
        * `gradInput` contains only valid values after a [backward(input, gradOutput)](module.md#nn.Module.backward).
        * [backward(input, gradOutput)](module.md#nn.Module.backward) uses certain computations obtained during [forward(input)](module.md#nn.Module.forward). You _must_ call `forward()` before calling a `backward()`, on the _same_ `input`, or your gradients are going to be incorrect!
        
        
        """

   'nn.overview.plugandplay':
      'prefix': 'nnoverviewplugandplay'
      'body': """
        <a name="nn.overview.plugandplay"></a>
        ### Plug and play ###
        
        Building a simple neural network can be achieved by constructing an available layer.
        A linear neural network (perceptron!) is built only in one line:
        ```lua
        mlp = nn.Linear(10,1) -- perceptron with 10 inputs
        ```
        
        More complex neural networks are easily built using container classes
        [Sequential](containers.md#nn.Sequential) and [Concat](containers.md#nn.Concat). `Sequential` plugs
        layer in a feed-forward fully connected manner. `Concat` concatenates in
        one layer several modules: they take the same inputs, and their output is
        concatenated.
        
        Creating a one hidden-layer multi-layer perceptron is thus just as easy as:
        ```lua
        mlp = nn.Sequential()
        mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units
        mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function
        mlp:add( nn.Linear(25, 1) ) -- 1 output
        ```
        
        Of course, `Sequential` and `Concat` can contains other
        `Sequential` or `Concat`, allowing you to try the craziest neural
        networks you ever dreamt of! See the [[#nn.Modules|complete list of
        available modules]].
        
        
        """

   'nn.overview.training':
      'prefix': 'nnoverviewtraining'
      'body': """
        <a name="nn.overview.training"></a>
        ### Training a neural network ###
        
        Once you built your neural network, you have to choose a particular
        [Criterion](criterion.md#nn.Criterions) to train it. A criterion is a class which
        describes the cost to be minimized during training.
        
        You can then train the neural network by using the
        [StochasticGradient](training.md#nn.StochasticGradient) class.
        
        ```lua
        criterion = nn.MSECriterion() -- Mean Squared Error criterion
        trainer = nn.StochasticGradient(mlp, criterion)
        trainer:train(dataset) -- train using some examples
        ```
        
        StochasticGradient expect as a `dataset` an object which implements
        the operator `dataset[index]` and implements the method
        `dataset:size()`. The `size()` methods returns the number of
        examples and `dataset[i]` has to return the i-th example.
        
        An `example` has to be an object which implements the operator
        `example[field]`, where `field` might take the value `1` (input
        features) or `2` (corresponding label which will be given to the
        criterion).  The input is usually a Tensor (except if you use special
        kind of gradient modules, like [table layers](table.md#nn.TableLayers)). The
        label type depends on the criterion.  For example, the
        [MSECriterion](criterion.md#nn.MSECriterion) expect a Tensor, but the
        [ClassNLLCriterion](criterion.md#nn.ClassNLLCriterion) except a integer number (the
        class).
        
        Such a dataset is easily constructed by using Lua tables, but it could
        any `C` object for example, as long as required operators/methods
        are implemented.  [See an example](containers.md#nn.DoItStochasticGradient).
        
        `StochasticGradient` being written in `Lua`, it is extremely easy
        to cut-and-paste it and create a variant to it adapted to your needs
        (if the constraints of `StochasticGradient` do not satisfy you).
        
        
        """

   'nn.overview.lowlevel':
      'prefix': 'nnoverviewlowlevel'
      'body': """
        <a name="nn.overview.lowlevel"></a>
        #### Low Level Training ####
        
        If you want to program the `StochasticGradient` by hand, you
        essentially need to control the use of forwards and backwards through
        the network yourself.  For example, here is the code fragment one
        would need to make a gradient step given an input `x`, a desired
        output `y`, a network `mlp` and a given criterion `criterion`
        and learning rate `learningRate`:
        
        ```lua
        function gradUpdate(mlp, x, y, criterion, learningRate) 
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        ```
        For example, if you wish to use your own criterion you can simple replace 
        `gradCriterion` with the gradient vector of your criterion of choice.
        
        
        """

   'nn.overview.sharedparams':
      'prefix': 'nnoverviewsharedparams'
      'body': """
        <a name="nn.overview.sharedparams"></a>
        ### A Note on Sharing Parameters ###
        
        By using `:share(...)` and the Container Modules, one can easily create very
        complex architectures. In order to make sure that the network is going to
        train properly, one need to pay attention to the way the sharing is applied,
        because it might depend on the optimization procedure.
        
        * If you are using an optimization algorithm that iterates over the modules
        of your network (by calling `:updateParameters` for example), only the
        parameters of the network should be shared.
        * If you use the flattened parameter tensor to optimize the network, 
        obtained by calling `:getParameters`, for example for the package `optim`, 
        then you need to share both the parameters and the gradParameters.
        
        Here is an example for the first case:
        
        ```lua
        -- our optimization procedure will iterate over the modules, so only share
        -- the parameters
        mlp = nn.Sequential()
        linear = nn.Linear(2,2)
        linear_clone = linear:clone('weight','bias') -- clone sharing the parameters
        mlp:add(linear)
        mlp:add(linear_clone)
        function gradUpdate(mlp, x, y, criterion, learningRate) 
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        ```
        
        And for the second case:
        
        ```lua
        -- our optimization procedure will use all the parameters at once, because
        -- it requires the flattened parameters and gradParameters Tensors. Thus,
        -- we need to share both the parameters and the gradParameters
        mlp = nn.Sequential()
        linear = nn.Linear(2,2)
        -- need to share the parameters and the gradParameters as well
        linear_clone = linear:clone('weight','bias','gradWeight','gradBias')
        mlp:add(linear)
        mlp:add(linear_clone)
        params, gradParams = mlp:getParameters()
        function gradUpdate(mlp, x, y, criterion, learningRate, params, gradParams)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        -- adds the gradients to all the parameters at once
        params:add(-learningRate, gradParams)
        end
        ```
        
        """

   'torch.PipeFile.dok':
      'prefix': 'torchPipeFiledok'
      'body': """
        <a name="torch.PipeFile.dok"></a>
        # PipeFile #
        
        Parent classes: [DiskFile](diskfile.md)
        
        A `PipeFile` is a particular `File` which is able to perform basic read/write operations
        on a command pipe. It implements all methods described in [DiskFile](diskfile.md) and [File](file.md).
        
        The file might be open in read or write mode, depending on the parameter
        `mode` (which can take the value `"r"` or `"w"`) 
        given to the [torch.PipeFile(fileName, mode)](#torch.PipeFile). Read-write mode is not allowed.
        
        
        """

   'torch.PipeFile':
      'prefix': 'torchPipeFile'
      'body': """
        <a name="torch.PipeFile"></a>
        ### torch.PipeFile(command, [mode], [quiet]) ###
        
        _Constructor_ which execute `command` by opening a pipe in read or write
        `mode`. Valid `mode` are `"r"` (read) or `"w"` (write). Default is read
        mode.
        
        If (and only if) `quiet` is `true`, no error will be raised in case of
        problem opening the file: instead `nil` will be returned.
        
        
        """

   'torch.random.dok':
      'prefix': 'torchrandomdok'
      'body': """
        <a name="torch.random.dok"></a>
        # Random Numbers #
        
        Torch provides accurate mathematical random generation, based on
        [Mersenne Twister](http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html)
        random number generator.
        
        
        """

   ':torch.seed.dok':
      'prefix': ':torchseeddok'
      'body': """
        <a name=":torch.seed.dok"></a>
        ## Seed Handling ##
        
        The random number generator is provided with a random seed via
        [seed()](#torch.seed) when torch is being initialized. It can be
        reinitialized using [seed()](#torch.seed) or [manualSeed()](#torch.manualSeed).
        
        Initial seed can be obtained using [initialSeed()](#torch.initialSeed).
        
        Setting a particular seed allows the user to (re)-generate a particular sequence
        of random numbers. Example:
        
        ```
        > torch.manualSeed(123)
        > = torch.uniform()
        0.69646918727085
        > return  torch.uniform()
        0.71295532141812
        > return  torch.uniform()
        0.28613933874294
        > torch.manualSeed(123)
        > return  torch.uniform()
        0.69646918727085
        > return  torch.uniform()
        0.71295532141812
        > return  torch.uniform()
        0.28613933874294
        > torch.manualSeed(torch.initialSeed())
        > return  torch.uniform()
        0.69646918727085
        > return  torch.uniform()
        0.71295532141812
        > return  torch.uniform()
        0.28613933874294
        ```
        
        To regenerate a sequence of random numbers starting from a specific point
        in the sequence, one can save the state of the random number generator
        using [getRNGState()](#torch.getRNGState) and then reset the random number
        generator to that state using [setRNGState()](#torch.setRNGState). Example:
        
        ```
        > torch.manualSeed(123)
        > = torch.uniform()
        0.69646918727085
        > s = torch.getRNGState()
        > return  torch.uniform()
        0.71295532141812
        > return  torch.uniform()
        0.28613933874294
        > torch.setRNGState(s)
        > return  torch.uniform()
        0.71295532141812
        > return  torch.uniform()
        0.28613933874294
        ```
        
        
        """

   'torch.seed':
      'prefix': 'torchseed'
      'body': """
        <a name="torch.seed"></a>
        ### [number] seed() ###
        
        Set the seed of the random number generator using `/dev/urandom`
        (on Windows the time of the computer with granularity of seconds is used).
        Returns the seed obtained.
        
        
        """

   'torch.manualSeed':
      'prefix': 'torchmanualSeed'
      'body': """
        <a name="torch.manualSeed"></a>
        ### manualSeed(number) ###
        
        Set the seed of the random number generator to the given `number`.
        
        
        """

   'torch.initialSeed':
      'prefix': 'torchinitialSeed'
      'body': """
        <a name="torch.initialSeed"></a>
        ### initialSeed() ###
        
        Returns the initial seed used to initialize the random generator.
        
        
        """

   'torch.getRNGState':
      'prefix': 'torchgetRNGState'
      'body': """
        <a name="torch.getRNGState"></a>
        ### [Tensor] getRNGState() ###
        Returns the current state of the random number generator as a torch.ByteTensor.
        This can then be used to set the state of the RNG so that the same sequence of
        random numbers is produced.
        
        
        """

   'torch.setRNGState':
      'prefix': 'torchsetRNGState'
      'body': """
        <a name="torch.setRNGState"></a>
        ### [Tensor] setRNGState(state) ###
        Set the state of the random number generator. If `state` was obtained earlier
        using `getRNGState` then the random number generator should now generate the
        same numbers as it did from the point where `state` was obtained. This function
        returns its argument, `state`.
        
        
        """

   'torch.random':
      'prefix': 'torchrandom'
      'body': """
        <a name="torch.random"></a>
        ### [number] random([a], [b]) ###
        
        Returns an unsigned 32 bit integer random number from [a,b]. By default `a` is 1 and `b` is 2^32.
        
        
        """

   'torch.uniform':
      'prefix': 'torchuniform'
      'body': """
        <a name="torch.uniform"></a>
        ### [number] uniform([a],[b]) ###
        
        Returns a random real number according to uniform distribution on [a,b). By default `a` is 0 and `b` is 1.
        
        
        """

   'torch.normal':
      'prefix': 'torchnormal'
      'body': """
        <a name="torch.normal"></a>
        ### [number] normal([mean],[stdv]) ###
        
        Returns a random real number according to a normal distribution with the given `mean` and standard deviation `stdv`.
        `stdv` must be positive.
        
        
        """

   'torch.exponential':
      'prefix': 'torchexponential'
      'body': """
        <a name="torch.exponential"></a>
        ### [number] exponential(lambda) ###
        
        Returns a random real number according to the exponential distribution
        ''p(x) = lambda * exp(-lambda * x)''
        
        
        """

   'torch.cauchy':
      'prefix': 'torchcauchy'
      'body': """
        <a name="torch.cauchy"></a>
        ### [number] cauchy(median, sigma) ###
        
        Returns a random real number according to the Cauchy distribution
        ''p(x) = sigma/(pi*(sigma^2 + (x-median)^2))''
        
        
        """

   'torch.logNormal':
      'prefix': 'torchlogNormal'
      'body': """
        <a name="torch.logNormal"></a>
        ### [number] logNormal(mean, stdv) ###
        
        Returns a random real number according to the log-normal distribution, with
        the given `mean` and standard deviation `stdv`.
        `stdv` must be positive.
        
        
        """

   'torch.geometric':
      'prefix': 'torchgeometric'
      'body': """
        <a name="torch.geometric"></a>
        ### [number] geometric(p) ###
        
        Returns a random integer number according to a geometric distribution
        ''p(i) = (1-p) * p^(i-1)`. `p` must satisfy `0 < p < 1''.
        
        
        """

   'torch.bernoulli':
      'prefix': 'torchbernoulli'
      'body': """
        <a name="torch.bernoulli"></a>
        ### [number] bernoulli([p]) ###
        
        Returns `1` with probability `p` and `0` with probability `1-p`. `p` must satisfy `0 <= p <= 1`.
        By default `p` is equal to `0.5`.
        
        """

   'torch.serialization.dok':
      'prefix': 'torchserializationdok'
      'body': """
        <a name="torch.serialization.dok"></a>
        # Serialization #
        
        Torch provides 4 high-level methods to serialize/deserialize arbitrary Lua/Torch objects.
        These functions are just abstractions over the [File](#torch.File) object, and were created
        for convenience (these are very common routines).
        
        The first two functions are useful to serialize/deserialize data to/from files:
        
        - `torch.save(filename, object [, format, referenced])`
        - `[object] torch.load(filename [, format, referenced])`
        
        The next two functions are useful to serialize/deserialize data to/from strings:
        
        - `[str] torch.serialize(object)`
        - `[object] torch.deserialize(str)`
        
        Serializing to files is useful to save arbitrary data structures, or share them with other people.
        Serializing to strings is useful to store arbitrary data structures in databases, or 3rd party
        software.
        
        
        """

   'torch.save':
      'prefix': 'torchsave'
      'body': """
        <a name="torch.save"></a>
        ### torch.save(filename, object [, format, referenced]) ###
        
        Writes `object` into a file named `filename`. The `format` can be set to
        `ascii` or `binary` (default is binary). Binary format is platform
        dependent, but typically more compact and faster to read/write. The ASCII
        format is platform-independent, and should be used to share data structures
        across platforms. The option `referenced` specifies if
        [object references](file.md#torch.File.referenced) should be tracked or not
        (`true` by default).
        
        ```
        -- arbitrary object:
        obj = {
        mat = torch.randn(10,10),
        name = '10',
        test = {
        entry = 1
        }
        }
        
        -- save to disk:
        torch.save('test.dat', obj)
        ```
        
        
        """

   'torch.load':
      'prefix': 'torchload'
      'body': """
        <a name="torch.load"></a>
        ### [object] torch.load(filename [, format, referenced]) ###
        
        Reads `object` from a file named `filename`. The `format` can be set to
        `ascii` or `binary` (default is binary). Binary format is platform
        dependent, but typically more compact and faster to read/write. The ASCII
        format is platform-independent, and should be used to share data structures
        across platforms. The option `referenced` specifies if
        [object references](file.md#torch.File.referenced) should be tracked or not
        (`true` by default). Note that files written with `referenced` at `true`
        cannot be loaded with `referenced` at `false`.
        
        ```
        -- given serialized object from section above, reload:
        obj = torch.load('test.dat')
        
        print(obj)
        -- will print:
        -- {[mat]  = DoubleTensor - size: 10x10
        --  [name] = string : "10"
        --  [test] = table - size: 0}
        ```
        
        
        """

   'torch.serialize':
      'prefix': 'torchserialize'
      'body': """
        <a name="torch.serialize"></a>
        ### [str] torch.serialize(object [, format]) ###
        
        Serializes `object` into a string. The `format` can be set
        to `ascii` or `binary` (default is binary). Binary format is platform
        dependent, but typically more compact and faster to read/write. The ASCII
        format is platform-independent, and should be used to share data structures
        across platforms.
        
        ```
        -- arbitrary object:
        obj = {
        mat = torch.randn(10,10),
        name = '10',
        test = {
        entry = 1
        }
        }
        
        -- serialize:
        str = torch.serialize(obj)
        ```
        
        
        """

   'torch.deserialize':
      'prefix': 'torchdeserialize'
      'body': """
        <a name="torch.deserialize"></a>
        ### [object] torch.deserialize(str [, format]) ###
        
        Deserializes `object` from a string. The `format` can be set
        to `ascii` or `binary` (default is binary). Binary format is platform
        dependent, but typically more compact and faster to read/write. The ASCII
        format is platform-independent, and should be used to share data structures
        across platforms.
        
        ```
        -- given serialized object from section above, deserialize:
        obj = torch.deserialize(str)
        
        print(obj)
        -- will print:
        -- {[mat]  = DoubleTensor - size: 10x10
        --  [name] = string : "10"
        --  [test] = table - size: 0}
        ```
        
        
        """

   'nn.simplelayers.dok':
      'prefix': 'nnsimplelayersdok'
      'body': """
        <a name="nn.simplelayers.dok"></a>
        # Simple layers #
        Simple Modules are used for various tasks like adapting Tensor methods and providing affine transformations :
        
        * Parameterized Modules :
        * [Linear](#nn.Linear) : a linear transformation ;
        * [SparseLinear](#nn.SparseLinear) : a linear transformation with sparse inputs ;
        * [Add](#nn.Add) : adds a bias term to the incoming data ;
        * [Mul](#nn.Mul) : multiply a single scalar factor to the incoming data ;
        * [CMul](#nn.CMul) : a component-wise multiplication to the incoming data ;
        * [Euclidean](#nn.Euclidean) : the euclidean distance of the input to `k` mean centers ;
        * [WeightedEuclidean](#nn.WeightedEuclidean) : similar to [Euclidean](#nn.Euclidean), but additionally learns a diagonal covariance matrix ;
        * [Cosine](#nn.Cosine) : the cosine similarity of the input to `k` mean centers ;
        * Modules that adapt basic Tensor methods :
        * [Copy](#nn.Copy) : a [copy](https://github.com/torch/torch7/blob/master/doc/tensor.md#torch.Tensor.copy) of the input with [type](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-or-string-typetype) casting ;
        * [Narrow](#nn.Narrow) : a [narrow](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-narrowdim-index-size) operation over a given dimension ;
        * [Replicate](#nn.Replicate) : [repeats](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-repeattensorresult-sizes) input `n` times along its first dimension ;
        * [Reshape](#nn.Reshape) : a [reshape](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchreshaperes-x-m-n) of the inputs ;
        * [View](#nn.View) : a [view](https://github.com/torch/torch7/blob/master/doc/tensor.md#result-viewresult-tensor-sizes) of the inputs ;
        * [Select](#nn.Select) : a [select](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-selectdim-index) over a given dimension ;
        * [Index](#nn.Index) : a [index](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-indexdim-index) over a given dimension ;
        * Modules that adapt mathematical Tensor methods :
        * [Max](#nn.Max) : a [max](https://github.com/torch/torch7/blob/master/doc/maths.md#torch.max) operation over a given dimension ;
        * [Min](#nn.Min) : a [min](https://github.com/torch/torch7/blob/master/doc/maths.md#torchminresval-resind-x) operation over a given dimension ;
        * [Mean](#nn.Mean) : a [mean](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchmeanres-x-dim) operation over a given dimension ;
        * [Sum](#nn.Sum) : a [sum](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchsumres-x) operation over a given dimension ;
        * [Exp](#nn.Exp) : an element-wise [exp](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchexpres-x) operation ;
        * [Abs](#nn.Abs) : an element-wise [abs](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchabsres-x) operation ;
        * [Power](#nn.Power) : an element-wise [pow](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchpowres-x) operation ;
        * [Square](#nn.Square) : an element-wise square operation ;
        * [Sqrt](#nn.Sqrt) : an element-wise [sqrt](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchsqrtres-x) operation ;
        * [Clamp](#nn.Clamp) : an element-wise [clamp](https://github.com/torch/torch7/blob/master/doc/maths.md#res-torchclampres-tensor1-min_value-max_value) operation ;
        * [Normalize](#nn.Normalize) : normalizes the input to have unit `L_p` norm ;
        * [MM](#nn.MM) : matrix-matrix multiplication (also supports batches of matrices) ;
        * Miscellaneous Modules :
        * [BatchNormalization](#nn.BatchNormalization) : mean/std normalization over the mini-batch inputs (with an optional affine transform) ;
        * [Identity](#nn.Identity) : forward input as-is to output (useful with [ParallelTable](table.md#nn.ParallelTable)) ;
        * [Dropout](#nn.Dropout) : masks parts of the `input` using binary samples from a [bernoulli](http://en.wikipedia.org/wiki/Bernoulli_distribution) distribution ;
        * [SpatialDropout](#nn.SpatialDropout) : same as Dropout but for spatial inputs where adjacent pixels are strongly correlated ;
        * [Padding](#nn.Padding) : adds padding to a dimension ;
        * [L1Penalty](#nn.L1Penalty) : adds an L1 penalty to an input (for sparsity) ;
        * [GradientReversal](#nn.GradientReversal) : reverses the gradient (to maximize an objective function) ;
        
        
        """

   'nn.Linear':
      'prefix': 'nnLinear'
      'body': """
        <a name="nn.Linear"></a>
        ## Linear ##
        
        ```lua
        module = nn.Linear(inputDimension, outputDimension)
        ```
        
        Applies a linear transformation to the incoming data, i.e. `y = Ax + b`. The `input` tensor given in `forward(input)` must be either a vector (1D tensor) or matrix (2D tensor). If the input is a matrix, then each row is assumed to be an input sample of given batch.
        
        You can create a layer in the following way:
        
        ```lua
        module = nn.Linear(10, 5)  -- 10 inputs, 5 outputs
        ```
        
        Usually this would be added to a network of some kind, e.g.:
        
        ```lua
        mlp = nn.Sequential()
        mlp:add(module)
        ```
        
        The weights and biases (_A_ and _b_) can be viewed with:
        
        ```lua
        print(module.weight)
        print(module.bias)
        ```
        
        The gradients for these weights can be seen with:
        
        ```lua
        print(module.gradWeight)
        print(module.gradBias)
        ```
        
        As usual with `nn` modules, applying the linear transformation is performed with:
        
        ```lua
        x = torch.Tensor(10) -- 10 inputs
        y = module:forward(x)
        ```
        
        
        """

   'nn.SparseLinear':
      'prefix': 'nnSparseLinear'
      'body': """
        <a name="nn.SparseLinear"></a>
        ## SparseLinear ##
        
        ```lua
        module = nn.SparseLinear(inputDimension, outputDimension)
        ```
        
        Applies a linear transformation to the incoming sparse data, i.e. `y = Ax + b`. The `input` tensor given in `forward(input)` must be a sparse vector represented as 2D tensor of the form torch.Tensor(N, 2) where the pairs represent indices and values.
        The SparseLinear layer is useful when the number of input dimensions is very large and the input data is sparse.
        
        You can create a sparse linear layer in the following way:
        
        ```lua
        module = nn.SparseLinear(10000, 2)  -- 10000 inputs, 2 outputs
        ```
        
        The sparse linear module may be used as part of a larger network, and apart from the form of the input, [SparseLinear](#nn.SparseLinear) operates in exactly the same way as the [Linear](#nn.Linear) layer.
        
        A sparse input vector may be created as so...
        
        ```lua
        x = torch.Tensor({ {1, 0.1}, {2, 0.3}, {10, 0.3}, {31, 0.2} })
        
        print(x)
        
        1.0000   0.1000
        2.0000   0.3000
        10.0000   0.3000
        31.0000   0.2000
        [torch.Tensor of dimension 4x2]
        ```
        
        The first column contains indices, the second column contains values in a a vector where all other elements are zeros. The indices should not exceed the stated dimensions of the input to the layer (10000 in the example).
        
        
        """

   'nn.Dropout':
      'prefix': 'nnDropout'
      'body': """
        <a name="nn.Dropout"></a>
        ## Dropout ##
        
        ```lua
        module = nn.Dropout(p)
        ```
        
        During training, `Dropout` masks parts of the `input` using binary samples from a [bernoulli](http://en.wikipedia.org/wiki/Bernoulli_distribution) distribution.
        Each `input` element has a probability of `p` of being dropped, i.e having its commensurate output element be zero. This has proven an effective technique for regularization and preventing the co-adaptation of neurons (see [Hinton et al. 2012](http://arxiv.org/abs/1207.0580)).
        
        Furthermore, the ouputs are scaled by a factor of `1/(1-p)` during training. This allows the `input` to be simply forwarded as-is during evaluation.
        
        In this example, we demonstrate how the call to [forward](module.md#output-forwardinput) samples different `outputs` to dropout (the zeros) given the same `input`:
        
        ```lua
        module = nn.Dropout()
        
        > x = torch.Tensor{{1, 2, 3, 4}, {5, 6, 7, 8}}
        
        > module:forward(x)
        2   0   0   8
        10   0  14   0
        [torch.DoubleTensor of dimension 2x4]
        
        > module:forward(x)
        0   0   6   0
        10   0   0   0
        [torch.DoubleTensor of dimension 2x4]
        ```
        
        [Backward](module.md#gradinput-backwardinput-gradoutput) drops out the gradients at the same location:
        
        ```lua
        > module:forward(x)
        0   4   0   0
        10  12   0  16
        [torch.DoubleTensor of dimension 2x4]
        
        > module:backward(x, x:clone():fill(1))
        0  2  0  0
        2  2  0  2
        [torch.DoubleTensor of dimension 2x4]
        ```
        
        In both cases the `gradOutput` and `input` are scaled by `1/(1-p)`, which in this case is `2`.
        
        During [evaluation](module.md#evaluate), `Dropout` does nothing more than forward the input such that all elements of the input are considered.
        
        ```lua
        > module:evaluate()
        
        > module:forward(x)
        1  2  3  4
        5  6  7  8
        [torch.DoubleTensor of dimension 2x4]
        ```
        
        We can return to training our model by first calling [Module:training()](module.md#training):
        
        ```lua
        > module:training()
        
        > return module:forward(x)
        2   4   6   0
        0   0   0  16
        [torch.DoubleTensor of dimension 2x4]
        ```
        
        When used, `Dropout` should normally be applied to the input of parameterized [Modules](module.md#nn.Module) like [Linear](#nn.Linear) or [SpatialConvolution](convolution.md#nn.SpatialConvolution). A `p` of `0.5` (the default) is usually okay for hidden layers. `Dropout` can sometimes be used successfully on the dataset inputs with a `p` around `0.2`. It sometimes works best following [Transfer](transfer.md) Modules like [ReLU](transfer.md#nn.ReLU). All this depends a great deal on the dataset so its up to the user to try different combinations.
        
        
        """

   'nn.SpatialDropout':
      'prefix': 'nnSpatialDropout'
      'body': """
        <a name="nn.SpatialDropout"></a>
        ## SpatialDropout ##
        
        `module` = `nn.SpatialDropout(p)`
        
        This version performs the same function as ```nn.Dropout```, however it assumes the 2 right-most dimensions of the input are spatial, performs one Bernoulli trial per output feature when training, and extends this dropout value across the entire feature map.
        
        As described in the paper "Efficient Object Localization Using Convolutional Networks" (http://arxiv.org/abs/1411.4280), if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then iid dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.  In this case, ```nn.SpatialDropout``` will help promote independence between feature maps and should be used instead.
        
        ```nn.SpatialDropout``` accepts 3D or 4D inputs.  If the input is 3D than a layout of (features x height x width) is assumed and for 4D (batch x features x height x width) is assumed.
        
        
        """

   'nn.Abs':
      'prefix': 'nnAbs'
      'body': """
        <a name="nn.Abs"></a>
        ## Abs ##
        
        ```lua
        module = Abs()
        ```
        
        ```lua
        m = nn.Abs()
        ii = torch.linspace(-5, 5)
        oo = m:forward(ii)
        go = torch.ones(100)
        gi = m:backward(ii, go)
        gnuplot.plot({'f(x)', ii, oo, '+-'}, {'df/dx', ii, gi, '+-'})
        gnuplot.grid(true)
        ```
        
        ![](image/abs.png)
        
        
        
        """

   'nn.Add'></a>
## Add ##

```lua
module = nn.Add(inputDimension, scalar)
```

Applies a bias term to the incoming data, i.e. `yi = x_i + b_i`,  or if `scalar = true` then uses a single bias term, `yi = x_i + b`.

Example:

```lua
y = torch.Tensor(5)
mlp = nn.Sequential()
mlp:add(nn.Add(5))

function gradUpdate(mlp, x, y, criterion, learningRate)
   local pred = mlp:forward(x)
   local err = criterion:forward(pred, y)
   local gradCriterion = criterion:backward(pred, y)
   mlp:zeroGradParameters()
   mlp:backward(x, gradCriterion)
   mlp:updateParameters(learningRate)
   return err
end

for i = 1, 10000 do
   x = torch.rand(5)
   y:copy(x);
   for i = 1, 5 do y[i] = y[i] + i; end
   err = gradUpdate(mlp, x, y, nn.MSECriterion(), 0.01)
end

print(mlp:get(1).bias)
```

gives the output:

```lua
 1.0000
 2.0000
 3.0000
 4.0000
 5.0000
[torch.Tensor of dimension 5]
```

i.e. the network successfully learns the input `x` has been shifted to produce the output `y`.


<a name=':
      'prefix': 'nnAdd'></a>
## Add ##

```lua
module = nnAdd(inputDimension, scalar)
```

Applies a bias term to the incoming data, ie `yi = x_i + b_i`,  or if `scalar = true` then uses a single bias term, `yi = x_i + b`

Example:

```lua
y = torchTensor(5)
mlp = nnSequential()
mlp:add(nnAdd(5))

function gradUpdate(mlp, x, y, criterion, learningRate)
   local pred = mlp:forward(x)
   local err = criterion:forward(pred, y)
   local gradCriterion = criterion:backward(pred, y)
   mlp:zeroGradParameters()
   mlp:backward(x, gradCriterion)
   mlp:updateParameters(learningRate)
   return err
end

for i = 1, 10000 do
   x = torchrand(5)
   y:copy(x);
   for i = 1, 5 do y[i] = y[i] + i; end
   err = gradUpdate(mlp, x, y, nnMSECriterion(), 001)
end

print(mlp:get(1)bias)
```

gives the output:

```lua
 10000
 20000
 30000
 40000
 50000
[torchTensor of dimension 5]
```

ie the network successfully learns the input `x` has been shifted to produce the output `y`


<a name='
      'body': """
        <a name='nn.Add'></a>
        ## Add ##
        
        ```lua
        module = nn.Add(inputDimension, scalar)
        ```
        
        Applies a bias term to the incoming data, i.e. `yi = x_i + b_i`,  or if `scalar = true` then uses a single bias term, `yi = x_i + b`.
        
        Example:
        
        ```lua
        y = torch.Tensor(5)
        mlp = nn.Sequential()
        mlp:add(nn.Add(5))
        
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        return err
        end
        
        for i = 1, 10000 do
        x = torch.rand(5)
        y:copy(x);
        for i = 1, 5 do y[i] = y[i] + i; end
        err = gradUpdate(mlp, x, y, nn.MSECriterion(), 0.01)
        end
        
        print(mlp:get(1).bias)
        ```
        
        gives the output:
        
        ```lua
        1.0000
        2.0000
        3.0000
        4.0000
        5.0000
        [torch.Tensor of dimension 5]
        ```
        
        i.e. the network successfully learns the input `x` has been shifted to produce the output `y`.
        
        
        
        """

   'nn.Mul':
      'prefix': 'nnMul'
      'body': """
        <a name="nn.Mul"></a>
        ## Mul ##
        
        ```lua
        module = nn.Mul()
        ```
        
        Applies a _single_ scaling factor to the incoming data, i.e. `y = w x`, where `w` is a scalar.
        
        Example:
        
        ```lua
        y = torch.Tensor(5)
        mlp = nn.Sequential()
        mlp:add(nn.Mul())
        
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        return err
        end
        
        for i = 1, 10000 do
        x = torch.rand(5)
        y:copy(x)
        y:mul(math.pi)
        err = gradUpdate(mlp, x, y, nn.MSECriterion(), 0.01)
        end
        
        print(mlp:get(1).weight)
        ```
        
        gives the output:
        
        ```lua
        3.1416
        [torch.Tensor of dimension 1]
        ```
        
        i.e. the network successfully learns the input `x` has been scaled by pi.
        
        
        """

   'nn.CMul'></a>
## CMul ##

```lua
module = nn.CMul(size)
```

Applies a component-wise multiplication to the incoming data, i.e. `y_i = w_i * x_i`. Argument `size` can be one or many numbers (sizes) or a `torch.LongStorage`. For example, `nn.CMul(3,4,5)` is equivalent to `nn.CMul(torch.LongStorage{3,4,5})`.

Example:

```lua
mlp = nn.Sequential()
mlp:add(nn.CMul(5))

y = torch.Tensor(5)
sc = torch.Tensor(5)
for i = 1, 5 do sc[i] = i; end -- scale input with this

function gradUpdate(mlp, x, y, criterion, learningRate)
   local pred = mlp:forward(x)
   local err = criterion:forward(pred, y)
   local gradCriterion = criterion:backward(pred, y)
   mlp:zeroGradParameters()
   mlp:backward(x, gradCriterion)
   mlp:updateParameters(learningRate)
   return err
end

for i = 1, 10000 do
   x = torch.rand(5)
   y:copy(x)
   y:cmul(sc)
   err = gradUpdate(mlp, x, y, nn.MSECriterion(), 0.01)
end

print(mlp:get(1).weight)
```

gives the output:

```lua
 1.0000
 2.0000
 3.0000
 4.0000
 5.0000
[torch.Tensor of dimension 5]
```

i.e. the network successfully learns the input `x` has been scaled by those scaling factors to produce the output `y`.


<a name=':
      'prefix': 'nnCMul'></a>
## CMul ##

```lua
module = nnCMul(size)
```

Applies a component-wise multiplication to the incoming data, ie `y_i = w_i * x_i` Argument `size` can be one or many numbers (sizes) or a `torchLongStorage` For example, `nnCMul(3,4,5)` is equivalent to `nnCMul(torchLongStorage{3,4,5})`

Example:

```lua
mlp = nnSequential()
mlp:add(nnCMul(5))

y = torchTensor(5)
sc = torchTensor(5)
for i = 1, 5 do sc[i] = i; end -- scale input with this

function gradUpdate(mlp, x, y, criterion, learningRate)
   local pred = mlp:forward(x)
   local err = criterion:forward(pred, y)
   local gradCriterion = criterion:backward(pred, y)
   mlp:zeroGradParameters()
   mlp:backward(x, gradCriterion)
   mlp:updateParameters(learningRate)
   return err
end

for i = 1, 10000 do
   x = torchrand(5)
   y:copy(x)
   y:cmul(sc)
   err = gradUpdate(mlp, x, y, nnMSECriterion(), 001)
end

print(mlp:get(1)weight)
```

gives the output:

```lua
 10000
 20000
 30000
 40000
 50000
[torchTensor of dimension 5]
```

ie the network successfully learns the input `x` has been scaled by those scaling factors to produce the output `y`


<a name='
      'body': """
        <a name='nn.CMul'></a>
        ## CMul ##
        
        ```lua
        module = nn.CMul(size)
        ```
        
        Applies a component-wise multiplication to the incoming data, i.e. `y_i = w_i * x_i`. Argument `size` can be one or many numbers (sizes) or a `torch.LongStorage`. For example, `nn.CMul(3,4,5)` is equivalent to `nn.CMul(torch.LongStorage{3,4,5})`.
        
        Example:
        
        ```lua
        mlp = nn.Sequential()
        mlp:add(nn.CMul(5))
        
        y = torch.Tensor(5)
        sc = torch.Tensor(5)
        for i = 1, 5 do sc[i] = i; end -- scale input with this
        
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        return err
        end
        
        for i = 1, 10000 do
        x = torch.rand(5)
        y:copy(x)
        y:cmul(sc)
        err = gradUpdate(mlp, x, y, nn.MSECriterion(), 0.01)
        end
        
        print(mlp:get(1).weight)
        ```
        
        gives the output:
        
        ```lua
        1.0000
        2.0000
        3.0000
        4.0000
        5.0000
        [torch.Tensor of dimension 5]
        ```
        
        i.e. the network successfully learns the input `x` has been scaled by those scaling factors to produce the output `y`.
        
        
        
        """

   'nn.Max':
      'prefix': 'nnMax'
      'body': """
        <a name="nn.Max"></a>
        ## Max ##
        
        ```lua
        module = nn.Max(dimension, nInputDim)
        ```
        
        Applies a max operation over dimension `dimension`.
        Hence, if an `nxpxq` Tensor was given as input, and `dimension` = `2` then an `nxq` matrix would be output.
        When `nInputDim` is provided, inputs larger than that value will be considered batches where the actual `dimension` to apply the max operation will be dimension `dimension + 1`.
        
        
        """

   'nn.Min':
      'prefix': 'nnMin'
      'body': """
        <a name="nn.Min"></a>
        ## Min ##
        
        ```lua
        module = nn.Min(dimension, nInputDim)
        ```
        
        Applies a min operation over dimension `dimension`.
        Hence, if an `nxpxq` Tensor was given as input, and `dimension` = `2` then an `nxq` matrix would be output.
        When `nInputDim` is provided, inputs larger than that value will be considered batches where the actual `dimension` to apply the min operation will be dimension `dimension + 1`.
        
        
        """

   'nn.Mean':
      'prefix': 'nnMean'
      'body': """
        <a name="nn.Mean"></a>
        ## Mean ##
        
        ```lua
        module = nn.Mean(dimension, nInputDim)
        ```
        
        Applies a mean operation over dimension `dimension`.
        Hence, if an `nxpxq` Tensor was given as input, and `dimension` = `2` then an `nxq` matrix would be output.
        When `nInputDim` is provided, inputs larger than that value will be considered batches where the actual `dimension` to apply the mean operation will be dimension `dimension + 1`.
        
        
        """

   'nn.Sum':
      'prefix': 'nnSum'
      'body': """
        <a name="nn.Sum"></a>
        ## Sum ##
        
        ```lua
        module = nn.Sum(dimension, nInputDim)
        ```
        
        Applies a sum operation over dimension `dimension`.
        Hence, if an `nxpxq` Tensor was given as input, and `dimension` = `2` then an `nxq` matrix would be output.
        When `nInputDim` is provided, inputs larger than that value will be considered batches where the actual `dimension` to apply the sum operation will be dimension `dimension + 1`.
        
        
        
        """

   'nn.Euclidean':
      'prefix': 'nnEuclidean'
      'body': """
        <a name="nn.Euclidean"></a>
        ## Euclidean ##
        
        ```lua
        module = nn.Euclidean(inputSize,outputSize)
        ```
        
        Outputs the Euclidean distance of the input to `outputSize` centers, i.e. this layer has the weights `w_j`,  for `j` = `1`,..,`outputSize`, where `w_j` are vectors of dimension `inputSize`.
        
        The distance `y_j` between center `j` and input `x` is formulated as `y_j = || w_j - x ||`.
        
        
        """

   'nn.WeightedEuclidean':
      'prefix': 'nnWeightedEuclidean'
      'body': """
        <a name="nn.WeightedEuclidean"></a>
        ## WeightedEuclidean ##
        
        ```lua
        module = nn.WeightedEuclidean(inputSize,outputSize)
        ```
        
        This module is similar to [Euclidean](#nn.Euclidean), but additionally learns a separate diagonal covariance matrix across the features of the input space _for each center_.
        
        In other words, for each of the `outputSize` centers `w_j`, there is a diagonal covariance matrices `c_j`, for `j` = `1`,..,`outputSize`, where `c_j` are stored as vectors of size `inputSize`.
        
        The distance `y_j` between center `j` and input `x` is formulated as `y_j = || c_j * (w_j - x) ||`.
        
        
        """

   'nn.Cosine':
      'prefix': 'nnCosine'
      'body': """
        <a name="nn.Cosine"></a>
        ## Cosine ##
        
        ```lua
        module = nn.Cosine(inputSize,outputSize)
        ```
        
        Outputs the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of the input to `outputSize` centers, i.e. this layer has the weights `w_j`,  for `j` = `1`,..,`outputSize`, where `w_j` are vectors of dimension `inputSize`.
        
        The distance `y_j` between center `j` and input `x` is formulated as `y_j = (x  w_j) / ( || w_j || * || x || )`.
        
        
        
        """

   'nn.Identity':
      'prefix': 'nnIdentity'
      'body': """
        <a name="nn.Identity"></a>
        ## Identity ##
        
        ```lua
        module = nn.Identity()
        ```
        
        
        Creates a module that returns whatever is input to it as output.
        This is useful when combined with the module [ParallelTable](table.md#nn.ParallelTable) in case you do not wish to do anything to one of the input Tensors.
        
        Example:
        
        ```lua
        mlp = nn.Identity()
        print(mlp:forward(torch.ones(5, 2)))
        ```
        
        gives the output:
        
        ```lua
        1  1
        1  1
        1  1
        1  1
        1  1
        [torch.Tensor of dimension 5x2]
        ```
        
        Here is a more useful example, where one can implement a network which also computes a Criterion using this module:
        
        ```lua
        pred_mlp = nn.Sequential()  -- A network that makes predictions given x.
        pred_mlp:add(nn.Linear(5, 4))
        pred_mlp:add(nn.Linear(4, 3))
        
        xy_mlp = nn.ParallelTable() -- A network for predictions and for keeping the
        xy_mlp:add(pred_mlp)        -- true label for comparison with a criterion
        xy_mlp:add(nn.Identity())   -- by forwarding both x and y through the network.
        
        mlp = nn.Sequential()       -- The main network that takes both x and y.
        mlp:add(xy_mlp)             -- It feeds x and y to parallel networks;
        cr = nn.MSECriterion()
        cr_wrap = nn.CriterionTable(cr)
        mlp:add(cr_wrap)            -- and then applies the criterion.
        
        for i = 1, 100 do           -- Do a few training iterations
        x = torch.ones(5)        -- Make input features.
        y = torch.Tensor(3)
        y:copy(x:narrow(1,1,3))  -- Make output label.
        err = mlp:forward{x,y}   -- Forward both input and output.
        print(err)               -- Print error from criterion.
        
        mlp:zeroGradParameters() -- Do backprop...
        mlp:backward({x, y})
        mlp:updateParameters(0.05)
        end
        ```
        
        
        """

   'nn.Copy':
      'prefix': 'nnCopy'
      'body': """
        <a name="nn.Copy"></a>
        ## Copy ##
        
        ```lua
        module = nn.Copy(inputType, outputType, [forceCopy, dontCast])
        ```
        
        This layer copies the input to output with type casting from `inputType` to `outputType`. Unless `forceCopy` is true, when the first two arguments are the same, the input isn't copied, only transfered as the output. The default `forceCopy` is false.
        When `dontCast` is true, a call to `nn.Copy:type(type)` will not cast the module's `output` and `gradInput` Tensors to the new type. The default is false.
        
        
        """

   'nn.Narrow':
      'prefix': 'nnNarrow'
      'body': """
        <a name="nn.Narrow"></a>
        ## Narrow ##
        
        ```lua
        module = nn.Narrow(dimension, offset, length)
        ```
        
        Narrow is application of [narrow](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-narrowdim-index-size) operation in a module.
        
        
        """

   'nn.Replicate':
      'prefix': 'nnReplicate'
      'body': """
        <a name="nn.Replicate"></a>
        ## Replicate ##
        
        ```lua
        module = nn.Replicate(nFeature [, dim, ndim])
        ```
        
        This class creates an output where the input is replicated `nFeature` times along dimension `dim` (default 1). 
        There is no memory allocation or memory copy in this module. 
        It sets the [stride](https://github.com/torch/torch7/blob/master/doc/tensor.md#torch.Tensor.stride) along the `dim`th dimension to zero.
        When provided, `ndim` should specify the number of non-batch dimensions.
        This allows the module to replicate the same non-batch dimension `dim` for both batch and non-batch `inputs`.
        
        ```lua
        > x = torch.linspace(1, 5, 5)
        1
        2
        3
        4
        5
        [torch.DoubleTensor of dimension 5]
        
        > m = nn.Replicate(3)
        > o = m:forward(x)
        1  2  3  4  5
        1  2  3  4  5
        1  2  3  4  5
        [torch.DoubleTensor of dimension 3x5]
        
        > x:fill(13)
        13
        13
        13
        13
        13
        [torch.DoubleTensor of dimension 5]
        
        > print(o)
        13  13  13  13  13
        13  13  13  13  13
        13  13  13  13  13
        [torch.DoubleTensor of dimension 3x5]
        ```
        
        
        
        """

   'nn.Reshape':
      'prefix': 'nnReshape'
      'body': """
        <a name="nn.Reshape"></a>
        ## Reshape ##
        
        ```lua
        module = nn.Reshape(dimension1, dimension2, ... [, batchMode])
        ```
        
        
        Reshapes an `nxpxqx..`  Tensor into a `dimension1xdimension2x...` Tensor, taking the elements row-wise.
        
        The optional last argument `batchMode`, when `true` forces the first dimension of the input to be considered the batch dimension, and thus keep its size fixed. This is necessary when dealing with batch sizes of one. When `false`, it forces the entire input (including the first dimension) to be reshaped to the input size. Default `batchMode=nil`, which means that the module considers inputs with more elements than the produce of provided sizes, i.e. `dimension1xdimension2x...`, to be batches.
        
        Example:
        
        ```lua
        > x = torch.Tensor(4,4)
        > for i = 1, 4 do
        >    for j = 1, 4 do
        >       x[i][j] = (i-1)*4+j
        >    end
        > end
        > print(x)
        
        1   2   3   4
        5   6   7   8
        9  10  11  12
        13  14  15  16
        [torch.Tensor of dimension 4x4]
        
        > print(nn.Reshape(2,8):forward(x))
        
        1   2   3   4   5   6   7   8
        9  10  11  12  13  14  15  16
        [torch.Tensor of dimension 2x8]
        
        > print(nn.Reshape(8,2):forward(x))
        
        1   2
        3   4
        5   6
        7   8
        9  10
        11  12
        13  14
        15  16
        [torch.Tensor of dimension 8x2]
        
        > print(nn.Reshape(16):forward(x))
        
        1
        2
        3
        4
        5
        6
        7
        8
        9
        10
        11
        12
        13
        14
        15
        16
        [torch.Tensor of dimension 16]
        
        > y = torch.Tensor(1, 4):fill(0)
        > print(y)
        
        0  0  0  0
        [torch.DoubleTensor of dimension 1x4]
        
        > print(nn.Reshape(4):forward(y))
        
        0  0  0  0
        [torch.DoubleTensor of dimension 1x4]
        
        > print(nn.Reshape(4, false):forward(y))
        
        0
        0
        0
        0
        [torch.DoubleTensor of dimension 4]
        
        ```
        
        
        """

   'nn.View':
      'prefix': 'nnView'
      'body': """
        <a name="nn.View"></a>
        ## View ##
        
        ```lua
        module = nn.View(sizes)
        ```
        
        This module creates a new view of the input tensor using the `sizes` passed to the constructor. The parameter `sizes` can either be a `LongStorage` or numbers.
        The method `setNumInputDims()` allows to specify the expected number of dimensions of the inputs of the modules. This makes it possible to use minibatch inputs when using a size `-1` for one of the dimensions.
        
        Example 1:
        
        ```lua
        > x = torch.Tensor(4, 4)
        > for i = 1, 4 do
        >    for j = 1, 4 do
        >       x[i][j] = (i-1)*4+j
        >    end
        > end
        > print(x)
        
        1   2   3   4
        5   6   7   8
        9  10  11  12
        13  14  15  16
        [torch.Tensor of dimension 4x4]
        
        > print(nn.View(2, 8):forward(x))
        
        1   2   3   4   5   6   7   8
        9  10  11  12  13  14  15  16
        [torch.DoubleTensor of dimension 2x8]
        
        > print(nn.View(torch.LongStorage{8,2}):forward(x))
        
        1   2
        3   4
        5   6
        7   8
        9  10
        11  12
        13  14
        15  16
        [torch.DoubleTensor of dimension 8x2]
        
        > print(nn.View(16):forward(x))
        
        1
        2
        3
        4
        5
        6
        7
        8
        9
        10
        11
        12
        13
        14
        15
        16
        [torch.DoubleTensor of dimension 16]
        ```
        
        Example 2:
        ```lua
        > input = torch.Tensor(2, 3)
        > minibatch = torch.Tensor(5, 2, 3)
        > m = nn.View(-1):setNumInputDims(2)
        > print(#m:forward(input))
        
        6
        [torch.LongStorage of size 1]
        
        > print(#m:forward(minibatch))
        
        5
        6
        [torch.LongStorage of size 2]
        ```
        
        
        """

   'nn.Select':
      'prefix': 'nnSelect'
      'body': """
        <a name="nn.Select"></a>
        ## Select ##
        
        ```lua
        module = nn.Select(dim, index)
        ```
        
        Selects a dimension and index of a  `nxpxqx..`  Tensor.
        
        Example:
        
        ```lua
        mlp = nn.Sequential()
        mlp:add(nn.Select(1, 3))
        
        x = torch.randn(10, 5)
        print(x)
        print(mlp:forward(x))
        ```
        
        gives the output:
        
        ```lua
        0.9720 -0.0836  0.0831 -0.2059 -0.0871
        0.8750 -2.0432 -0.1295 -2.3932  0.8168
        0.0369  1.1633  0.6483  1.2862  0.6596
        0.1667 -0.5704 -0.7303  0.3697 -2.2941
        0.4794  2.0636  0.3502  0.3560 -0.5500
        -0.1898 -1.1547  0.1145 -1.1399  0.1711
        -1.5130  1.4445  0.2356 -0.5393 -0.6222
        -0.6587  0.4314  1.1916 -1.4509  1.9400
        0.2733  1.0911  0.7667  0.4002  0.1646
        0.5804 -0.5333  1.1621  1.5683 -0.1978
        [torch.Tensor of dimension 10x5]
        
        0.0369
        1.1633
        0.6483
        1.2862
        0.6596
        [torch.Tensor of dimension 5]
        ```
        
        This can be used in conjunction with [Concat](containers.md#nn.Concat) to emulate the behavior of [Parallel](containers.md#nn.Parallel), or to select various parts of an input Tensor to perform operations on. Here is a fairly complicated example:
        
        ```lua
        mlp = nn.Sequential()
        c = nn.Concat(2)
        for i = 1, 10 do
        local t = nn.Sequential()
        t:add(nn.Select(1, i))
        t:add(nn.Linear(3, 2))
        t:add(nn.Reshape(2, 1))
        c:add(t)
        end
        mlp:add(c)
        
        pred = mlp:forward(torch.randn(10, 3))
        print(pred)
        
        for i = 1, 10000 do     -- Train for a few iterations
        x = torch.randn(10, 3)
        y = torch.ones(2, 10)
        pred = mlp:forward(x)
        
        criterion = nn.MSECriterion()
        err = criterion:forward(pred, y)
        gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(0.01)
        print(err)
        end
        ```
        
        
        """

   'nn.Index':
      'prefix': 'nnIndex'
      'body': """
        <a name="nn.Index"></a>
        ## Index ##
        
        ```lua
        module = nn.Index(dim)
        ```
        
        Applies the Tensor [index](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor-indexdim-index) operation along the given dimension. So
        
        ```lua 
        nn.Index(dim):forward{t,i} 
        ```
        gives the same output as 
        ```lua
        t:index(dim, i)
        ```
        
        
        """

   'nn.Exp':
      'prefix': 'nnExp'
      'body': """
        <a name="nn.Exp"></a>
        ## Exp ##
        
        ```lua
        module = nn.Exp()
        ```
        
        Applies the `exp` function element-wise to the input Tensor, thus outputting a Tensor of the same dimension.
        
        ```lua
        ii = torch.linspace(-2, 2)
        m = nn.Exp()
        oo = m:forward(ii)
        go = torch.ones(100)
        gi = m:backward(ii,go)
        gnuplot.plot({'f(x)', ii, oo, '+-'}, {'df/dx', ii, gi, '+-'})
        gnuplot.grid(true)
        ```
        
        ![](image/exp.png)
        
        
        
        """

   'nn.Square':
      'prefix': 'nnSquare'
      'body': """
        <a name="nn.Square"></a>
        ## Square ##
        
        ```lua
        module = nn.Square()
        ```
        
        Takes the square of each element.
        
        ```lua
        ii = torch.linspace(-5, 5)
        m = nn.Square()
        oo = m:forward(ii)
        go = torch.ones(100)
        gi = m:backward(ii, go)
        gnuplot.plot({'f(x)', ii, oo, '+-'}, {'df/dx', ii, gi, '+-'})
        gnuplot.grid(true)
        ```
        
        ![](image/square.png)
        
        
        
        """

   'nn.Sqrt':
      'prefix': 'nnSqrt'
      'body': """
        <a name="nn.Sqrt"></a>
        ## Sqrt ##
        
        ```lua
        module = nn.Sqrt()
        ```
        
        Takes the square root of each element.
        
        ```lua
        ii = torch.linspace(0, 5)
        m = nn.Sqrt()
        oo = m:forward(ii)
        go = torch.ones(100)
        gi = m:backward(ii, go)
        gnuplot.plot({'f(x)', ii, oo, '+-'}, {'df/dx', ii, gi, '+-'})
        gnuplot.grid(true)
        ```
        
        ![](image/sqrt.png)
        
        
        
        """

   'nn.Power':
      'prefix': 'nnPower'
      'body': """
        <a name="nn.Power"></a>
        ## Power ##
        
        ```lua
        module = nn.Power(p)
        ```
        
        Raises each element to its `p`-th power.
        
        ```lua
        ii = torch.linspace(0, 2)
        m = nn.Power(1.25)
        oo = m:forward(ii)
        go = torch.ones(100)
        gi = m:backward(ii, go)
        gnuplot.plot({'f(x)', ii, oo, '+-'}, {'df/dx', ii, gi, '+-'})
        gnuplot.grid(true)
        ```
        
        ![](image/power.png)
        
        
        """

   'nn.Clamp':
      'prefix': 'nnClamp'
      'body': """
        <a name="nn.Clamp"></a>
        ## Clamp ##
        
        ```lua
        module = nn.Clamp(min_value, max_value)
        ```
        
        Clamps all elements into the range `[min_value, max_value]`.
        Output is identical to input in the range, otherwise elements less than `min_value` (or greater than `max_value`) are saturated to `min_value` (or `max_value`).
        
        ```lua
        A = torch.randn(2, 5)
        m = nn.Clamp(-0.1, 0.5)
        B = m:forward(A)
        
        print(A)  -- input
        -1.1321  0.0227 -0.4672  0.6519 -0.5380
        0.9061 -1.0858  0.3697 -0.8120 -1.6759
        [torch.DoubleTensor of size 3x5]
        
        print(B)  -- output
        -0.1000  0.0227 -0.1000  0.5000 -0.1000
        0.5000 -0.1000  0.3697 -0.1000 -0.1000
        [torch.DoubleTensor of size 3x5]
        ```
        
        
        """

   'nn.Normalize':
      'prefix': 'nnNormalize'
      'body': """
        <a name="nn.Normalize"></a>
        ## Normalize ##
        
        ```lua
        module = nn.Normalize(p, [eps])
        ```
        Normalizes the input Tensor to have unit `L_p` norm. The smoothing parameter `eps` prevents division by zero when the input contains all zero elements (default = `1e-10`).
        
        Input can be 1D or 2D (in which case it's considered as in batch mode)
        
        ```lua
        A = torch.randn(3, 5)
        m = nn.Normalize(2)
        B = m:forward(A) -- B is also 3 x 5
        -- take the L2 norm over the second axis:
        print(torch.norm(B, 2, 2)) -- norms is [1, 1, 1]
        ```
        
        `Normalize` has a specialized implementation for the `inf` norm, which corresponds to the maximum norm.
        ```lua
        A = torch.randn(3,5)
        m = nn.Normalize(math.huge) -- uses maximum/inf norm
        B = m:forward(A)
        maxA = torch.abs(A):max(2)
        print(A,B,maxA)
        ```
        
        
        """

   'nn.MM':
      'prefix': 'nnMM'
      'body': """
        <a name="nn.MM"></a>
        ## MM ##
        
        ```lua
        module = nn.MM(transA, transB)
        ```
        
        Performs multiplications on one or more pairs of matrices. If `transA` is set to true, the first matrix is transposed before multiplication. If `transB` is set to true, the second matrix is transposed before multiplication. By default, the matrices do not get transposed.
        
        The module also accepts 3D inputs which are interpreted as batches of matrices. When using batches, the first input matrix should be of size `b x m x n` and the second input matrix should be of size `b x n x p` (assuming `transA` and `transB` are not set). If `transA` or `transB` is set, transpose takes place between the second and the third dimensions for the corresponding matrix.
        
        ```lua
        model = nn.MM()
        A = torch.randn(b, m, n)
        B = torch.randn(b, n, p)
        C = model:forward({A, B})  -- C will be of size `b x m x p`
        
        model = nn.MM(true, false)
        A = torch.randn(b, n, m)
        B = torch.randn(b, n, p)
        C = model:forward({A, B})  -- C will be of size `b x m x p`
        ```
        
        
        
        """

   'nn.BatchNormalization':
      'prefix': 'nnBatchNormalization'
      'body': """
        <a name="nn.BatchNormalization"></a>
        ## BatchNormalization ##
        
        ```lua
        module = nn.BatchNormalization(N [, eps] [, momentum] [,affine])
        ```
        where `N` is the dimensionality of input
        `eps` is a small value added to the standard-deviation to avoid divide-by-zero. Defaults to `1e-5`.
        `affine` is a boolean. When set to false, the learnable affine transform is disabled. Defaults to true
        
        During training, this layer keeps a running estimate of its computed mean and std.
        The running sum is kept with a default momentum of 0.1 (unless over-ridden)
        During evaluation, this running mean/std is used for normalization.
        
        Implements Batch Normalization as described in [the paper](http://arxiv.org/pdf/1502.03167v3.pdf): "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" by Sergey Ioffe, Christian Szegedy.
        
        The operation implemented is:
        
        ```lua
        x - mean(x)
        y =  ----------------------------- * gamma + beta
        standard-deviation(x) + eps
        ```
        
        where the mean and standard-deviation are calculated per-dimension over the mini-batches and where gamma and beta are learnable parameter vectors of size `N` (where `N` is the input size).
        The learning of gamma and beta is optional.
        The module only accepts 2D inputs.
        
        ```lua
        -- with learnable parameters
        model = nn.BatchNormalization(m)
        A = torch.randn(b, m)
        C = model:forward(A)  -- C will be of size `b x m`
        
        -- without learnable parameters
        model = nn.BatchNormalization(m, nil, nil, false)
        A = torch.randn(b, m)
        C = model:forward(A)  -- C will be of size `b x m`
        ```
        
        
        """

   'nn.Padding':
      'prefix': 'nnPadding'
      'body': """
        <a name="nn.Padding"></a>
        ## Padding ##
        
        `module` = `nn.Padding(dim, pad [, nInputDim, value])`
        
        This module adds `pad` units of padding to dimension `dim` of the input.
        If `pad` is negative, padding is added to the left, otherwise, it is added to the right of the dimension. When `nInputDim` is provided, inputs larger than that value will be considered batches where the actual `dim` to be padded will
        be dimension `dim + 1`. When `value` is provide, the padding will be filled with that `value`. The default `value` is zero.
        
        Example 1:
        
        ```lua
        module = nn.Padding(1, 2, 1, -1) --pad right x2
        module:forward(torch.randn(3)) --non-batch input
        0.2008
        0.4848
        -1.0783
        -1.0000
        -1.0000
        [torch.DoubleTensor of dimension 5]
        ```
        
        Example 2:
        
        ```lua
        module = nn.Padding(1, -2, 1, -1) --pad left x2
        module:forward(torch.randn(2, 3)) --batch input
        -1.0000 -1.0000  1.0203  0.2704 -1.6164
        -1.0000 -1.0000 -0.2219 -0.6529 -1.9218
        [torch.DoubleTensor of dimension 2x5]
        ```
        
        
        
        """

   'nn.L1Penalty':
      'prefix': 'nnL1Penalty'
      'body': """
        <a name="nn.L1Penalty"></a>
        ## L1Penalty ##
        
        ```lua
        penalty = nn.L1Penalty(L1weight, sizeAverage)
        ```
        
        L1Penalty is an inline module that in its forward propagation copies the input Tensor directly to the output, and computes an L1 loss of the latent state (input) and stores it in the module's `loss` field.
        During backward propagation: `gradInput = gradOutput + gradLoss`.
        
        This module can be used in autoencoder architectures to apply L1 losses to internal latent state without having to use Identity and parallel containers to carry the internal code to an output criterion.
        
        Example (sparse autoencoder, note: decoder should be normalized):
        
        ```lua
        encoder = nn.Sequential()
        encoder:add(nn.Linear(3, 128))
        encoder:add(nn.Threshold())
        decoder = nn.Linear(128, 3)
        
        autoencoder = nn.Sequential()
        autoencoder:add(encoder)
        autoencoder:add(nn.L1Penalty(l1weight))
        autoencoder:add(decoder)
        
        criterion = nn.MSECriterion()  -- To measure reconstruction error
        -- ...
        ```
        
        
        """

   'nn.GradientReversal':
      'prefix': 'nnGradientReversal'
      'body': """
        <a name="nn.GradientReversal"></a>
        ## GradientReversal ##
        
        `module` = `nn.GradientReversal()`
        
        This module preserves the input, but reverses the gradient. This can be used to maximise an objective function whilst using gradient descent, as in "Domain-Adversarial Training of Neural Networks" (http://arxiv.org/abs/1505.07818).
        
        """

   'torch.Storage.dok':
      'prefix': 'torchStoragedok'
      'body': """
        <a name="torch.Storage.dok"></a>
        # Storage #
        
        """

   'torch.CharStorage.dok':
      'prefix': 'torchCharStoragedok'
      'body': """
        <a name="torch.CharStorage.dok"></a>
        
        """

   'torch.ByteStorage.dok':
      'prefix': 'torchByteStoragedok'
      'body': """
        <a name="torch.ByteStorage.dok"></a>
        
        """

   'torch.IntStorage.dok':
      'prefix': 'torchIntStoragedok'
      'body': """
        <a name="torch.IntStorage.dok"></a>
        
        """

   'torch.ShortStorage.dok':
      'prefix': 'torchShortStoragedok'
      'body': """
        <a name="torch.ShortStorage.dok"></a>
        
        """

   'torch.FloatStorage.dok':
      'prefix': 'torchFloatStoragedok'
      'body': """
        <a name="torch.FloatStorage.dok"></a>
        
        """

   'torch.LongStorage.dok':
      'prefix': 'torchLongStoragedok'
      'body': """
        <a name="torch.LongStorage.dok"></a>
        
        """

   'torch.DoubleStorage.dok':
      'prefix': 'torchDoubleStoragedok'
      'body': """
        <a name="torch.DoubleStorage.dok"></a>
        
        _Storages_ are basically a way for `Lua` to access memory of a `C` pointer
        or array. _Storages_ can also [map the contents of a file to memory](#__torch.StorageMap).
        A `Storage` is an array of _basic_ `C` types. For arrays of `Torch` objects,
        use the `Lua` tables.
        
        Several `Storage` classes for all the basic `C` types exist and have the
        following self-explanatory names: `ByteStorage`, `CharStorage`, `ShortStorage`,
        `IntStorage`, `LongStorage`, `FloatStorage`, `DoubleStorage`.
        
        Note that `ByteStorage` and `CharStorage` represent both arrays of bytes. `ByteStorage` represents an array of
        _unsigned_ chars, while `CharStorage` represents an array of _signed_ chars.
        
        Conversions between two `Storage` type might be done using `copy`:
        ```lua
        x = torch.IntStorage(10):fill(1)
        y = torch.DoubleStorage(10):copy(x)
        ```
        
        [Classical storages](#torch.Storage) are [serializable](file.md#torch.File.serialization).
        [Storages mapping a file](#__torch.StorageMap) are also [serializable](file.md#torch.File.serialization),
        but _will be saved as a normal storage_. High-level serialization commands are described in the
        [serialization](serialization.md) section.
        
        An alias `torch.Storage()` is made over your preferred Storage type,
        controlled by the
        [torch.setdefaulttensortype](utility.md#torch.setdefaulttensortype)
        function. By default, this "points" on `torch.DoubleStorage`.
        
        ## Constructors and Access Methods ##
        
        
        """

   'torch.Storage':
      'prefix': 'torchStorage'
      'body': """
        <a name="torch.Storage"></a>
        ### torch.TYPEStorage([size [, ptr]]) ###
        
        Returns a new `Storage` of type `TYPE`. Valid `TYPE` are `Byte`, `Char`, `Short`,
        `Int`, `Long`, `Float`, and `Double`. If `size` is given, resize the
        `Storage` accordingly, else create an empty `Storage`.
        The optional second argument `ptr` is a number whose value is a
        pointer to a memory chunk of size `size*sizeof(TYPE)` (for example coming from the
        [`torch.data()`](https://github.com/torch/torch7/blob/master/doc/tensor.md#result-datatensor-asnumber)
        method). The Storage will take care of freeing the memory
        chunk: it _must not be freed by the caller_!
        
        Example:
        ```lua
        -- Creates a Storage of 10 double:
        x = torch.DoubleStorage(10)
        ```
        
        The data in the `Storage` is _uninitialized_.
        
        
        """

   'torch.Storage':
      'prefix': 'torchStorage'
      'body': """
        <a name="torch.Storage"></a>
        ### torch.TYPEStorage(table) ###
        
        `table` is assumed to be a Lua array of numbers. The constructor returns a new storage of the specified `TYPE`,
        of the size of the table, containing all the table elements converted
        
        Example:
        ```lua
        > = torch.IntStorage({1,2,3,4})
        
        1
        2
        3
        4
        [torch.IntStorage of size 4]
        ```
        
        
        """

   'torch.Storage':
      'prefix': 'torchStorage'
      'body': """
        <a name="torch.Storage"></a>
        ### torch.TYPEStorage(storage [, offset [, size]]) ###
        
        Returns a new `Storage` of type `TYPE`, which is a view on the first argument. The first argument must be of the same type `TYPE`. An optional `offset` can be provided (defaults to 1). An optional `size` can also be provided to restrict the size of the new storage (defaults to `storage:size()-(offset-1)`).
        
        Example:
        ```lua
        -- Creates a Storage of 10 double:
        > x = torch.DoubleStorage(10)
        
        -- Creates a view on this Storage, starting at offset 3, with a size of 5:
        > y = torch.DoubleStorage(x, 3, 5)
        
        -- Modifying elements of y will modify x:
        > x:fill(0)
        > y:fill(1)
        > print(x)
        0
        0
        1
        1
        1
        1
        1
        0
        0
        0
        [torch.DoubleStorage of size 10]
        ```
        
        
        """

   'torch.Storage':
      'prefix': 'torchStorage'
      'body': """
        <a name="torch.Storage"></a>
        ### torch.TYPEStorage(filename [, shared [, size [, sharedMem]]]) ###
        
        """

   '__torch.StorageMap':
      'prefix': '__torchStorageMap'
      'body': """
        <a name="__torch.StorageMap"></a>
        
        Returns a new kind of `Storage` which maps the contents of the given
        `filename` to memory. Valid `TYPE` are `Byte`, `Char`, `Short`, `Int`, `Long`,
        `Float`, and `Double`. If the optional boolean argument `shared` is `true`,
        the mapped memory is shared amongst all processes on the computer.
        
        When `shared` is `true`, the file must be accessible in read-write mode. Any
        changes on the storage will be written in the file. The changes might be written
        only after destruction of the storage.
        
        When `shared` is `false` (or not provided), the file must be at least
        readable. Any changes on the storage will not affect the file. Note:
        changes made on the file after creation of the storage have an unspecified
        effect on the storage contents.
        
        If `size` is specified, it is the [size](#torch.Storage.size) of the returned
        `Storage` (in elements). In this case, if `shared` is `false` then the file must
        already contain at least
        ```lua
        size*(size of TYPE)
        ```
        bytes. If `shared` is `true` then the file will be created if necessary, and
        extended if necessary to that many bytes in length.
        
        If `size` is not specified then the [size](#torch.Storage.size) of the returned
        `Storage`  will be
        ```lua
        (size of file in byte)/(size of TYPE)
        ```
        elements provided a non empty file already exists.
        
        If `sharedMem` is true then, the file will be created (or mapped) from the shared
        memory area using [`shm_open()`](http://linux.die.net/man/3/shm_open). On Linux systems
        this is implemented at `/dev/shm` partition on RAM for interprocess communication.
        
        
        Example:
        ```lua
        $ echo "Hello World" > hello.txt
        $ lua
        Lua 5.1.3  Copyright (C) 1994-2008 Lua.org, PUC-Rio
        > require 'torch'
        > x = torch.CharStorage('hello.txt')
        > = x
        72
        101
        108
        108
        111
        32
        87
        111
        114
        108
        100
        10
        [torch.CharStorage of size 12]
        
        > = x:string()
        Hello World
        
        > = x:fill(42):string()
        ************
        >
        $ cat hello.txt
        Hello World
        $ lua
        Lua 5.1.3  Copyright (C) 1994-2008 Lua.org, PUC-Rio
        > require 'torch'
        > x = torch.CharStorage('hello.txt', true)
        > = x:string()
        Hello World
        
        > x:fill(42)
        >
        $ cat hello.txt
        ************
        ```
        
        
        """

   '__torch.StorageSharp':
      'prefix': '__torchStorageSharp'
      'body': """
        <a name="__torch.StorageSharp"></a>
        ### [number] #self ###
        
        Returns the number of elements in the storage. Equivalent to [size()](#torch.Storage.size).
        
        
        """

   'torch.Storage.__index__':
      'prefix': 'torchStorage__index__'
      'body': """
        <a name="torch.Storage.__index__"></a>
        ### [number] self[index] ###
        
        Returns or set the element at position `index` in the storage. Valid range
        of `index` is 1 to [size()](#torch.Storage.size).
        
        Example:
        ```lua
        x = torch.DoubleStorage(10)
        print(x[5])
        ```
        
        
        """

   'torch.Storage.copy':
      'prefix': 'torchStoragecopy'
      'body': """
        <a name="torch.Storage.copy"></a>
        ### [self] copy(storage) ###
        
        Copy another `storage`. The types of the two storages might be different: in that case
        a conversion of types occur (which might result, of course, in loss of precision or rounding).
        This method returns self, allowing things like:
        ```lua
        x = torch.IntStorage(10):fill(1)
        y = torch.DoubleStorage(10):copy(x) -- y won't be nil!
        ```
        
        
        """

   'torch.Storage.fill':
      'prefix': 'torchStoragefill'
      'body': """
        <a name="torch.Storage.fill"></a>
        ### [self] fill(value) ###
        
        Fill the `Storage` with the given value. This method returns self, allowing things like:
        ```lua
        x = torch.IntStorage(10):fill(0) -- x won't be nil!
        ```
        
        
        """

   'torch.Storage.resize':
      'prefix': 'torchStorageresize'
      'body': """
        <a name="torch.Storage.resize"></a>
        ### [self] resize(size) ###
        
        Resize the storage to the provided `size`. _The new contents are undetermined_.
        
        This function returns self, allowing things like:
        ```lua
        x = torch.DoubleStorage(10):fill(1)
        y = torch.DoubleStorage():resize(x:size()):copy(x) -- y won't be nil!
        ```
        
        
        """

   'torch.Storage.size':
      'prefix': 'torchStoragesize'
      'body': """
        <a name="torch.Storage.size"></a>
        ### [number] size() ###
        
        Returns the number of elements in the storage. Equivalent to [#](#__torch.StorageSharp).
        
        
        """

   'torch.Storage.string':
      'prefix': 'torchStoragestring'
      'body': """
        <a name="torch.Storage.string"></a>
        ### [self] string(str) ###
        
        This function is available only on `ByteStorage` and `CharStorage`.
        
        This method resizes the storage to the length of the provided
        string `str`, and copy the contents of `str` into the storage. The `NULL` terminating character is not copied,
        but `str` might contain `NULL` characters. The method returns the `Storage`.
        ```lua
        > x = torch.CharStorage():string("blah blah")
        > print(x)
        98
        108
        97
        104
        32
        98
        108
        97
        104
        [torch.CharStorage of size 9]
        ```
        
        
        """

   'torch.Storage.string':
      'prefix': 'torchStoragestring'
      'body': """
        <a name="torch.Storage.string"></a>
        ### [string] string() ###
        
        This function is available only on `ByteStorage` and `CharStorage`.
        
        The contents of the storage viewed as a string are returned. The string might contain
        `NULL` characters.
        ```lua
        > x = torch.CharStorage():string("blah blah")
        > print(x:string())
        blah blah
        ```
        
        ## Reference counting methods ##
        
        Storages are reference-counted. It means that each time an object (C or the
        Lua state) need to keep a reference over a storage, the corresponding
        storage reference counter will be [increased](#torch.Storage.retain). The
        reference counter is [decreased]((#torch.Storage.free)) when the object
        does not need the storage anymore.
        
        These methods should be used with extreme care. In general, they should
        never be called, except if you know what you are doing, as the handling of
        references is done automatically. They can be useful in threaded
        environments. Note that these methods are atomic operations.
        
        
        """

   'torch.Storage.retain':
      'prefix': 'torchStorageretain'
      'body': """
        <a name="torch.Storage.retain"></a>
        ### retain() ###
        
        Increment the reference counter of the storage.
        
        
        """

   'torch.Storage.free':
      'prefix': 'torchStoragefree'
      'body': """
        <a name="torch.Storage.free"></a>
        ### free() ###
        
        Decrement the reference counter of the storage. Free the storage if the
        counter is at 0.
        
        """

   'nn.TableLayers':
      'prefix': 'nnTableLayers'
      'body': """
        <a name="nn.TableLayers"></a>
        # Table Layers #
        
        This set of modules allows the manipulation of `table`s through the layers of a neural network.
        This allows one to build very rich architectures:
        
        * `table` Container Modules encapsulate sub-Modules:
        * [`ConcatTable`](#nn.ConcatTable): applies each member module to the same input     [`Tensor`](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor) and outputs a `table`;
        * [`ParallelTable`](#nn.ParallelTable): applies the `i`-th member module to the `i`-th input and outputs a `table`;
        * Table Conversion Modules convert between `table`s and `Tensor`s or `table`s:
        * [`SplitTable`](#nn.SplitTable): splits a `Tensor` into a `table` of `Tensor`s;
        * [`JoinTable`](#nn.JoinTable): joins a `table` of `Tensor`s into a `Tensor`;
        * [`MixtureTable`](#nn.MixtureTable): mixture of experts weighted by a gater;
        * [`SelectTable`](#nn.SelectTable): select one element from a `table`;
        * [`NarrowTable`](#nn.NarrowTable): select a slice of elements from a `table`;
        * [`FlattenTable`](#nn.FlattenTable): flattens a nested `table` hierarchy;
        * Pair Modules compute a measure like distance or similarity from a pair (`table`) of input `Tensor`s:
        * [`PairwiseDistance`](#nn.PairwiseDistance): outputs the `p`-norm. distance between inputs;
        * [`DotProduct`](#nn.DotProduct): outputs the dot product (similarity) between inputs;
        * [`CosineDistance`](#nn.CosineDistance): outputs the cosine distance between inputs;
        * CMath Modules perform element-wise operations on a `table` of `Tensor`s:
        * [`CAddTable`](#nn.CAddTable): addition of input `Tensor`s;
        * [`CSubTable`](#nn.CSubTable): substraction of input `Tensor`s;
        * [`CMulTable`](#nn.CMulTable): multiplication of input `Tensor`s;
        * [`CDivTable`](#nn.CDivTable): division of input `Tensor`s;
        * `Table` of Criteria:
        * [`CriterionTable`](#nn.CriterionTable): wraps a [Criterion](criterion.md#nn.Criterion) so that it can accept a `table` of inputs.
        
        `table`-based modules work by supporting `forward()` and `backward()` methods that can accept `table`s as inputs.
        It turns out that the usual [`Sequential`](containers.md#nn.Sequential) module can do this, so all that is needed is other child modules that take advantage of such `table`s.
        
        ```lua
        mlp = nn.Sequential()
        t = {x, y, z}
        pred = mlp:forward(t)
        pred = mlp:forward{x, y, z}      -- This is equivalent to the line before
        ```
        
        
        """

   'nn.ConcatTable':
      'prefix': 'nnConcatTable'
      'body': """
        <a name="nn.ConcatTable"></a>
        ## ConcatTable ##
        
        ```lua
        module = nn.ConcatTable()
        ```
        
        `ConcatTable` is a container module that applies each member module to the same input [`Tensor`](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor) or `table`.
        
        ```
        +-----------+
        +----> {member1, |
        +-------+    |    |           |
        | input +----+---->  member2, |
        +-------+    |    |           |
        or        +---->  member3} |
        {input}          +-----------+
        ```
        
        ### Example 1
        
        ```lua
        mlp = nn.ConcatTable()
        mlp:add(nn.Linear(5, 2))
        mlp:add(nn.Linear(5, 3))
        
        pred = mlp:forward(torch.randn(5))
        for i, k in ipairs(pred) do print(i, k) end
        ```
        
        which gives the output:
        
        ```lua
        1
        -0.4073
        0.0110
        [torch.Tensor of dimension 2]
        
        2
        0.0027
        -0.0598
        -0.1189
        [torch.Tensor of dimension 3]
        ```
        
        ### Example 2
        
        ```lua
        mlp = nn.ConcatTable()
        mlp:add(nn.Identity())
        mlp:add(nn.Identity())
        
        pred = mlp:forward{torch.randn(2), {torch.randn(3)}}
        print(pred)
        ```
        
        which gives the output (using [th](https://github.com/torch/trepl)):
        
        ```lua
        {
        1 :
        {
        1 : DoubleTensor - size: 2
        2 :
        {
        1 : DoubleTensor - size: 3
        }
        }
        2 :
        {
        1 : DoubleTensor - size: 2
        2 :
        {
        1 : DoubleTensor - size: 3
        }
        }
        }
        ```
        
        
        
        """

   'nn.ParallelTable':
      'prefix': 'nnParallelTable'
      'body': """
        <a name="nn.ParallelTable"></a>
        ## ParallelTable ##
        
        ```lua
        module = nn.ParallelTable()
        ```
        
        `ParallelTable` is a container module that, in its `forward()` method, applies the `i`-th member module to the `i`-th input, and outputs a `table` of the set of outputs.
        
        ```
        +----------+         +-----------+
        | {input1, +---------> {member1, |
        |          |         |           |
        |  input2, +--------->  member2, |
        |          |         |           |
        |  input3} +--------->  member3} |
        +----------+         +-----------+
        ```
        
        ### Example
        
        ```lua
        mlp = nn.ParallelTable()
        mlp:add(nn.Linear(10, 2))
        mlp:add(nn.Linear(5, 3))
        
        x = torch.randn(10)
        y = torch.rand(5)
        
        pred = mlp:forward{x, y}
        for i, k in pairs(pred) do print(i, k) end
        ```
        
        which gives the output:
        
        ```lua
        1
        0.0331
        0.7003
        [torch.Tensor of dimension 2]
        
        2
        0.0677
        -0.1657
        -0.7383
        [torch.Tensor of dimension 3]
        ```
        
        
        
        """

   'nn.SplitTable':
      'prefix': 'nnSplitTable'
      'body': """
        <a name="nn.SplitTable"></a>
        ## SplitTable ##
        
        ```lua
        module = SplitTable(dimension, nInputDims)
        ```
        
        Creates a module that takes a [`Tensor`](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor) as input and outputs several `table`s, splitting the `Tensor` along the specified `dimension`.
        In the diagram below, `dimension` is equal to `1`.
        
        ```
        +----------+         +-----------+
        | input[1] +---------> {member1, |
        +----------+-+         |           |
        | input[2] +----------->  member2, |
        +----------+-+           |           |
        | input[3] +------------->  member3} |
        +----------+             +-----------+
        ```
        
        The optional parameter `nInputDims` allows to specify the number of dimensions that this module will receive.
        This makes it possible to forward both minibatch and non-minibatch `Tensor`s through the same module.
        
        ### Example 1
        
        ```lua
        mlp = nn.SplitTable(2)
        x = torch.randn(4, 3)
        pred = mlp:forward(x)
        for i, k in ipairs(pred) do print(i, k) end
        ```
        
        gives the output:
        
        ```lua
        1
        1.3885
        1.3295
        0.4281
        -1.0171
        [torch.Tensor of dimension 4]
        
        2
        -1.1565
        -0.8556
        -1.0717
        -0.8316
        [torch.Tensor of dimension 4]
        
        3
        -1.3678
        -0.1709
        -0.0191
        -2.5871
        [torch.Tensor of dimension 4]
        ```
        
        ### Example 2
        
        ```lua
        mlp = nn.SplitTable(1)
        pred = mlp:forward(torch.randn(4, 3))
        for i, k in ipairs(pred) do print(i, k) end
        ```
        
        gives the output:
        
        ```lua
        1
        1.6114
        0.9038
        0.8419
        [torch.Tensor of dimension 3]
        
        2
        2.4742
        0.2208
        1.6043
        [torch.Tensor of dimension 3]
        
        3
        1.3415
        0.2984
        0.2260
        [torch.Tensor of dimension 3]
        
        4
        2.0889
        1.2309
        0.0983
        [torch.Tensor of dimension 3]
        ```
        
        ### Example 3
        
        ```lua
        mlp = nn.SplitTable(1, 2)
        pred = mlp:forward(torch.randn(2, 4, 3))
        for i, k in ipairs(pred) do print(i, k) end
        pred = mlp:forward(torch.randn(4, 3))
        for i, k in ipairs(pred) do print(i, k) end
        ```
        
        gives the output:
        
        ```lua
        1
        -1.3533  0.7448 -0.8818
        -0.4521 -1.2463  0.0316
        [torch.DoubleTensor of dimension 2x3]
        
        2
        0.1130 -1.3904  1.4620
        0.6722  2.0910 -0.2466
        [torch.DoubleTensor of dimension 2x3]
        
        3
        0.4672 -1.2738  1.1559
        0.4664  0.0768  0.6243
        [torch.DoubleTensor of dimension 2x3]
        
        4
        0.4194  1.2991  0.2241
        2.9786 -0.6715  0.0393
        [torch.DoubleTensor of dimension 2x3]
        
        
        1
        -1.8932
        0.0516
        -0.6316
        [torch.DoubleTensor of dimension 3]
        
        2
        -0.3397
        -1.8881
        -0.0977
        [torch.DoubleTensor of dimension 3]
        
        3
        0.0135
        1.2089
        0.5785
        [torch.DoubleTensor of dimension 3]
        
        4
        -0.1758
        -0.0776
        -1.1013
        [torch.DoubleTensor of dimension 3]
        ```
        
        The module also supports indexing from the end using negative dimensions. This allows to use this module when the number of dimensions of the input is unknown.
        
        ### Example
        
        ```lua
        m = nn.SplitTable(-2)
        out = m:forward(torch.randn(3, 2))
        for i, k in ipairs(out) do print(i, k) end
        out = m:forward(torch.randn(1, 3, 2))
        for i, k in ipairs(out) do print(i, k) end
        ```
        
        gives the output:
        
        ```
        1
        0.1420
        -0.5698
        [torch.DoubleTensor of size 2]
        
        2
        0.1663
        0.1197
        [torch.DoubleTensor of size 2]
        
        3
        0.4198
        -1.1394
        [torch.DoubleTensor of size 2]
        
        
        1
        -2.4941
        -1.4541
        [torch.DoubleTensor of size 1x2]
        
        2
        0.4594
        1.1946
        [torch.DoubleTensor of size 1x2]
        
        3
        -2.3322
        -0.7383
        [torch.DoubleTensor of size 1x2]
        ```
        
        ### A more complicated example
        
        ```lua
        mlp = nn.Sequential()       -- Create a network that takes a Tensor as input
        mlp:add(nn.SplitTable(2))
        c = nn.ParallelTable()      -- The two Tensor slices go through two different Linear
        c:add(nn.Linear(10, 3))     -- Layers in Parallel
        c:add(nn.Linear(10, 7))
        mlp:add(c)                  -- Outputing a table with 2 elements
        p = nn.ParallelTable()      -- These tables go through two more linear layers separately
        p:add(nn.Linear(3, 2))
        p:add(nn.Linear(7, 1))
        mlp:add(p)
        mlp:add(nn.JoinTable(1))    -- Finally, the tables are joined together and output.
        
        pred = mlp:forward(torch.randn(10, 2))
        print(pred)
        
        for i = 1, 100 do           -- A few steps of training such a network..
        x = torch.ones(10, 2)
        y = torch.Tensor(3)
        y:copy(x:select(2, 1):narrow(1, 1, 3))
        pred = mlp:forward(x)
        
        criterion = nn.MSECriterion()
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(0.05)
        
        print(err)
        end
        ```
        
        
        
        """

   'nn.JoinTable':
      'prefix': 'nnJoinTable'
      'body': """
        <a name="nn.JoinTable"></a>
        ## JoinTable ##
        
        ```lua
        module = JoinTable(dimension, nInputDims)
        ```
        
        Creates a module that takes a `table` of `Tensor`s as input and outputs a [`Tensor`](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor) by joining them together along dimension `dimension`.
        In the diagram below `dimension` is set to `1`.
        
        ```
        +----------+             +-----------+
        | {input1, +-------------> output[1] |
        |          |           +-----------+-+
        |  input2, +-----------> output[2] |
        |          |         +-----------+-+
        |  input3} +---------> output[3] |
        +----------+         +-----------+
        ```
        
        The optional parameter `nInputDims` allows to specify the number of dimensions that this module will receive. This makes it possible to forward both minibatch and non-minibatch `Tensor`s through the same module.
        
        ### Example 1
        
        ```lua
        x = torch.randn(5, 1)
        y = torch.randn(5, 1)
        z = torch.randn(2, 1)
        
        print(nn.JoinTable(1):forward{x, y})
        print(nn.JoinTable(2):forward{x, y})
        print(nn.JoinTable(1):forward{x, z})
        ```
        
        gives the output:
        
        ```lua
        1.3965
        0.5146
        -1.5244
        -0.9540
        0.4256
        0.1575
        0.4491
        0.6580
        0.1784
        -1.7362
        [torch.DoubleTensor of dimension 10x1]
        
        1.3965  0.1575
        0.5146  0.4491
        -1.5244  0.6580
        -0.9540  0.1784
        0.4256 -1.7362
        [torch.DoubleTensor of dimension 5x2]
        
        1.3965
        0.5146
        -1.5244
        -0.9540
        0.4256
        -1.2660
        1.0869
        [torch.Tensor of dimension 7x1]
        ```
        
        ### Example 2
        
        ```lua
        module = nn.JoinTable(2, 2)
        
        x = torch.randn(3, 1)
        y = torch.randn(3, 1)
        
        mx = torch.randn(2, 3, 1)
        my = torch.randn(2, 3, 1)
        
        print(module:forward{x, y})
        print(module:forward{mx, my})
        ```
        
        gives the output:
        
        ```lua
        0.4288  1.2002
        -1.4084 -0.7960
        -0.2091  0.1852
        [torch.DoubleTensor of dimension 3x2]
        
        (1,.,.) =
        0.5561  0.1228
        -0.6792  0.1153
        0.0687  0.2955
        
        (2,.,.) =
        2.5787  1.8185
        -0.9860  0.6756
        0.1989 -0.4327
        [torch.DoubleTensor of dimension 2x3x2]
        ```
        
        ### A more complicated example
        
        ```lua
        mlp = nn.Sequential()         -- Create a network that takes a Tensor as input
        c = nn.ConcatTable()          -- The same Tensor goes through two different Linear
        c:add(nn.Linear(10, 3))       -- Layers in Parallel
        c:add(nn.Linear(10, 7))
        mlp:add(c)                    -- Outputing a table with 2 elements
        p = nn.ParallelTable()        -- These tables go through two more linear layers
        p:add(nn.Linear(3, 2))        -- separately.
        p:add(nn.Linear(7, 1))
        mlp:add(p)
        mlp:add(nn.JoinTable(1))      -- Finally, the tables are joined together and output.
        
        pred = mlp:forward(torch.randn(10))
        print(pred)
        
        for i = 1, 100 do             -- A few steps of training such a network..
        x = torch.ones(10)
        y = torch.Tensor(3); y:copy(x:narrow(1, 1, 3))
        pred = mlp:forward(x)
        
        criterion= nn.MSECriterion()
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(0.05)
        
        print(err)
        end
        ```
        
        
        
        """

   'nn.MixtureTable'></a>
## MixtureTable ##

`module` = `MixtureTable([dim])`

Creates a module that takes a `table` `{gater, experts}` as input and outputs
the mixture of `experts` (a `Tensor` or `table` of `Tensor`s) using a
`gater` `Tensor`. When `dim` is provided, it specifies the dimension of
the `experts` `Tensor` that will be interpolated (or mixed). Otherwise,
the `experts` should take the form of a `table` of `Tensor`s. This
Module works for `experts` of dimension 1D or more, and for a
1D or 2D `gater`, i.e. for single examples or mini-batches.

Considering an `input = {G, E}` with a single example, then
the mixture of experts `Tensor` `E` with
gater `Tensor` `G` has the following form:
```lua
output = G[1]*E[1] + G[2]*E[2] + ... + G[n]*E[n]
```
where `dim = 1`, `n = E:size(dim) = G:size(dim)` and `G:dim() == 1`.
Note that `E:dim() >= 2`, such that `output:dim() = E:dim() - 1`.

Example 1:
Using this Module, an arbitrary mixture of `n` 2-layer experts
by a 2-layer gater could be constructed as follows:
```lua
experts = nn.ConcatTable()
for i = 1, n do
   local expert = nn.Sequential()
   expert:add(nn.Linear(3, 4))
   expert:add(nn.Tanh())
   expert:add(nn.Linear(4, 5))
   expert:add(nn.Tanh())
   experts:add(expert)
end

gater = nn.Sequential()
gater:add(nn.Linear(3, 7))
gater:add(nn.Tanh())
gater:add(nn.Linear(7, n))
gater:add(nn.SoftMax())

trunk = nn.ConcatTable()
trunk:add(gater)
trunk:add(experts)

moe = nn.Sequential()
moe:add(trunk)
moe:add(nn.MixtureTable())
```
Forwarding a batch of 2 examples gives us something like this:
```lua
> =moe:forward(torch.randn(2, 3))
-0.2152  0.3141  0.3280 -0.3772  0.2284
 0.2568  0.3511  0.0973 -0.0912 -0.0599
[torch.DoubleTensor of dimension 2x5]
```

Example 2:
In the following, the `MixtureTable` expects `experts` to be a `Tensor` of
`size = {1, 4, 2, 5, n}`:
```lua
experts = nn.Concat(5)
for i = 1, n do
   local expert = nn.Sequential()
   expert:add(nn.Linear(3, 4))
   expert:add(nn.Tanh())
   expert:add(nn.Linear(4, 4*2*5))
   expert:add(nn.Tanh())
   expert:add(nn.Reshape(4, 2, 5, 1))
   experts:add(expert)
end

gater = nn.Sequential()
gater:add(nn.Linear(3, 7))
gater:add(nn.Tanh())
gater:add(nn.Linear(7, n))
gater:add(nn.SoftMax())

trunk = nn.ConcatTable()
trunk:add(gater)
trunk:add(experts)

moe = nn.Sequential()
moe:add(trunk)
moe:add(nn.MixtureTable(5))
```
Forwarding a batch of 2 examples gives us something like this:
```lua
> =moe:forward(torch.randn(2, 3)):size()
 2
 4
 2
 5
[torch.LongStorage of size 4]

```

<a name=':
      'prefix': 'nnMixtureTable'></a>
## MixtureTable ##

`module` = `MixtureTable([dim])`

Creates a module that takes a `table` `{gater, experts}` as input and outputs
the mixture of `experts` (a `Tensor` or `table` of `Tensor`s) using a
`gater` `Tensor` When `dim` is provided, it specifies the dimension of
the `experts` `Tensor` that will be interpolated (or mixed) Otherwise,
the `experts` should take the form of a `table` of `Tensor`s This
Module works for `experts` of dimension 1D or more, and for a
1D or 2D `gater`, ie for single examples or mini-batches

Considering an `input = {G, E}` with a single example, then
the mixture of experts `Tensor` `E` with
gater `Tensor` `G` has the following form:
```lua
output = G[1]*E[1] + G[2]*E[2] +  + G[n]*E[n]
```
where `dim = 1`, `n = E:size(dim) = G:size(dim)` and `G:dim() == 1`
Note that `E:dim() >= 2`, such that `output:dim() = E:dim() - 1`

Example 1:
Using this Module, an arbitrary mixture of `n` 2-layer experts
by a 2-layer gater could be constructed as follows:
```lua
experts = nnConcatTable()
for i = 1, n do
   local expert = nnSequential()
   expert:add(nnLinear(3, 4))
   expert:add(nnTanh())
   expert:add(nnLinear(4, 5))
   expert:add(nnTanh())
   experts:add(expert)
end

gater = nnSequential()
gater:add(nnLinear(3, 7))
gater:add(nnTanh())
gater:add(nnLinear(7, n))
gater:add(nnSoftMax())

trunk = nnConcatTable()
trunk:add(gater)
trunk:add(experts)

moe = nnSequential()
moe:add(trunk)
moe:add(nnMixtureTable())
```
Forwarding a batch of 2 examples gives us something like this:
```lua
> =moe:forward(torchrandn(2, 3))
-02152  03141  03280 -03772  02284
 02568  03511  00973 -00912 -00599
[torchDoubleTensor of dimension 2x5]
```

Example 2:
In the following, the `MixtureTable` expects `experts` to be a `Tensor` of
`size = {1, 4, 2, 5, n}`:
```lua
experts = nnConcat(5)
for i = 1, n do
   local expert = nnSequential()
   expert:add(nnLinear(3, 4))
   expert:add(nnTanh())
   expert:add(nnLinear(4, 4*2*5))
   expert:add(nnTanh())
   expert:add(nnReshape(4, 2, 5, 1))
   experts:add(expert)
end

gater = nnSequential()
gater:add(nnLinear(3, 7))
gater:add(nnTanh())
gater:add(nnLinear(7, n))
gater:add(nnSoftMax())

trunk = nnConcatTable()
trunk:add(gater)
trunk:add(experts)

moe = nnSequential()
moe:add(trunk)
moe:add(nnMixtureTable(5))
```
Forwarding a batch of 2 examples gives us something like this:
```lua
> =moe:forward(torchrandn(2, 3)):size()
 2
 4
 2
 5
[torchLongStorage of size 4]

```

<a name='
      'body': """
        <a name='nn.MixtureTable'></a>
        ## MixtureTable ##
        
        `module` = `MixtureTable([dim])`
        
        Creates a module that takes a `table` `{gater, experts}` as input and outputs
        the mixture of `experts` (a `Tensor` or `table` of `Tensor`s) using a
        `gater` `Tensor`. When `dim` is provided, it specifies the dimension of
        the `experts` `Tensor` that will be interpolated (or mixed). Otherwise,
        the `experts` should take the form of a `table` of `Tensor`s. This
        Module works for `experts` of dimension 1D or more, and for a
        1D or 2D `gater`, i.e. for single examples or mini-batches.
        
        Considering an `input = {G, E}` with a single example, then
        the mixture of experts `Tensor` `E` with
        gater `Tensor` `G` has the following form:
        ```lua
        output = G[1]*E[1] + G[2]*E[2] + ... + G[n]*E[n]
        ```
        where `dim = 1`, `n = E:size(dim) = G:size(dim)` and `G:dim() == 1`.
        Note that `E:dim() >= 2`, such that `output:dim() = E:dim() - 1`.
        
        Example 1:
        Using this Module, an arbitrary mixture of `n` 2-layer experts
        by a 2-layer gater could be constructed as follows:
        ```lua
        experts = nn.ConcatTable()
        for i = 1, n do
        local expert = nn.Sequential()
        expert:add(nn.Linear(3, 4))
        expert:add(nn.Tanh())
        expert:add(nn.Linear(4, 5))
        expert:add(nn.Tanh())
        experts:add(expert)
        end
        
        gater = nn.Sequential()
        gater:add(nn.Linear(3, 7))
        gater:add(nn.Tanh())
        gater:add(nn.Linear(7, n))
        gater:add(nn.SoftMax())
        
        trunk = nn.ConcatTable()
        trunk:add(gater)
        trunk:add(experts)
        
        moe = nn.Sequential()
        moe:add(trunk)
        moe:add(nn.MixtureTable())
        ```
        Forwarding a batch of 2 examples gives us something like this:
        ```lua
        > =moe:forward(torch.randn(2, 3))
        -0.2152  0.3141  0.3280 -0.3772  0.2284
        0.2568  0.3511  0.0973 -0.0912 -0.0599
        [torch.DoubleTensor of dimension 2x5]
        ```
        
        Example 2:
        In the following, the `MixtureTable` expects `experts` to be a `Tensor` of
        `size = {1, 4, 2, 5, n}`:
        ```lua
        experts = nn.Concat(5)
        for i = 1, n do
        local expert = nn.Sequential()
        expert:add(nn.Linear(3, 4))
        expert:add(nn.Tanh())
        expert:add(nn.Linear(4, 4*2*5))
        expert:add(nn.Tanh())
        expert:add(nn.Reshape(4, 2, 5, 1))
        experts:add(expert)
        end
        
        gater = nn.Sequential()
        gater:add(nn.Linear(3, 7))
        gater:add(nn.Tanh())
        gater:add(nn.Linear(7, n))
        gater:add(nn.SoftMax())
        
        trunk = nn.ConcatTable()
        trunk:add(gater)
        trunk:add(experts)
        
        moe = nn.Sequential()
        moe:add(trunk)
        moe:add(nn.MixtureTable(5))
        ```
        Forwarding a batch of 2 examples gives us something like this:
        ```lua
        > =moe:forward(torch.randn(2, 3)):size()
        2
        4
        2
        5
        [torch.LongStorage of size 4]
        
        ```
        
        
        """

   'nn.SelectTable':
      'prefix': 'nnSelectTable'
      'body': """
        <a name="nn.SelectTable"></a>
        ## SelectTable ##
        
        `module` = `SelectTable(index)`
        
        Creates a module that takes a `table` as input and outputs the element at index `index` (positive or negative). 
        This can be either a `table` or a [`Tensor`](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor).
        
        The gradients of the non-`index` elements are zeroed `Tensor`s of the same size. This is true regardless of the
        depth of the encapsulated `Tensor` as the function used internally to do so is recursive.
        
        Example 1:
        ```lua
        > input = {torch.randn(2, 3), torch.randn(2, 1)}
        > =nn.SelectTable(1):forward(input)
        -0.3060  0.1398  0.2707
        0.0576  1.5455  0.0610
        [torch.DoubleTensor of dimension 2x3]
        
        > =nn.SelectTable(-1):forward(input)
        2.3080
        -0.2955
        [torch.DoubleTensor of dimension 2x1]
        
        > =table.unpack(nn.SelectTable(1):backward(input, torch.randn(2, 3)))
        -0.4891 -0.3495 -0.3182
        -2.0999  0.7381 -0.5312
        [torch.DoubleTensor of dimension 2x3]
        
        0
        0
        [torch.DoubleTensor of dimension 2x1]
        
        ```
        
        Example 2:
        ```lua
        > input = {torch.randn(2, 3), {torch.randn(2, 1), {torch.randn(2, 2)}}}
        
        > =nn.SelectTable(2):forward(input)
        {
        1 : DoubleTensor - size: 2x1
        2 :
        {
        1 : DoubleTensor - size: 2x2
        }
        }
        
        > =table.unpack(nn.SelectTable(2):backward(input, {torch.randn(2, 1), {torch.randn(2, 2)}}))
        0 0 0
        0 0 0
        [torch.DoubleTensor of dimension 2x3]
        
        {
        1 : DoubleTensor - size: 2x1
        2 :
        {
        1 : DoubleTensor - size: 2x2
        }
        }
        
        > gradInput = nn.SelectTable(1):backward(input, torch.randn(2, 3))
        
        > =gradInput
        {
        1 : DoubleTensor - size: 2x3
        2 :
        {
        1 : DoubleTensor - size: 2x1
        2 :
        {
        1 : DoubleTensor - size: 2x2
        }
        }
        }
        
        > =gradInput[1]
        -0.3400 -0.0404  1.1885
        1.2865  0.4107  0.6506
        [torch.DoubleTensor of dimension 2x3]
        
        > gradInput[2][1]
        0
        0
        [torch.DoubleTensor of dimension 2x1]
        
        > gradInput[2][2][1]
        0 0
        0 0
        [torch.DoubleTensor of dimension 2x2]
        
        ```
        
        
        """

   'nn.NarrowTable':
      'prefix': 'nnNarrowTable'
      'body': """
        <a name="nn.NarrowTable"></a>
        ## NarrowTable ##
        
        `module` = `NarrowTable(offset [, length])`
        
        Creates a module that takes a `table` as input and outputs the subtable 
        starting at index `offset` having `length` elements (defaults to 1 element).
        The elements can be either a `table` or a [`Tensor`](https://github.com/torch/torch7/blob/master/doc/tensor.md#tensor).
        
        The gradients of the elements not included in the subtable are zeroed `Tensor`s of the same size. 
        This is true regardless of the depth of the encapsulated `Tensor` as the function used internally to do so is recursive.
        
        Example:
        ```lua
        > input = {torch.randn(2, 3), torch.randn(2, 1), torch.randn(1, 2)}
        > =nn.NarrowTable(2,2):forward(input)
        {
        1 : DoubleTensor - size: 2x1
        2 : DoubleTensor - size: 1x2
        }
        
        > =nn.NarrowTable(1):forward(input)
        {
        1 : DoubleTensor - size: 2x3
        }
        
        > =table.unpack(nn.NarrowTable(1,2):backward(input, {torch.randn(2, 3), torch.randn(2, 1)}))
        1.9528 -0.1381  0.2023
        0.2297 -1.5169 -1.1871
        [torch.DoubleTensor of size 2x3]
        
        -1.2023
        -0.4165
        [torch.DoubleTensor of size 2x1]
        
        0  0
        [torch.DoubleTensor of size 1x2]
        
        ```
        
        
        """

   'nn.FlattenTable':
      'prefix': 'nnFlattenTable'
      'body': """
        <a name="nn.FlattenTable"></a>
        ## FlattenTable ##
        
        `module` = `FlattenTable()`
        
        Creates a module that takes an arbitrarily deep `table` of `Tensor`s (potentially nested) as input and outputs a `table` of `Tensor`s, where the output `Tensor` in index `i` is the `Tensor` with post-order DFS index `i` in the input `table`.
        
        This module is particularly useful in combination with nn.Identity() to create networks that can append to their input `table`.
        
        Example:
        ```lua
        x = {torch.rand(1), {torch.rand(2), {torch.rand(3)}}, torch.rand(4)}
        print(x)
        print(nn.FlattenTable():forward(x))
        ```
        gives the output:
        ```lua
        {
        1 : DoubleTensor - size: 1
        2 :
        {
        1 : DoubleTensor - size: 2
        2 :
        {
        1 : DoubleTensor - size: 3
        }
        }
        3 : DoubleTensor - size: 4
        }
        {
        1 : DoubleTensor - size: 1
        2 : DoubleTensor - size: 2
        3 : DoubleTensor - size: 3
        4 : DoubleTensor - size: 4
        }
        ```
        
        
        """

   'nn.PairwiseDistance':
      'prefix': 'nnPairwiseDistance'
      'body': """
        <a name="nn.PairwiseDistance"></a>
        ## PairwiseDistance ##
        
        `module` = `PairwiseDistance(p)` creates a module that takes a `table` of two vectors as input and outputs the distance between them using the `p`-norm.
        
        Example:
        ```lua
        mlp_l1 = nn.PairwiseDistance(1)
        mlp_l2 = nn.PairwiseDistance(2)
        x = torch.Tensor({1, 2, 3})
        y = torch.Tensor({4, 5, 6})
        print(mlp_l1:forward({x, y}))
        print(mlp_l2:forward({x, y}))
        ```
        gives the output:
        ```lua
        9
        [torch.Tensor of dimension 1]
        
        5.1962
        [torch.Tensor of dimension 1]
        ```
        
        A more complicated example:
        ```lua
        -- imagine we have one network we are interested in, it is called "p1_mlp"
        p1_mlp= nn.Sequential(); p1_mlp:add(nn.Linear(5, 2))
        
        -- But we want to push examples towards or away from each other
        -- so we make another copy of it called p2_mlp
        -- this *shares* the same weights via the set command, but has its own set of temporary gradient storage
        -- that's why we create it again (so that the gradients of the pair don't wipe each other)
        p2_mlp= nn.Sequential(); p2_mlp:add(nn.Linear(5, 2))
        p2_mlp:get(1).weight:set(p1_mlp:get(1).weight)
        p2_mlp:get(1).bias:set(p1_mlp:get(1).bias)
        
        -- we make a parallel table that takes a pair of examples as input. they both go through the same (cloned) mlp
        prl = nn.ParallelTable()
        prl:add(p1_mlp)
        prl:add(p2_mlp)
        
        -- now we define our top level network that takes this parallel table and computes the pairwise distance between
        -- the pair of outputs
        mlp= nn.Sequential()
        mlp:add(prl)
        mlp:add(nn.PairwiseDistance(1))
        
        -- and a criterion for pushing together or pulling apart pairs
        crit = nn.HingeEmbeddingCriterion(1)
        
        -- lets make two example vectors
        x = torch.rand(5)
        y = torch.rand(5)
        
        
        -- Use a typical generic gradient update function
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        
        -- push the pair x and y together, notice how then the distance between them given
        -- by  print(mlp:forward({x, y})[1]) gets smaller
        for i = 1, 10 do
        gradUpdate(mlp, {x, y}, 1, crit, 0.01)
        print(mlp:forward({x, y})[1])
        end
        
        
        -- pull apart the pair x and y, notice how then the distance between them given
        -- by  print(mlp:forward({x, y})[1]) gets larger
        
        for i = 1, 10 do
        gradUpdate(mlp, {x, y}, -1, crit, 0.01)
        print(mlp:forward({x, y})[1])
        end
        
        ```
        
        
        """

   'nn.DotProduct':
      'prefix': 'nnDotProduct'
      'body': """
        <a name="nn.DotProduct"></a>
        ## DotProduct ##
        
        `module` = `DotProduct()` creates a module that takes a `table` of two vectors (or matrices if in batch mode) as input and outputs the dot product between them.
        
        Example:
        ```lua
        mlp = nn.DotProduct()
        x = torch.Tensor({1, 2, 3})
        y = torch.Tensor({4, 5, 6})
        print(mlp:forward({x, y}))
        ```
        gives the output:
        ```lua
        32
        [torch.Tensor of dimension 1]
        ```
        
        
        A more complicated example:
        ```lua
        
        -- Train a ranking function so that mlp:forward({x, y}, {x, z}) returns a number
        -- which indicates whether x is better matched with y or z (larger score = better match), or vice versa.
        
        mlp1 = nn.Linear(5, 10)
        mlp2 = mlp1:clone('weight', 'bias')
        
        prl = nn.ParallelTable();
        prl:add(mlp1); prl:add(mlp2)
        
        mlp1 = nn.Sequential()
        mlp1:add(prl)
        mlp1:add(nn.DotProduct())
        
        mlp2 = mlp1:clone('weight', 'bias')
        
        mlp = nn.Sequential()
        prla = nn.ParallelTable()
        prla:add(mlp1)
        prla:add(mlp2)
        mlp:add(prla)
        
        x = torch.rand(5);
        y = torch.rand(5)
        z = torch.rand(5)
        
        
        print(mlp1:forward{x, x})
        print(mlp1:forward{x, y})
        print(mlp1:forward{y, y})
        
        
        crit = nn.MarginRankingCriterion(1);
        
        -- Use a typical generic gradient update function
        function gradUpdate(mlp, x, y, criterion, learningRate)
        local pred = mlp:forward(x)
        local err = criterion:forward(pred, y)
        local gradCriterion = criterion:backward(pred, y)
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        
        inp = {{x, y}, {x, z}}
        
        math.randomseed(1)
        
        -- make the pair x and y have a larger dot product than x and z
        
        for i = 1, 100 do
        gradUpdate(mlp, inp, 1, crit, 0.05)
        o1 = mlp1:forward{x, y}[1];
        o2 = mlp2:forward{x, z}[1];
        o = crit:forward(mlp:forward{{x, y}, {x, z}}, 1)
        print(o1, o2, o)
        end
        
        print "________________**"
        
        -- make the pair x and z have a larger dot product than x and y
        
        for i = 1, 100 do
        gradUpdate(mlp, inp, -1, crit, 0.05)
        o1 = mlp1:forward{x, y}[1];
        o2 = mlp2:forward{x, z}[1];
        o = crit:forward(mlp:forward{{x, y}, {x, z}}, -1)
        print(o1, o2, o)
        end
        ```
        
        
        
        """

   'nn.CosineDistance':
      'prefix': 'nnCosineDistance'
      'body': """
        <a name="nn.CosineDistance"></a>
        ## CosineDistance ##
        
        `module` = `CosineDistance()` creates a module that takes a `table` of two vectors (or matrices if in batch mode) as input and outputs the cosine distance between them.
        
        Examples:
        ```lua
        mlp = nn.CosineDistance()
        x = torch.Tensor({1, 2, 3})
        y = torch.Tensor({4, 5, 6})
        print(mlp:forward({x, y}))
        ```
        gives the output:
        ```lua
        0.9746
        [torch.Tensor of dimension 1]
        ```
        `CosineDistance` also accepts batches:
        ```lua
        mlp = nn.CosineDistance()
        x = torch.Tensor({{1,2,3},{1,2,-3}})
        y = torch.Tensor({{4,5,6},{-4,5,6}})
        print(mlp:forward({x,y}))
        ```
        gives the output:
        ```lua
        0.9746
        -0.3655
        [torch.DoubleTensor of size 2]
        ```
        
        A more complicated example:
        ```lua
        
        -- imagine we have one network we are interested in, it is called "p1_mlp"
        p1_mlp= nn.Sequential(); p1_mlp:add(nn.Linear(5, 2))
        
        -- But we want to push examples towards or away from each other
        -- so we make another copy of it called p2_mlp
        -- this *shares* the same weights via the set command, but has its own set of temporary gradient storage
        -- that's why we create it again (so that the gradients of the pair don't wipe each other)
        p2_mlp= p1_mlp:clone('weight', 'bias')
        
        -- we make a parallel table that takes a pair of examples as input. they both go through the same (cloned) mlp
        prl = nn.ParallelTable()
        prl:add(p1_mlp)
        prl:add(p2_mlp)
        
        -- now we define our top level network that takes this parallel table and computes the cosine distance between
        -- the pair of outputs
        mlp= nn.Sequential()
        mlp:add(prl)
        mlp:add(nn.CosineDistance())
        
        
        -- lets make two example vectors
        x = torch.rand(5)
        y = torch.rand(5)
        
        -- Grad update function..
        function gradUpdate(mlp, x, y, learningRate)
        local pred = mlp:forward(x)
        if pred[1]*y < 1 then
        gradCriterion = torch.Tensor({-y})
        mlp:zeroGradParameters()
        mlp:backward(x, gradCriterion)
        mlp:updateParameters(learningRate)
        end
        end
        
        -- push the pair x and y together, the distance should get larger..
        for i = 1, 1000 do
        gradUpdate(mlp, {x, y}, 1, 0.1)
        if ((i%100)==0) then print(mlp:forward({x, y})[1]);end
        end
        
        
        -- pull apart the pair x and y, the distance should get smaller..
        
        for i = 1, 1000 do
        gradUpdate(mlp, {x, y}, -1, 0.1)
        if ((i%100)==0) then print(mlp:forward({x, y})[1]);end
        end
        ```
        
        
        
        
        """

   'nn.CriterionTable':
      'prefix': 'nnCriterionTable'
      'body': """
        <a name="nn.CriterionTable"></a>
        ## CriterionTable ##
        
        `module` = `CriterionTable(criterion)`
        
        Creates a module that wraps a Criterion module so that it can accept a `table` of inputs. Typically the `table` would contain two elements: the input and output `x` and `y` that the Criterion compares.
        
        Example:
        ```lua
        mlp = nn.CriterionTable(nn.MSECriterion())
        x = torch.randn(5)
        y = torch.randn(5)
        print(mlp:forward{x, x})
        print(mlp:forward{x, y})
        ```
        gives the output:
        ```lua
        0
        1.9028918413199
        ```
        
        Here is a more complex example of embedding the criterion into a network:
        ```lua
        
        function table.print(t)
        for i, k in pairs(t) do print(i, k); end
        end
        
        mlp = nn.Sequential();                          -- Create an mlp that takes input
        main_mlp = nn.Sequential();		      -- and output using ParallelTable
        main_mlp:add(nn.Linear(5, 4))
        main_mlp:add(nn.Linear(4, 3))
        cmlp = nn.ParallelTable();
        cmlp:add(main_mlp)
        cmlp:add(nn.Identity())
        mlp:add(cmlp)
        mlp:add(nn.CriterionTable(nn.MSECriterion())) -- Apply the Criterion
        
        for i = 1, 20 do                                 -- Train for a few iterations
        x = torch.ones(5);
        y = torch.Tensor(3); y:copy(x:narrow(1, 1, 3))
        err = mlp:forward{x, y}                         -- Pass in both input and output
        print(err)
        
        mlp:zeroGradParameters();
        mlp:backward({x, y} );
        mlp:updateParameters(0.05);
        end
        ```
        
        
        """

   'nn.CAddTable':
      'prefix': 'nnCAddTable'
      'body': """
        <a name="nn.CAddTable"></a>
        ## CAddTable ##
        
        Takes a `table` of `Tensor`s and outputs summation of all `Tensor`s.
        
        ```lua
        ii = {torch.ones(5), torch.ones(5)*2, torch.ones(5)*3}
        =ii[1]
        1
        1
        1
        1
        1
        [torch.DoubleTensor of dimension 5]
        
        return ii[2]
        2
        2
        2
        2
        2
        [torch.DoubleTensor of dimension 5]
        
        return ii[3]
        3
        3
        3
        3
        3
        [torch.DoubleTensor of dimension 5]
        
        m = nn.CAddTable()
        =m:forward(ii)
        6
        6
        6
        6
        6
        [torch.DoubleTensor of dimension 5]
        ```
        
        
        
        """

   'nn.CSubTable':
      'prefix': 'nnCSubTable'
      'body': """
        <a name="nn.CSubTable"></a>
        ## CSubTable ##
        
        Takes a `table` with two `Tensor` and returns the component-wise
        subtraction between them.
        
        ```lua
        m = nn.CSubTable()
        =m:forward({torch.ones(5)*2.2, torch.ones(5)})
        1.2000
        1.2000
        1.2000
        1.2000
        1.2000
        [torch.DoubleTensor of dimension 5]
        ```
        
        
        """

   'nn.CMulTable':
      'prefix': 'nnCMulTable'
      'body': """
        <a name="nn.CMulTable"></a>
        ## CMulTable ##
        
        Takes a `table` of `Tensor`s and outputs the multiplication of all of them.
        
        ```lua
        ii = {torch.ones(5)*2, torch.ones(5)*3, torch.ones(5)*4}
        m = nn.CMulTable()
        =m:forward(ii)
        24
        24
        24
        24
        24
        [torch.DoubleTensor of dimension 5]
        
        ```
        
        
        """

   'nn.CDivTable':
      'prefix': 'nnCDivTable'
      'body': """
        <a name="nn.CDivTable"></a>
        ## CDivTable ##
        
        Takes a `table` with two `Tensor` and returns the component-wise
        division between them.
        
        ```lua
        m = nn.CDivTable()
        =m:forward({torch.ones(5)*2.2, torch.ones(5)*4.4})
        0.5000
        0.5000
        0.5000
        0.5000
        0.5000
        [torch.DoubleTensor of dimension 5]
        ```
        
        
        """

   'torch.Tensor.dok':
      'prefix': 'torchTensordok'
      'body': """
        <a name="torch.Tensor.dok"></a>
        # Tensor #
        
        The `Tensor` class is probably the most important class in
        `Torch`. Almost every package depends on this class. It is *__the__*
        class for handling numeric data. As with   pretty much anything in
        [Torch7](./../index.md), tensors are
        [serializable](file.md#torch.File.serialization).
        
        __Multi-dimensional matrix__
        
        A `Tensor` is a potentially multi-dimensional matrix. The number of
        dimensions is unlimited that can be created using
        [LongStorage](storage.md) with more dimensions.
        
        Example:
        ```lua
        --- creation of a 4D-tensor 4x5x6x2
        z = torch.Tensor(4,5,6,2)
        --- for more dimensions, (here a 6D tensor) one can do:
        s = torch.LongStorage(6)
        s[1] = 4; s[2] = 5; s[3] = 6; s[4] = 2; s[5] = 7; s[6] = 3;
        x = torch.Tensor(s)
        ```
        
        The number of dimensions of a `Tensor` can be queried by
        [nDimension()](#torch.nDimension) or
        [dim()](#torch.Tensor.dim). Size of the `i-th` dimension is
        returned by [size(i)](#torch.Tensor.size). A [LongStorage](storage.md)
        containing all the dimensions can be returned by
        [size()](#torch.Tensor.size).
        
        ```lua
        > x:nDimension()
        6
        > x:size()
        4
        5
        6
        2
        7
        3
        [torch.LongStorage of size 6]
        ```
        
        __Internal data representation__
        
        The actual data of a `Tensor` is contained into a
        [Storage](storage.md). It can be accessed using
        [`storage()`](#torch.storage). While the memory of a
        `Tensor` has to be contained in this unique `Storage`, it might
        not be contiguous: the first position used in the `Storage` is given
        by [`storageOffset()`](#torch.storageOffset) (starting at
        `1`). And the _jump_ needed to go from one element to another
        element in the `i-th` dimension is given by
        [`stride(i)`](#torch.Tensor.stride). In other words, given a 3D
        tensor
        
        ```lua
        x = torch.Tensor(7,7,7)
        ```
        accessing the element `(3,4,5)` can be done by
        ```lua
        > x[3][4][5]
        ```
        or equivalently (but slowly!)
        ```lua
        > x:storage()[x:storageOffset()
        +(3-1)*x:stride(1)+(4-1)*x:stride(2)+(5-1)*x:stride(3)]
        ```
        One could say that a `Tensor` is a particular way of _viewing_ a
        `Storage`: a `Storage` only represents a chunk of memory, while the
        `Tensor` interprets this chunk of memory as having dimensions:
        ```lua
        x = torch.Tensor(4,5)
        s = x:storage()
        for i=1,s:size() do -- fill up the Storage
        s[i] = i
        end
        > x -- s is interpreted by x as a 2D matrix
        1   2   3   4   5
        6   7   8   9  10
        11  12  13  14  15
        16  17  18  19  20
        [torch.DoubleTensor of dimension 4x5]
        ```
        
        Note also that in Torch7 ___elements in the same row___ [elements along the __last__ dimension]
        are contiguous in memory for a matrix [tensor]:
        ```lua
        x = torch.Tensor(4,5)
        i = 0
        
        x:apply(function()
        i = i + 1
        return i
        end)
        
        > x
        1   2   3   4   5
        6   7   8   9  10
        11  12  13  14  15
        16  17  18  19  20
        [torch.DoubleTensor of dimension 4x5]
        
        > x:stride()
        5
        1  -- element in the last dimension are contiguous!
        [torch.LongStorage of size 2]
        ```
        This is exactly like in C (and not `Fortran`).
        
        __Tensors of different types__
        
        Actually, several types of `Tensor` exists:
        ```lua
        ByteTensor -- contains unsigned chars
        CharTensor -- contains signed chars
        ShortTensor -- contains shorts
        IntTensor -- contains ints
        FloatTensor -- contains floats
        DoubleTensor -- contains doubles
        ```
        
        Most numeric operations are implemented _only_ for `FloatTensor` and `DoubleTensor`.
        Other Tensor types are useful if you want to save memory space.
        
        __Default Tensor type__
        
        For convenience, _an alias_ `torch.Tensor` is provided, which allows the user to write
        type-independent scripts, which can then ran after choosing the desired Tensor type with
        a call like
        ```lua
        torch.setdefaulttensortype('torch.FloatTensor')
        ```
        See [torch.setdefaulttensortype](utility.md#torch.setdefaulttensortype) for more details.
        By default, the alias "points" on `torch.DoubleTensor`.
        
        __Efficient memory management__
        
        _All_ tensor operations in this class do _not_ make any memory copy. All
        these methods transform the existing tensor, or return a new tensor
        referencing _the same storage_. This magical behavior is internally
        obtained by good usage of the [stride()](#torch.Tensor.stride) and
        [storageOffset()](#torch.storageOffset). Example:
        ```lua
        x = torch.Tensor(5):zero()
        > x
        0
        0
        0
        0
        0
        [torch.DoubleTensor of dimension 5]
        > x:narrow(1, 2, 3):fill(1) -- narrow() returns a Tensor
        -- referencing the same Storage as x
        > x
        0
        1
        1
        1
        0
        [torch.Tensor of dimension 5]
        ```
        
        If you really need to copy a `Tensor`, you can use the [copy()](#torch.Tensor.copy) method:
        ```lua
        y = torch.Tensor(x:size()):copy(x)
        ```
        Or the convenience method
        ```lua
        y = x:clone()
        ```
        
        We now describe all the methods for `Tensor`. If you want to specify the Tensor type,
        just replace `Tensor` by the name of the Tensor variant (like `CharTensor`).
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ## Tensor constructors ##
        
        Tensor constructors, create new Tensor object, optionally, allocating
        new memory. By default the elements of a newly allocated memory are
        not initialized, therefore, might contain arbitrary numbers. Here are
        several ways to construct a new `Tensor`.
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor() ###
        
        Returns an empty tensor.
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor(tensor) ###
        
        Returns a new tensor which reference the same
        [Storage](#torch.storage) than the given `tensor`. The
        [size](#torch.Tensor.size), [stride](#torch.Tensor.stride), and
        [storage offset](#torch.storageOffset) are the same than the
        given tensor.
        
        The new `Tensor` is now going to "view" the same [storage](storage.md)
        as the given `tensor`. As a result, any modification in the elements
        of the `Tensor` will have a impact on the elements of the given
        `tensor`, and vice-versa. No memory copy!
        
        ```lua
        x = torch.Tensor(2,5):fill(3.14)
        > x
        3.1400  3.1400  3.1400  3.1400  3.1400
        3.1400  3.1400  3.1400  3.1400  3.1400
        [torch.DoubleTensor of dimension 2x5]
        
        y = torch.Tensor(x)
        > y
        3.1400  3.1400  3.1400  3.1400  3.1400
        3.1400  3.1400  3.1400  3.1400  3.1400
        [torch.DoubleTensor of dimension 2x5]
        
        y:zero()
        > x -- elements of x are the same as y!
        0 0 0 0 0
        0 0 0 0 0
        [torch.DoubleTensor of dimension 2x5]
        ```
        
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor(sz1 [,sz2 [,sz3 [,sz4]]]]) ###
        
        Create a tensor up to 4 dimensions. The tensor size will be `sz1 x sz2 x sx3 x sz4`.
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor(sizes, [strides]) ###
        
        Create a tensor of any number of dimensions. The
        [LongStorage](storage.md) `sizes` gives the size in each dimension of
        the tensor. The optional [LongStorage](storage.md) `strides` gives the
        jump necessary to go from one element to the next one in the each
        dimension. Of course, `sizes` and `strides` must have the same
        number of elements. If not given, or if some elements of `strides`
        are _negative_, the [stride()](#torch.Tensor.stride) will be
        computed such that the tensor is as contiguous as possible in memory.
        
        Example, create a 4D 4x4x3x2 tensor:
        ```lua
        x = torch.Tensor(torch.LongStorage({4,4,3,2}))
        ```
        
        Playing with the strides can give some interesting things:
        ```lua
        x = torch.Tensor(torch.LongStorage({4}), torch.LongStorage({0})):zero() -- zeroes the tensor
        x[1] = 1 -- all elements point to the same address!
        > x
        1
        1
        1
        1
        [torch.DoubleTensor of dimension 4]
        ```
        
        Note that _negative strides are not allowed_, and, if given as
        argument when constructing the Tensor, will be interpreted as //choose
        the right stride such that the Tensor is contiguous in memory//.
        
        Note _this method cannot be used to create `torch.LongTensor`s_.
        The constructor [from a storage](tensor.md#torchtensorstorage-storageoffset-sizes-strides) will be used:
        ```lua
        a = torch.LongStorage({1,2}) -- We have a torch.LongStorage containing the values 1 and 2
        -- General case for TYPE ~= Long, e.g. for TYPE = Float:
        b = torch.FloatTensor(a)
        -- Creates a new torch.FloatTensor with 2 dimensions, the first of size 1 and the second of size 2
        > b:size()
        1
        2
        [torch.LongStorage of size 2]
        
        -- Special case of torch.LongTensor
        c = torch.LongTensor(a)
        -- Creates a new torch.LongTensor that uses a as storage and thus contains the values 1 and 2
        > c
        1
        2
        [torch.LongTensor of size 2]
        ```
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor(storage, [storageOffset, sizes, [strides]]) ###
        
        Returns a tensor which uses the existing [Storage](storage.md)
        `storage`, starting at position `storageOffset` (>=1).  The size
        of each dimension of the tensor is given by the
        [LongStorage](storage.md) `sizes`.
        
        If only `storage` is provided, it will create a 1D Tensor viewing
        the all Storage.
        
        The jump necessary to go from one element to the next one in each
        dimension is given by the optional argument [LongStorage](storage.md)
        `strides`. If not given, or if some elements of `strides` are
        negative, the [stride()](#torch.Tensor.stride) will be computed such
        that the tensor is as contiguous as possible in memory.
        
        Any modification in the elements of the `Storage` will have an
        impact on the elements of the new `Tensor`, and vice-versa. There is
        no memory copy!
        
        ```lua
        -- creates a storage with 10 elements
        s = torch.Storage(10):fill(1)
        
        -- we want to see it as a 2x5 tensor
        x = torch.Tensor(s, 1, torch.LongStorage{2,5})
        > x
        1  1  1  1  1
        1  1  1  1  1
        [torch.DoubleTensor of dimension 2x5]
        
        x:zero()
        > s -- the storage contents have been modified
        0
        0
        0
        0
        0
        0
        0
        0
        0
        0
        [torch.DoubleStorage of size 10]
        ```
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor(storage, [storageOffset, sz1 [, st1 ... [, sz4 [, st4]]]]) ###
        
        Convenience constructor (for the previous constructor) assuming a
        number of dimensions inferior or equal to 4. `szi` is the size in
        the `i-th` dimension, and `sti` is the stride in the `i-th`
        dimension.
        
        
        """

   'torch.Tensor':
      'prefix': 'torchTensor'
      'body': """
        <a name="torch.Tensor"></a>
        ### torch.Tensor(table) ###
        
        The argument is assumed to be a Lua array of numbers. The constructor
        returns a new Tensor of the size of the table, containing all the table
        elements. The table might be multi-dimensional.
        
        Example:
        ```lua
        > torch.Tensor({{1,2,3,4}, {5,6,7,8}})
        1  2  3  4
        5  6  7  8
        [torch.DoubleTensor of dimension 2x4]
        ```
        
        ## A note on function calls ##
        
        The rest of this guide will present many functions that can be used to manipulate tensors. Most functions have been
        defined so that they can be called flexibly, either in an object-oriented "method call" style i.e. `src:function(...)`
        or a more "functional" style `torch.function(src, ...)`, where `src` is a tensor. Note that these different invocations
        may differ in whether they modify the tensor in-place, or create a new tensor. Additionally, some functions can be
        called in the form `dst:function(src, ...)` which usually suggests that the result of the operation on the `src` tensor
        will be stored in the tensor `dst`.  Further details are given in the individual function definitions, below, but it
        should be noted that the documentation is currently incomplete in this regard, and readers are encouraged to experiment
        in an interactive session.
        
        ## Cloning ##
        
        
        """

   'torch.Tensor.clone':
      'prefix': 'torchTensorclone'
      'body': """
        <a name="torch.Tensor.clone"></a>
        ### [Tensor] clone() ###
        
        Returns a clone of a tensor. The memory is copied.
        
        ```lua
        i = 0
        x = torch.Tensor(5):apply(function(x)
        i = i + 1
        return i
        end)
        > x
        1
        2
        3
        4
        5
        [torch.DoubleTensor of dimension 5]
        
        -- create a clone of x
        y = x:clone()
        > y
        1
        2
        3
        4
        5
        [torch.DoubleTensor of dimension 5]
        
        -- fill up y with 1
        y:fill(1)
        > y
        1
        1
        1
        1
        1
        [torch.DoubleTensor of dimension 5]
        
        -- the contents of x were not changed:
        > x
        1
        2
        3
        4
        5
        [torch.DoubleTensor of dimension 5]
        ```
        
        
        """

   'torch.Tensor.contiguous':
      'prefix': 'torchTensorcontiguous'
      'body': """
        <a name="torch.Tensor.contiguous"></a>
        ### [Tensor] contiguous ###
        
        * If the given Tensor contents are contiguous in memory, returns the exact same Tensor (no memory copy).
        * Otherwise (_not contiguous in memory_), returns a [clone](#torch.Tensor.clone) (memory _copy_).
        
        ```lua
        x = torch.Tensor(2,3):fill(1)
        > x
        1  1  1
        1  1  1
        [torch.DoubleTensor of dimension 2x3]
        
        -- x is contiguous, so y points to the same thing
        y = x:contiguous():fill(2)
        > y
        2  2  2
        2  2  2
        [torch.DoubleTensor of dimension 2x3]
        
        -- contents of x have been changed
        > x
        2  2  2
        2  2  2
        [torch.DoubleTensor of dimension 2x3]
        
        -- x:t() is not contiguous, so z is a clone
        z = x:t():contiguous():fill(3.14)
        > z
        3.1400  3.1400
        3.1400  3.1400
        3.1400  3.1400
        [torch.DoubleTensor of dimension 3x2]
        
        -- contents of x have not been changed
        > x
        2  2  2
        2  2  2
        [torch.DoubleTensor of dimension 2x3]
        ```
        
        
        """

   'torch.type':
      'prefix': 'torchtype'
      'body': """
        <a name="torch.type"></a>
        ### [Tensor or string] type(type) ###
        
        __If `type` is `nil`__, returns a string containing the type name of
        the given tensor.
        
        ```lua
        = torch.Tensor():type()
        torch.DoubleTensor
        ```
        
        __If `type` is a string__ describing a Tensor type, and is equal to
        the given tensor typename, returns the exact same tensor (//no memory
        copy//).
        
        ```lua
        x = torch.Tensor(3):fill(3.14)
        > x
        3.1400
        3.1400
        3.1400
        [torch.DoubleTensor of dimension 3]
        
        y = x:type('torch.DoubleTensor')
        > y
        3.1400
        3.1400
        3.1400
        [torch.DoubleTensor of dimension 3]
        
        -- zero y contents
        y:zero()
        
        -- contents of x have been changed
        > x
        0
        0
        0
        [torch.DoubleTensor of dimension 3]
        
        ```
        
        __If `type` is a string__ describing a Tensor type, different from
        the type name of the given Tensor, returns a new Tensor of the
        specified type, whose contents corresponds to the contents of the
        original Tensor, casted to the given type (//memory copy occurs, with
        possible loss of precision//).
        
        ```lua
        x = torch.Tensor(3):fill(3.14)
        > x
        3.1400
        3.1400
        3.1400
        [torch.DoubleTensor of dimension 3]
        
        y = x:type('torch.IntTensor')
        > y
        3
        3
        3
        [torch.IntTensor of dimension 3]
        
        ```
        
        
        """

   'torch.Tensor.typeAs':
      'prefix': 'torchTensortypeAs'
      'body': """
        <a name="torch.Tensor.typeAs"></a>
        ### [Tensor] typeAs(tensor) ###
        
        Convenience method for the [type](#torch.type) method. Equivalent to
        ```lua
        type(tensor:type())
        ```
        
        
        """

   'torch.isTensor':
      'prefix': 'torchisTensor'
      'body': """
        <a name="torch.isTensor"></a>
        ### [boolean] isTensor(object) ###
        
        Returns `true` iff the provided `object` is one of the `torch.*Tensor` types.
        
        ```lua
        > torch.isTensor(torch.randn(3,4))
        true
        
        > torch.isTensor(torch.randn(3,4)[1])
        true
        
        > torch.isTensor(torch.randn(3,4)[1][2])
        false
        ```
        
        
        """

   'torch.byte':
      'prefix': 'torchbyte'
      'body': """
        <a name="torch.byte"></a>
        ### [Tensor] byte(), char(), short(), int(), long(), float(), double() ###
        
        """

   'torch.Tensor.short':
      'prefix': 'torchTensorshort'
      'body': """
        <a name="torch.Tensor.short"></a>
        
        """

   'torch.Tensor.char':
      'prefix': 'torchTensorchar'
      'body': """
        <a name="torch.Tensor.char"></a>
        
        """

   'torch.Tensor.long':
      'prefix': 'torchTensorlong'
      'body': """
        <a name="torch.Tensor.long"></a>
        
        """

   'torch.Tensor.int':
      'prefix': 'torchTensorint'
      'body': """
        <a name="torch.Tensor.int"></a>
        
        """

   'torch.Tensor.double':
      'prefix': 'torchTensordouble'
      'body': """
        <a name="torch.Tensor.double"></a>
        
        """

   'torch.Tensor.float':
      'prefix': 'torchTensorfloat'
      'body': """
        <a name="torch.Tensor.float"></a>
        
        Convenience methods for the [type](#torch.type) method. For e.g.,
        ```lua
        x = torch.Tensor(3):fill(3.14)
        > x
        3.1400
        3.1400
        3.1400
        [torch.DoubleTensor of dimension 3]
        
        -- calling type('torch.IntTensor')
        > x:type('torch.IntTensor')
        3
        3
        3
        [torch.IntTensor of dimension 3]
        
        
        -- is equivalent to calling int()
        > x:int()
        3
        3
        3
        [torch.IntTensor of dimension 3]
        ```
        
        ## Querying the size and structure ##
        
        
        """

   'torch.nDimension':
      'prefix': 'torchnDimension'
      'body': """
        <a name="torch.nDimension"></a>
        ### [number] nDimension() ###
        
        Returns the number of dimensions in a `Tensor`.
        ```lua
        x = torch.Tensor(4,5) -- a matrix
        > x:nDimension()
        2
        ```
        
        
        """

   'torch.Tensor.dim':
      'prefix': 'torchTensordim'
      'body': """
        <a name="torch.Tensor.dim"></a>
        ### [number] dim() ###
        
        Same as [nDimension()](#torch.nDimension).
        
        
        """

   'torch.Tensor.size':
      'prefix': 'torchTensorsize'
      'body': """
        <a name="torch.Tensor.size"></a>
        ### [number] size(dim) ###
        
        Returns the size of the specified dimension `dim`. Example:
        ```lua
        x = torch.Tensor(4,5):zero()
        > x
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        [torch.DoubleTensor of dimension 4x5]
        
        > x:size(2) -- gets the number of columns
        5
        ```
        
        
        """

   'torch.Tensor.size':
      'prefix': 'torchTensorsize'
      'body': """
        <a name="torch.Tensor.size"></a>
        ### [LongStorage] size() ###
        
        Returns a [LongStorage](storage.md) containing the size of each dimension
        of the tensor.
        ```lua
        x = torch.Tensor(4,5):zero()
        > x
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        [torch.DoubleTensor of dimension 4x5]
        
        > x:size()
        4
        5
        [torch.LongStorage of size 2]
        ```
        
        
        """

   'torch.Tensor.size':
      'prefix': 'torchTensorsize'
      'body': """
        <a name="torch.Tensor.size"></a>
        ### [LongStorage] #self ###
        
        Same as [size()](#torch.Tensor.size) method.
        
        
        """

   'torch.Tensor.stride':
      'prefix': 'torchTensorstride'
      'body': """
        <a name="torch.Tensor.stride"></a>
        ### [number] stride(dim) ###
        
        Returns the jump necessary to go from one element to the next one in the
        specified dimension `dim`. Example:
        ```lua
        x = torch.Tensor(4,5):zero()
        > x
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        [torch.DoubleTensor of dimension 4x5]
        
        -- elements in a row are contiguous in memory
        > x:stride(2)
        1
        
        -- to go from one element to the next one in a column
        -- we need here to jump the size of the row
        > x:stride(1)
        5
        ```
        
        Note also that in `Torch` _elements in the same row_ [elements along the __last__ dimension]
        are contiguous in memory for a matrix [tensor].
        
        
        """

   'torch.Tensor.stride':
      'prefix': 'torchTensorstride'
      'body': """
        <a name="torch.Tensor.stride"></a>
        ### [LongStorage] stride() ###
        
        Returns the jump necessary to go from one element to the next one in each dimension. Example:
        ```lua
        x = torch.Tensor(4,5):zero()
        > x
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        0 0 0 0 0
        [torch.DoubleTensor of dimension 4x5]
        
        > x:stride()
        5
        1 -- elements are contiguous in a row [last dimension]
        [torch.LongStorage of size 2]
        ```
        
        Note also that in `Torch` _elements in the same row_ [elements along the __last__ dimension]
        are contiguous in memory for a matrix [tensor].
        
        
        """

   'torch.storage':
      'prefix': 'torchstorage'
      'body': """
        <a name="torch.storage"></a>
        ### [Storage] storage() ###
        
        Returns the [Storage](storage.md) used to store all the elements of the `Tensor`.
        Basically, a `Tensor` is a particular way of _viewing_ a `Storage`.
        ```lua
        x = torch.Tensor(4,5)
        s = x:storage()
        for i=1,s:size() do -- fill up the Storage
        s[i] = i
        end
        
        > x -- s is interpreted by x as a 2D matrix
        1   2   3   4   5
        6   7   8   9  10
        11  12  13  14  15
        16  17  18  19  20
        [torch.DoubleTensor of dimension 4x5]
        ```
        
        
        """

   'torch.Tensor.isContiguous':
      'prefix': 'torchTensorisContiguous'
      'body': """
        <a name="torch.Tensor.isContiguous"></a>
        ### [boolean] isContiguous() ###
        
        Returns `true` iff the elements of the `Tensor` are contiguous in memory.
        ```lua
        -- normal tensors are contiguous in memory
        x = torch.randn(4,5)
        > x:isContiguous()
        true
        
        -- y now "views" the 3rd column of x
        -- the storage of y is the same than x
        -- so the memory cannot be contiguous
        y = x:select(2, 3)
        > y:isContiguous()
        false
        
        -- indeed, to jump to one element to
        -- the next one, the stride is 5
        > y:stride()
        5
        [torch.LongStorage of size 1]
        ```
        
        
        """

   'torch.Tensor.isSize':
      'prefix': 'torchTensorisSize'
      'body': """
        <a name="torch.Tensor.isSize"></a>
        ### [boolean] isSize(storage) ###
        
        Returns `true` iff the dimensions of the `Tensor` match the elements of the `storage`.
        ```lua
        x = torch.Tensor(4,5)
        y = torch.LongStorage({4,5})
        z = torch.LongStorage({5,4,1})
        > x:isSize(y)
        true
        
        > x:isSize(z)
        false
        
        > x:isSize(x:size())
        true
        ```
        
        
        """

   'torch.Tensor.isSameSizeAs':
      'prefix': 'torchTensorisSameSizeAs'
      'body': """
        <a name="torch.Tensor.isSameSizeAs"></a>
        ### [boolean] isSameSizeAs(tensor) ###
        
        Returns `true` iff the dimensions of the `Tensor` and the argument `Tensor` are exactly the same.
        ```lua
        x = torch.Tensor(4,5)
        y = torch.Tensor(4,5)
        > x:isSameSizeAs(y)
        true
        
        y = torch.Tensor(4,6)
        > x:isSameSizeAs(y)
        false
        ```
        
        
        """

   'torch.Tensor.nElement':
      'prefix': 'torchTensornElement'
      'body': """
        <a name="torch.Tensor.nElement"></a>
        ### [number] nElement() ###
        
        Returns the number of elements of a tensor.
        ```lua
        x = torch.Tensor(4,5)
        > x:nElement() -- 4x5 = 20!
        20
        ```
        
        
        """

   'torch.storageOffset':
      'prefix': 'torchstorageOffset'
      'body': """
        <a name="torch.storageOffset"></a>
        ### [number] storageOffset() ###
        
        Return the first index (starting at 1) used in the tensor's [storage](#torch.storage).
        
        
        """

   'torch.__index__':
      'prefix': 'torch__index__'
      'body': """
        <a name="torch.__index__"></a>
        ## Querying elements ##
        
        Elements of a tensor can be retrieved with the `[index]` operator.
        
        If `index` is a number, `[index]` operator is equivalent to a
        [`select(1, index)`](#torch.Tensor.select). If the tensor has more
        than one dimension, this operation returns a slice of the tensor that
        shares the same underlying storage. If the tensor is a 1D tensor, it
        returns the value at `index` in this tensor.
        
        If `index` is a table, the table must contain _n_ numbers, where
        _n_ is the [number of dimensions](#torch.nDimension) of the
        Tensor. It will return the element at the given position.
        
        In the same spirit, `index` might be a [LongStorage](storage.md),
        specifying the position (in the Tensor) of the element to be
        retrieved.
        
        If `index` is a `ByteTensor` in which each element is 0 or 1 then it acts as a
        selection mask used to extract a subset of the original tensor. This is
        particularly useful with [logical operators](maths.md#logical-operations-on-tensors)
        like [`torch.le`](maths.md#torchlea-b).
        
        Example:
        ```lua
        x = torch.Tensor(3,3)
        i = 0; x:apply(function() i = i + 1; return i end)
        > x
        1  2  3
        4  5  6
        7  8  9
        [torch.DoubleTensor of dimension 3x3]
        
        > x[2] -- returns row 2
        4
        5
        6
        [torch.DoubleTensor of dimension 3]
        
        > x[2][3] -- returns row 2, column 3
        6
        
        > x[{2,3}] -- another way to return row 2, column 3
        6
        
        > x[torch.LongStorage{2,3}] -- yet another way to return row 2, column 3
        6
        
        > x[torch.le(x,3)] -- torch.le returns a ByteTensor that acts as a mask
        1
        2
        3
        [torch.DoubleTensor of dimension 3]
        ```
        
        
        """

   'torch.Tensor.set':
      'prefix': 'torchTensorset'
      'body': """
        <a name="torch.Tensor.set"></a>
        ## Referencing a tensor to an existing tensor or chunk of memory ##
        
        A `Tensor` being a way of _viewing_ a [Storage](storage.md), it is
        possible to "set" a `Tensor` such that it views an existing [Storage](storage.md).
        
        Note that if you want to perform a set on an empty `Tensor` like
        ```lua
        y = torch.Storage(10)
        x = torch.Tensor()
        x:set(y, 1, 10)
        ```
        you might want in that case to use one of the [equivalent constructor](#torch.Tensor).
        ```lua
        y = torch.Storage(10)
        x = torch.Tensor(y, 1, 10)
        ```
        
        
        """

   'torch.Tensor.set':
      'prefix': 'torchTensorset'
      'body': """
        <a name="torch.Tensor.set"></a>
        ### [self] set(tensor) ###
        
        The `Tensor` is now going to "view" the same [storage](#torch.storage)
        as the given `tensor`. As the result, any modification in the elements of
        the `Tensor` will have an impact on the elements of the given `tensor`, and
        vice-versa. This is an efficient method, as there is no memory copy!
        
        ```lua
        x = torch.Tensor(2,5):fill(3.14)
        > x
        3.1400  3.1400  3.1400  3.1400  3.1400
        3.1400  3.1400  3.1400  3.1400  3.1400
        [torch.DoubleTensor of dimension 2x5]
        
        y = torch.Tensor():set(x)
        > y
        3.1400  3.1400  3.1400  3.1400  3.1400
        3.1400  3.1400  3.1400  3.1400  3.1400
        [torch.DoubleTensor of dimension 2x5]
        
        y:zero()
        > x -- elements of x are the same than y!
        0 0 0 0 0
        0 0 0 0 0
        [torch.DoubleTensor of dimension 2x5]
        ```
        
        
        """

   'torch.Tensor.isSetTo':
      'prefix': 'torchTensorisSetTo'
      'body': """
        <a name="torch.Tensor.isSetTo"></a>
        ### [boolean] isSetTo(tensor) ###
        
        Returns true iff the `Tensor` is set to the argument `Tensor`. Note: this is
        only true if the tensors are the same size, have the same strides and share the
        same storage and offset.
        
        ```lua
        x = torch.Tensor(2,5)
        y = torch.Tensor()
        > y:isSetTo(x)
        false
        > y:set(x)
        > y:isSetTo(x)
        true
        > y:t():isSetTo(x)
        false -- x and y have different strides
        ```
        
        
        """

   'torch.Tensor.set':
      'prefix': 'torchTensorset'
      'body': """
        <a name="torch.Tensor.set"></a>
        ### [self] set(storage, [storageOffset, sizes, [strides]]) ###
        
        The `Tensor` is now going to "view" the given
        [`storage`](storage.md), starting at position `storageOffset` (>=1)
        with the given [dimension `sizes`](#torch.Tensor.size) and the optional given
        [`strides`](#torch.Tensor.stride). As the result, any modification in the
        elements of the `Storage` will have a impact on the elements of the
        `Tensor`, and vice-versa. This is an efficient method, as there is no
        memory copy!
        
        If only `storage` is provided, the whole storage will be viewed as a 1D Tensor.
        
        ```lua
        -- creates a storage with 10 elements
        s = torch.Storage(10):fill(1)
        
        -- we want to see it as a 2x5 tensor
        sz = torch.LongStorage({2,5})
        x = torch.Tensor()
        x:set(s, 1, sz)
        > x
        1  1  1  1  1
        1  1  1  1  1
        [torch.DoubleTensor of dimension 2x5]
        
        x:zero()
        > s -- the storage contents have been modified
        0
        0
        0
        0
        0
        0
        0
        0
        0
        0
        [torch.DoubleStorage of size 10]
        ```
        
        
        """

   'torch.Tensor.set':
      'prefix': 'torchTensorset'
      'body': """
        <a name="torch.Tensor.set"></a>
        ### [self] set(storage, [storageOffset, sz1 [, st1 ... [, sz4 [, st4]]]]) ###
        
        This is a "shortcut" for previous method.
        It works up to 4 dimensions. `szi` is the size of the `i`-th dimension of the tensor.
        `sti` is the stride in the `i`-th dimension.
        
        ## Copying and initializing ##
        
        
        """

   'torch.Tensor.copy':
      'prefix': 'torchTensorcopy'
      'body': """
        <a name="torch.Tensor.copy"></a>
        ### [self] copy(tensor) ###
        
        Replace the elements of the `Tensor` by copying the elements of the given `tensor`. The
        [number of elements](#torch.Tensor.nElement) must match, but the
        sizes might be different.
        
        ```lua
        x = torch.Tensor(4):fill(1)
        y = torch.Tensor(2,2):copy(x)
        > x
        1
        1
        1
        1
        [torch.DoubleTensor of dimension 4]
        
        > y
        1  1
        1  1
        [torch.DoubleTensor of dimension 2x2]
        ```
        
        If a different type of `tensor` is given, then a type conversion occurs,
        which, of course, might result in loss of precision.
        
        
        """

   'torch.fill':
      'prefix': 'torchfill'
      'body': """
        <a name="torch.fill"></a>
        ### [self] fill(value) ###
        
        Fill the tensor with the given `value`.
        ```lua
        > torch.DoubleTensor(4):fill(3.14)
        3.1400
        3.1400
        3.1400
        3.1400
        [torch.DoubleTensor of dimension 4]
        ```
        
        
        """

   'torch.zero':
      'prefix': 'torchzero'
      'body': """
        <a name="torch.zero"></a>
        ### [self] zero() ###
        
        Fill the tensor with zeros.
        ```lua
        > torch.Tensor(4):zero()
        0
        0
        0
        0
        [torch.DoubleTensor of dimension 4]
        ```
        
        
        """

   'torch.resize.dok':
      'prefix': 'torchresizedok'
      'body': """
        <a name="torch.resize.dok"></a>
        ## Resizing ##
        
        __When resizing to a larger size__, the underlying [Storage](storage.md) is resized to fit
        all the elements of the `Tensor`.
        
        __When resizing to a smaller size__, the underlying [Storage](#Storage) is not resized.
        
        __Important note:__ the content of a `Tensor` after resizing is _undetermined_ as [strides](#torch.Tensor.stride)
        might have been completely changed. In particular, _the elements of the resized tensor are contiguous in memory_.
        
        
        """

   'torch.Tensor.resizeAs':
      'prefix': 'torchTensorresizeAs'
      'body': """
        <a name="torch.Tensor.resizeAs"></a>
        ### [self] resizeAs(tensor) ###
        
        Resize the `tensor` as the given `tensor` (of the same type).
        
        
        """

   'torch.resize':
      'prefix': 'torchresize'
      'body': """
        <a name="torch.resize"></a>
        ### [self] resize(sizes) ###
        
        Resize the `tensor` according to the given [LongStorage](storage.md) `sizes`.
        
        
        """

   'torch.resize':
      'prefix': 'torchresize'
      'body': """
        <a name="torch.resize"></a>
        ### [self] resize(sz1 [,sz2 [,sz3 [,sz4]]]]) ###
        
        Convenience method of the previous method, working for a number of dimensions up to 4.
        
        ## Extracting sub-tensors ##
        
        Each of these methods returns a `Tensor` which is a sub-tensor of the given
        tensor, _with the same `Storage`_. Hence, any modification in the memory of
        the sub-tensor will have an impact on the primary tensor, and vice-versa.
        
        These methods are very fast, as they do not involve any memory copy.
        
        
        """

   'torch.Tensor.narrow':
      'prefix': 'torchTensornarrow'
      'body': """
        <a name="torch.Tensor.narrow"></a>
        ### [self] narrow(dim, index, size) ###
        
        Returns a new `Tensor` which is a narrowed version of the current one: the dimension `dim` is narrowed
        from `index` to `index+size-1`.
        
        ```lua
        x = torch.Tensor(5, 6):zero()
        > x
        
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        [torch.DoubleTensor of dimension 5x6]
        
        y = x:narrow(1, 2, 3) -- narrow dimension 1 from index 2 to index 2+3-1
        y:fill(1) -- fill with 1
        > y
        1  1  1  1  1  1
        1  1  1  1  1  1
        1  1  1  1  1  1
        [torch.DoubleTensor of dimension 3x6]
        
        > x -- memory in x has been modified!
        0  0  0  0  0  0
        1  1  1  1  1  1
        1  1  1  1  1  1
        1  1  1  1  1  1
        0  0  0  0  0  0
        [torch.DoubleTensor of dimension 5x6]
        ```
        
        
        """

   'torch.Tensor.sub':
      'prefix': 'torchTensorsub'
      'body': """
        <a name="torch.Tensor.sub"></a>
        ### [Tensor] sub(dim1s, dim1e ... [, dim4s [, dim4e]]) ###
        
        This method is equivalent to do a series of
        [narrow](#torch.Tensor.narrow) up to the first 4 dimensions.  It
        returns a new `Tensor` which is a sub-tensor going from index
        `dimis` to `dimie` in the `i`-th dimension. Negative values are
        interpreted index starting from the end: `-1` is the last index,
        `-2` is the index before the last index, ...
        
        ```lua
        x = torch.Tensor(5, 6):zero()
        > x
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        [torch.DoubleTensor of dimension 5x6]
        
        y = x:sub(2,4):fill(1) -- y is sub-tensor of x:
        > y                    -- dimension 1 starts at index 2, ends at index 4
        1  1  1  1  1  1
        1  1  1  1  1  1
        1  1  1  1  1  1
        [torch.DoubleTensor of dimension 3x6]
        
        > x                    -- x has been modified!
        0  0  0  0  0  0
        1  1  1  1  1  1
        1  1  1  1  1  1
        1  1  1  1  1  1
        0  0  0  0  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        z = x:sub(2,4,3,4):fill(2) -- we now take a new sub-tensor
        > z                        -- dimension 1 starts at index 2, ends at index 4
        -- dimension 2 starts at index 3, ends at index 4
        2  2
        2  2
        2  2
        [torch.DoubleTensor of dimension 3x2]
        
        > x                        -- x has been modified
        0  0  0  0  0  0
        1  1  2  2  1  1
        1  1  2  2  1  1
        1  1  2  2  1  1
        0  0  0  0  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        > y                        -- y has been modified
        1  1  2  2  1  1
        1  1  2  2  1  1
        1  1  2  2  1  1
        [torch.DoubleTensor of dimension 3x6]
        
        > y:sub(-1, -1, 3, 4)      -- negative values = bounds
        2  2
        [torch.DoubleTensor of dimension 1x2]
        ```
        
        
        """

   'torch.Tensor.select':
      'prefix': 'torchTensorselect'
      'body': """
        <a name="torch.Tensor.select"></a>
        ### [Tensor] select(dim, index) ###
        
        Returns a new `Tensor` which is a tensor slice at the given `index` in the
        dimension `dim`. The returned tensor has one less dimension: the dimension
        `dim` is removed.  As a result, it is not possible to `select()` on a 1D
        tensor.
        
        Note that "selecting" on the first dimension is equivalent to use the [[] operator](#torch.__index__ )
        
        ```lua
        x = torch.Tensor(5,6):zero()
        > x
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        [torch.DoubleTensor of dimension 5x6]
        
        y = x:select(1, 2):fill(2) -- select row 2 and fill up
        > y
        2
        2
        2
        2
        2
        2
        [torch.DoubleTensor of dimension 6]
        
        > x
        0  0  0  0  0  0
        2  2  2  2  2  2
        0  0  0  0  0  0
        0  0  0  0  0  0
        0  0  0  0  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        z = x:select(2,5):fill(5) -- select column 5 and fill up
        > z
        5
        5
        5
        5
        5
        [torch.DoubleTensor of dimension 5]
        
        > x
        0  0  0  0  5  0
        2  2  2  2  5  2
        0  0  0  0  5  0
        0  0  0  0  5  0
        0  0  0  0  5  0
        [torch.DoubleTensor of dimension 5x6]
        ```
        
        
        """

   'torch.Tensor.indexing':
      'prefix': 'torchTensorindexing'
      'body': """
        <a name="torch.Tensor.indexing"></a>
        ### [Tensor] [{ dim1,dim2,... }] or [{ {dim1s,dim1e}, {dim2s,dim2e} }] ###
        
        The indexing operator [] can be used to combine narrow/sub and
        select in a concise and efficient way. It can also be used
        to copy, and fill (sub) tensors.
        
        This operator also works with an input mask made of a `ByteTensor` with 0 and 1
        elements, e.g with a [logical operator](maths.md#logical-operations-on-tensors).
        
        ```lua
        x = torch.Tensor(5, 6):zero()
        > x
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        0 0 0 0 0 0
        [torch.DoubleTensor of dimension 5x6]
        
        x[{ 1,3 }] = 1 -- sets element at (i=1,j=3) to 1
        > x
        0  0  1  0  0  0
        0  0  0  0  0  0
        0  0  0  0  0  0
        0  0  0  0  0  0
        0  0  0  0  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        x[{ 2,{2,4} }] = 2  -- sets a slice of 3 elements to 2
        > x
        0  0  1  0  0  0
        0  2  2  2  0  0
        0  0  0  0  0  0
        0  0  0  0  0  0
        0  0  0  0  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        x[{ {},4 }] = -1 -- sets the full 4th column to -1
        > x
        0  0  1 -1  0  0
        0  2  2 -1  0  0
        0  0  0 -1  0  0
        0  0  0 -1  0  0
        0  0  0 -1  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        x[{ {},2 }] = torch.range(1,5) -- copy a 1D tensor to a slice of x
        > x
        
        0  1  1 -1  0  0
        0  2  2 -1  0  0
        0  3  0 -1  0  0
        0  4  0 -1  0  0
        0  5  0 -1  0  0
        [torch.DoubleTensor of dimension 5x6]
        
        x[torch.lt(x,0)] = -2 -- sets all negative elements to -2 via a mask
        > x
        
        0  1  1 -2  0  0
        0  2  2 -2  0  0
        0  3  0 -2  0  0
        0  4  0 -2  0  0
        0  5  0 -2  0  0
        [torch.DoubleTensor of dimension 5x6]
        ```
        
        
        """

   'torch.Tensor.index':
      'prefix': 'torchTensorindex'
      'body': """
        <a name="torch.Tensor.index"></a>
        ### [Tensor] index(dim, index) ###
        
        Returns a new `Tensor` which indexes the original `Tensor` along dimension `dim`
        using the entries in `torch.LongTensor` `index`.
        The returned `Tensor` has the same number of dimensions as the original `Tensor`.
        The returned `Tensor` does __not__ use the same storage as the original `Tensor` -- see below for storing the result
        in an existing `Tensor`.
        
        ```lua
        x = torch.rand(5,5)
        > x
        0.8020  0.7246  0.1204  0.3419  0.4385
        0.0369  0.4158  0.0985  0.3024  0.8186
        0.2746  0.9362  0.2546  0.8586  0.6674
        0.7473  0.9028  0.1046  0.9085  0.6622
        0.1412  0.6784  0.1624  0.8113  0.3949
        [torch.DoubleTensor of dimension 5x5]
        
        y = x:index(1,torch.LongTensor{3,1})
        > y
        0.2746  0.9362  0.2546  0.8586  0.6674
        0.8020  0.7246  0.1204  0.3419  0.4385
        [torch.DoubleTensor of dimension 2x5]
        
        y:fill(1)
        > y
        1  1  1  1  1
        1  1  1  1  1
        [torch.DoubleTensor of dimension 2x5]
        
        > x
        0.8020  0.7246  0.1204  0.3419  0.4385
        0.0369  0.4158  0.0985  0.3024  0.8186
        0.2746  0.9362  0.2546  0.8586  0.6674
        0.7473  0.9028  0.1046  0.9085  0.6622
        0.1412  0.6784  0.1624  0.8113  0.3949
        [torch.DoubleTensor of dimension 5x5]
        
        ```
        
        Note the explicit `index` function is different than the indexing operator `[]`.
        The indexing operator `[]` is a syntactic shortcut for a series of select and narrow operations,
        therefore it always returns a new view on the original tensor that shares the same storage.
        However, the explicit `index` function can not use the same storage.
        
        It is possible to store the result into an existing Tensor with `result:index(source, ...)`:
        
        ```lua
        x = torch.rand(5,5)
        > x
        0.8020  0.7246  0.1204  0.3419  0.4385
        0.0369  0.4158  0.0985  0.3024  0.8186
        0.2746  0.9362  0.2546  0.8586  0.6674
        0.7473  0.9028  0.1046  0.9085  0.6622
        0.1412  0.6784  0.1624  0.8113  0.3949
        [torch.DoubleTensor of dimension 5x5]
        
        y = torch.Tensor()
        y:index(x,1,torch.LongTensor{3,1})
        > y
        0.2746  0.9362  0.2546  0.8586  0.6674
        0.8020  0.7246  0.1204  0.3419  0.4385
        [torch.DoubleTensor of dimension 2x5]
        ```
        
        
        
        """

   'torch.Tensor.indexCopy':
      'prefix': 'torchTensorindexCopy'
      'body': """
        <a name="torch.Tensor.indexCopy"></a>
        ### [Tensor] indexCopy(dim, index, tensor) ###
        
        Copies the elements of `tensor` into the original tensor by selecting the indices in the order
        given in `index`. The shape of `tensor` must exactly match the elements indexed or an error will be thrown.
        
        ```lua
        > x
        0.8020  0.7246  0.1204  0.3419  0.4385
        0.0369  0.4158  0.0985  0.3024  0.8186
        0.2746  0.9362  0.2546  0.8586  0.6674
        0.7473  0.9028  0.1046  0.9085  0.6622
        0.1412  0.6784  0.1624  0.8113  0.3949
        [torch.DoubleTensor of dimension 5x5]
        
        z=torch.Tensor(5,2)
        z:select(2,1):fill(-1)
        z:select(2,2):fill(-2)
        > z
        -1 -2
        -1 -2
        -1 -2
        -1 -2
        -1 -2
        [torch.DoubleTensor of dimension 5x2]
        
        x:indexCopy(2,torch.LongTensor{5,1},z)
        > x
        -2.0000  0.7246  0.1204  0.3419 -1.0000
        -2.0000  0.4158  0.0985  0.3024 -1.0000
        -2.0000  0.9362  0.2546  0.8586 -1.0000
        -2.0000  0.9028  0.1046  0.9085 -1.0000
        -2.0000  0.6784  0.1624  0.8113 -1.0000
        [torch.DoubleTensor of dimension 5x5]
        
        ```
        
        
        """

   'torch.Tensor.indexAdd':
      'prefix': 'torchTensorindexAdd'
      'body': """
        <a name="torch.Tensor.indexAdd"></a>
        ### [Tensor] indexAdd(dim, index, tensor) ###
        
        Accumulate the elements of `tensor` into the original tensor by adding to the indices in the order
        given in `index`. The shape of `tensor` must exactly match the elements indexed or an error will be thrown.
        
        ```lua
        Example 1
        
        > x
        -2.1742  0.5688 -1.0201  0.1383  1.0504
        0.0970  0.2169  0.1324  0.9553 -1.9518
        -0.7607  0.8947  0.1658 -0.2181 -2.1237
        -1.4099  0.2342  0.4549  0.6316 -0.2608
        0.0349  0.4713  0.0050  0.1677  0.2103
        [torch.DoubleTensor of size 5x5]
        
        z=torch.Tensor(5, 2)
        z:select(2,1):fill(-1)
        z:select(2,2):fill(-2)
        > z
        -1 -2
        -1 -2
        -1 -2
        -1 -2
        -1 -2
        [torch.DoubleTensor of dimension 5x2]
        
        > x:indexAdd(2,torch.LongTensor{5,1},z)
        > x
        -4.1742  0.5688 -1.0201  0.1383  0.0504
        -1.9030  0.2169  0.1324  0.9553 -2.9518
        -2.7607  0.8947  0.1658 -0.2181 -3.1237
        -3.4099  0.2342  0.4549  0.6316 -1.2608
        -1.9651  0.4713  0.0050  0.1677 -0.7897
        [torch.DoubleTensor of size 5x5]
        
        Example 2
        
        > a = torch.range(1, 5)
        > a
        1
        2
        3
        4
        5
        [torch.DoubleTensor of size 5]
        
        > a:indexAdd(1, torch.LongTensor{1, 1, 3, 3}, torch.range(1, 4))
        > a
        4
        2
        10
        4
        5
        [torch.DoubleTensor of size 5]
        
        ```
        
        
        """

   'torch.Tensor.indexFill':
      'prefix': 'torchTensorindexFill'
      'body': """
        <a name="torch.Tensor.indexFill"></a>
        ### [Tensor] indexFill(dim, index, val) ###
        
        Fills the elements of the original `Tensor` with value `val` by selecting the indices in the order
        given in `index`.
        
        ```lua
        x=torch.rand(5,5)
        > x
        0.8414  0.4121  0.3934  0.5600  0.5403
        0.3029  0.2040  0.7893  0.6079  0.6334
        0.3743  0.1389  0.1573  0.1357  0.8460
        0.2838  0.9925  0.0076  0.7220  0.5185
        0.8739  0.6887  0.4271  0.0385  0.9116
        [torch.DoubleTensor of dimension 5x5]
        
        x:indexFill(2,torch.LongTensor{4,2},-10)
        > x
        0.8414 -10.0000   0.3934 -10.0000   0.5403
        0.3029 -10.0000   0.7893 -10.0000   0.6334
        0.3743 -10.0000   0.1573 -10.0000   0.8460
        0.2838 -10.0000   0.0076 -10.0000   0.5185
        0.8739 -10.0000   0.4271 -10.0000   0.9116
        [torch.DoubleTensor of dimension 5x5]
        
        ```
        
        
        """

   'torch.Tensor.gather':
      'prefix': 'torchTensorgather'
      'body': """
        <a name="torch.Tensor.gather"></a>
        ### [Tensor] gather(dim, index) ###
        
        Creates a new `Tensor` from the original tensor by gathering a number of values from
        each "row", where the rows are along the dimension `dim`. The values in a `LongTensor`, passed as `index`,
        specify which values to take from each row. Specifically, the resulting `Tensor`, which will have the same size as
        the `index` tensor, is given by
        
        ```lua
        -- dim = 1
        result[i][j][k]... = src[index[i][j][k]...][j][k]...
        
        -- dim = 2
        result[i][j][k]... = src[i][index[i][j][k]...][k]...
        
        -- etc.
        ```
        where `src` is the original `Tensor`.
        
        The same number of values are selected from each row, and the same value cannot be selected from a row more than
        once. The values in the `index` tensor must not be larger than the length of the row, that is they must be between
        1 and `src:size(dim)` inclusive. It can be somewhat confusing to ensure that the `index` tensor has the correct shape.
        Viewed pictorially:
        
        ![The gather operation](gather.png)
        
        Numerically, to give an example, if `src` has size `n x m x p x q`, we are gathering along `dim = 3`, and we wish to
        gather `k` elements from each row (where `k <= p`) then `index` must have size `n x m x k x q`.
        
        It is possible to store the result into an existing Tensor with `result:gather(src, ...)`.
        
        ```lua
        x = torch.rand(5, 5)
        > x
        0.7259  0.5291  0.4559  0.4367  0.4133
        0.0513  0.4404  0.4741  0.0658  0.0653
        0.3393  0.1735  0.6439  0.1011  0.7923
        0.7606  0.5025  0.5706  0.7193  0.1572
        0.1720  0.3546  0.8354  0.8339  0.3025
        [torch.DoubleTensor of size 5x5]
        
        y = x:gather(1, torch.LongTensor{{1, 2, 3, 4, 5}, {2, 3, 4, 5, 1}})
        > y
        0.7259  0.4404  0.6439  0.7193  0.3025
        0.0513  0.1735  0.5706  0.8339  0.4133
        [torch.DoubleTensor of size 2x5]
        
        z = x:gather(2, torch.LongTensor{{1, 2}, {2, 3}, {3, 4}, {4, 5}, {5, 1}})
        > z
        0.7259  0.5291
        0.4404  0.4741
        0.6439  0.1011
        0.7193  0.1572
        0.3025  0.1720
        [torch.DoubleTensor of size 5x2]
        
        ```
        
        
        """

   'torch.Tensor.scatter':
      'prefix': 'torchTensorscatter'
      'body': """
        <a name="torch.Tensor.scatter"></a>
        ### [Tensor] scatter(dim, index, src|val) ###
        
        Writes all values from tensor `src` or the scalar `val` into `self` at the specified indices. The indices are specified
        with respect to the given dimension, `dim`, in the manner described in [gather](#torch.Tensor.gather). Note that, as
        for gather, the values of index must be between 1 and `self:size(dim)` inclusive and all values in a row along the
        specified dimension must be unique.
        
        ```lua
        x = torch.rand(2, 5)
        > x
        0.3227  0.4294  0.8476  0.9414  0.1159
        0.7338  0.5185  0.2947  0.0578  0.1273
        [torch.DoubleTensor of size 2x5]
        
        y = torch.zeros(3, 5):scatter(1, torch.LongTensor{{1, 2, 3, 1, 1}, {3, 1, 1, 2, 3}}, x)
        > y
        0.3227  0.5185  0.2947  0.9414  0.1159
        0.0000  0.4294  0.0000  0.0578  0.0000
        0.7338  0.0000  0.8476  0.0000  0.1273
        [torch.DoubleTensor of size 3x5]
        
        z = torch.zeros(2, 4):scatter(2, torch.LongTensor{{3}, {4}}, 1.23)
        > z
        0.0000  0.0000  1.2300  0.0000
        0.0000  0.0000  0.0000  1.2300
        [torch.DoubleTensor of size 2x4]
        
        ```
        
        
        """

   'torch.Tensor.maskedSelect':
      'prefix': 'torchTensormaskedSelect'
      'body': """
        <a name="torch.Tensor.maskedSelect"></a>
        ### [Tensor] maskedSelect(mask) ###
        
        Returns a new Tensor which contains all elements aligned to a `1` in the corresponding
        `mask`. This `mask` is a `torch.ByteTensor` of zeros and ones. The `mask` and
        `Tensor` must have the same number of elements. The resulting Tensor will
        be a 1D tensor of the same type as `Tensor` having size `mask:sum()`.
        
        ```lua
        x = torch.range(1,12):double():resize(3,4)
        > x
        1   2   3   4
        5   6   7   8
        9  10  11  12
        [torch.DoubleTensor of dimension 3x4]
        
        mask = torch.ByteTensor(2,6):bernoulli()
        > mask
        1  0  1  0  0  0
        1  1  0  0  0  1
        [torch.ByteTensor of dimension 2x6]
        
        y = x:maskedSelect(mask)
        > y
        1
        3
        7
        8
        12
        [torch.DoubleTensor of dimension 5]
        
        z = torch.DoubleTensor()
        z:maskedSelect(x, mask)
        > z
        1
        3
        7
        8
        12
        ```
        
        Note how the dimensions of the above `x`, `mask` and `y` do not match.
        Also note how an existing tensor `z` can be used to store the results.
        
        
        
        """

   'torch.Tensor.maskedCopy':
      'prefix': 'torchTensormaskedCopy'
      'body': """
        <a name="torch.Tensor.maskedCopy"></a>
        ### [Tensor] maskedCopy(mask, tensor) ###
        
        Copies the masked elements of `tensor` into itself. The masked elements are those elements having a
        corresponding `1` in the `mask` Tensor. This `mask` is a `torch.ByteTensor`
        of zeros and ones. The destination `Tensor` and the `mask` Tensor should have the same number of elements.
        The source `tensor` should have at least as many elements as the number of 1s in the `mask`.
        
        ```lua
        x = torch.range(1,4):double():resize(2,2)
        > x
        1  2
        3  4
        [torch.DoubleTensor of dimension 2x4]
        
        mask = torch.ByteTensor(1,8):bernoulli()
        > mask
        0  0  1  1  1  0  1  0
        [torch.ByteTensor of dimension 1x8]
        
        y = torch.DoubleTensor(2,4):fill(-1)
        > y
        -1 -1 -1 -1
        -1 -1 -1 -1
        [torch.DoubleTensor of dimension 2x4]
        
        y:maskedCopy(mask, x)
        > y
        -1 -1  1  2
        3 -1  4 -1
        [torch.DoubleTensor of dimension 2x4]
        ```
        
        Note how the dimensions of the above `x`, `mask` and `y' do not match,
        but the number of elements do.
        
        
        """

   'torch.Tensor.maskedFill':
      'prefix': 'torchTensormaskedFill'
      'body': """
        <a name="torch.Tensor.maskedFill"></a>
        ### [Tensor] maskedFill(mask, val) ###
        
        Fills the masked elements of itself with value `val`. The masked elements are those elements having a
        corresponding `1` in the `mask` Tensor. This `mask` is a `torch.ByteTensor`
        of zeros and ones. The `mask` and `Tensor` must have the same number of elements.
        
        ```lua
        x = torch.range(1,4):double():resize(1,4)
        > x
        1  2  3  4
        [torch.DoubleTensor of dimension 1x4]
        
        mask = torch.ByteTensor(2,2):bernoulli()
        > mask
        0  0
        1  1
        [torch.ByteTensor of dimension 2x2]
        
        x:maskedFill(mask, -1)
        > x
        1  2 -1 -1
        [torch.DoubleTensor of dimension 1x4]
        
        ```
        Note how the dimensions of the above `x` and `mask` do not match,
        but the number of elements do.
        
        ## Search ##
        
        Each of these methods returns a `LongTensor` corresponding to the indices of the
        given search operation.
        
        
        """

   'torch.Tensor.nonzero':
      'prefix': 'torchTensornonzero'
      'body': """
        <a name="torch.Tensor.nonzero"/>
        ### [LongTensor] nonzero(tensor)
        
        Finds and returns a `LongTensor` corresponding to the *subscript* indices of all
        non-zero elements in `tensor`.
        
        Note that torch uses the first argument on dispatch to determine the return
        type. Since the first argument is any `torch.TensorType`, but the return type
        is always `torch.LongTensor`, the function call
        `torch.nonzero(torch.LongTensor(), tensor)` does not work. However,
        `tensor.nonzero(torch.LongTensor(), tensor)` does work.
        
        ```lua
        > x = torch.rand(4, 4):mul(3):floor():int()
        > x
        2  0  2  0
        0  0  1  2
        0  2  2  1
        2  1  2  2
        [torch.IntTensor of dimension 4x4]
        
        > torch.nonzero(x)
        1  1
        1  3
        2  3
        2  4
        3  2
        3  3
        3  4
        4  1
        4  2
        4  3
        4  4
        [torch.LongTensor of dimension 11x2]
        
        > x:nonzero()
        1  1
        1  3
        2  3
        2  4
        3  2
        3  3
        3  4
        4  1
        4  2
        4  3
        4  4
        [torch.LongTensor of dimension 11x2]
        
        > indices = torch.LongTensor()
        > x.nonzero(indices, x)
        1  1
        1  3
        2  3
        2  4
        3  2
        3  3
        3  4
        4  1
        4  2
        4  3
        4  4
        [torch.LongTensor of dimension 11x2]
        
        > x:eq(1):nonzero()
        2  3
        3  4
        4  2
        [torch.LongTensor of dimension 3x2]
        
        ```
        
        ## Expanding/Replicating/Squeezing Tensors ##
        
        These methods returns a Tensor which is created by replications of the
        original tensor.
        
        
        """

   'torch.expand':
      'prefix': 'torchexpand'
      'body': """
        <a name="torch.expand"></a>
        #### [result] expand([result,] sizes) ####
        
        `sizes` can either be a `torch.LongStorage` or numbers. Expanding a tensor
        does not allocate new memory, but only creates a new view on the existing tensor where
        singleton dimensions can be expanded to multiple ones by setting the `stride` to 0.
        Any dimension that has size 1 can be expanded to arbitrary value without any new memory allocation. Attempting to
        expand along a dimension that does not have size 1 will result in an error.
        
        ```lua
        x = torch.rand(10,1)
        > x
        0.3837
        0.5966
        0.0763
        0.1896
        0.4958
        0.6841
        0.4038
        0.4068
        0.1502
        0.2239
        [torch.DoubleTensor of dimension 10x1]
        
        y = torch.expand(x,10,2)
        > y
        0.3837  0.3837
        0.5966  0.5966
        0.0763  0.0763
        0.1896  0.1896
        0.4958  0.4958
        0.6841  0.6841
        0.4038  0.4038
        0.4068  0.4068
        0.1502  0.1502
        0.2239  0.2239
        [torch.DoubleTensor of dimension 10x2]
        
        y:fill(1)
        > y
        1  1
        1  1
        1  1
        1  1
        1  1
        1  1
        1  1
        1  1
        1  1
        1  1
        [torch.DoubleTensor of dimension 10x2]
        
        > x
        1
        1
        1
        1
        1
        1
        1
        1
        1
        1
        [torch.DoubleTensor of dimension 10x1]
        
        i=0; y:apply(function() i=i+1;return i end)
        > y
        2   2
        4   4
        6   6
        8   8
        10  10
        12  12
        14  14
        16  16
        18  18
        20  20
        [torch.DoubleTensor of dimension 10x2]
        
        > x
        2
        4
        6
        8
        10
        12
        14
        16
        18
        20
        [torch.DoubleTensor of dimension 10x1]
        
        ```
        
        
        """

   'torch.Tensor.expandAs':
      'prefix': 'torchTensorexpandAs'
      'body': """
        <a name="torch.Tensor.expandAs"></a>
        #### [result] expandAs([result,] tensor) ####
        
        This is equivalent to `self:expand(tensor:size())`
        
        
        """

   'torch.repeatTensor':
      'prefix': 'torchrepeatTensor'
      'body': """
        <a name="torch.repeatTensor"></a>
        #### [Tensor] repeatTensor([result,] sizes) ####
        
        `sizes` can either be a `torch.LongStorage` or numbers. Repeating a tensor allocates
        new memory, unless `result` is provided, in which case its memory is
        resized. `sizes` specify the number of times the tensor is repeated in each dimension.
        
        ```lua
        x = torch.rand(5)
        > x
        0.7160
        0.6514
        0.0704
        0.7856
        0.7452
        [torch.DoubleTensor of dimension 5]
        
        > torch.repeatTensor(x,3,2)
        0.7160  0.6514  0.0704  0.7856  0.7452  0.7160  0.6514  0.0704  0.7856  0.7452
        0.7160  0.6514  0.0704  0.7856  0.7452  0.7160  0.6514  0.0704  0.7856  0.7452
        0.7160  0.6514  0.0704  0.7856  0.7452  0.7160  0.6514  0.0704  0.7856  0.7452
        [torch.DoubleTensor of dimension 3x10]
        
        > torch.repeatTensor(x,3,2,1)
        (1,.,.) =
        0.7160  0.6514  0.0704  0.7856  0.7452
        0.7160  0.6514  0.0704  0.7856  0.7452
        
        (2,.,.) =
        0.7160  0.6514  0.0704  0.7856  0.7452
        0.7160  0.6514  0.0704  0.7856  0.7452
        
        (3,.,.) =
        0.7160  0.6514  0.0704  0.7856  0.7452
        0.7160  0.6514  0.0704  0.7856  0.7452
        [torch.DoubleTensor of dimension 3x2x5]
        
        ```
        
        
        """

   'torch.squeeze':
      'prefix': 'torchsqueeze'
      'body': """
        <a name="torch.squeeze"></a>
        #### [Tensor] squeeze([dim]) ####
        
        Removes all singleton dimensions of the tensor.
        If `dim` is given, squeezes only that particular dimension of the tensor.
        
        ```lua
        x=torch.rand(2,1,2,1,2)
        > x
        (1,1,1,.,.) =
        0.6020  0.8897
        
        (2,1,1,.,.) =
        0.4713  0.2645
        
        (1,1,2,.,.) =
        0.4441  0.9792
        
        (2,1,2,.,.) =
        0.5467  0.8648
        [torch.DoubleTensor of dimension 2x1x2x1x2]
        
        > torch.squeeze(x)
        (1,.,.) =
        0.6020  0.8897
        0.4441  0.9792
        
        (2,.,.) =
        0.4713  0.2645
        0.5467  0.8648
        [torch.DoubleTensor of dimension 2x2x2]
        
        > torch.squeeze(x,2)
        (1,1,.,.) =
        0.6020  0.8897
        
        (2,1,.,.) =
        0.4713  0.2645
        
        (1,2,.,.) =
        0.4441  0.9792
        
        (2,2,.,.) =
        0.5467  0.8648
        [torch.DoubleTensor of dimension 2x2x1x2]
        
        ```
        
        ## Manipulating the tensor view ##
        
        Each of these methods returns a `Tensor` which is another way of viewing
        the `Storage` of the given tensor. Hence, any modification in the memory of
        the sub-tensor will have an impact on the primary tensor, and vice-versa.
        
        These methods are very fast, because they do not involve any memory copy.
        
        
        """

   'torch.view':
      'prefix': 'torchview'
      'body': """
        <a name="torch.view"></a>
        ### [result] view([result,] tensor, sizes) ###
        
        Creates a view with different dimensions of the storage associated with `tensor`.
        If `result` is not passed, then a new tensor is returned, otherwise its storage is
        made to point to storage of `tensor`.
        
        `sizes` can either be a `torch.LongStorage` or numbers. If one of the dimensions
        is -1, the size of that dimension is inferred from the rest of the elements.
        
        
        ```lua
        x = torch.zeros(4)
        > x:view(2,2)
        0 0
        0 0
        [torch.DoubleTensor of dimension 2x2]
        
        > x:view(2,-1)
        0 0
        0 0
        [torch.DoubleTensor of dimension 2x2]
        
        > x:view(torch.LongStorage{2,2})
        0 0
        0 0
        [torch.DoubleTensor of dimension 2x2]
        
        > x
        0
        0
        0
        0
        [torch.DoubleTensor of dimension 4]
        ```
        
        
        """

   'torch.viewAs':
      'prefix': 'torchviewAs'
      'body': """
        <a name="torch.viewAs"></a>
        ### [result] viewAs([result,] tensor, template) ###
        
        Creates a view with the same dimensions as `template` of the storage associated
        with `tensor`. If `result` is not passed, then a new tensor is returned, otherwise its storage is
        made to point to storage of `tensor`.
        
        
        ```lua
        x = torch.zeros(4)
        y = torch.Tensor(2,2)
        > x:viewAs(y)
        0 0
        0 0
        [torch.DoubleTensor of dimension 2x2]
        ```
        
        
        
        """

   'torch.Tensor.transpose':
      'prefix': 'torchTensortranspose'
      'body': """
        <a name="torch.Tensor.transpose"></a>
        ### [Tensor] transpose(dim1, dim2) ###
        
        Returns a tensor where dimensions `dim1` and `dim2` have been swapped. For 2D tensors,
        the convenience method of [t()](#torch.Tensor.t) is available.
        ```lua
        x = torch.Tensor(3,4):zero()
        x:select(2,3):fill(7) -- fill column 3 with 7
        > x
        0  0  7  0
        0  0  7  0
        0  0  7  0
        [torch.DoubleTensor of dimension 3x4]
        
        y = x:transpose(1,2) -- swap dimension 1 and 2
        > y
        0  0  0
        0  0  0
        7  7  7
        0  0  0
        [torch.DoubleTensor of dimension 4x3]
        
        y:select(2, 3):fill(8) -- fill column 3 with 8
        > y
        0  0  8
        0  0  8
        7  7  8
        0  0  8
        [torch.DoubleTensor of dimension 4x3]
        
        > x -- contents of x have changed as well
        0  0  7  0
        0  0  7  0
        8  8  8  8
        [torch.DoubleTensor of dimension 3x4]
        ```
        
        
        
        """

   'torch.Tensor.t':
      'prefix': 'torchTensort'
      'body': """
        <a name="torch.Tensor.t"></a>
        ### [Tensor] t() ###
        
        Convenience method of [transpose()](#torch.Tensor.transpose) for 2D
        tensors. The given tensor must be 2 dimensional. Swap dimensions 1 and 2.
        ```lua
        x = torch.Tensor(3,4):zero()
        x:select(2,3):fill(7)
        y = x:t()
        > y
        0  0  0
        0  0  0
        7  7  7
        0  0  0
        [torch.DoubleTensor of dimension 4x3]
        
        > x
        0  0  7  0
        0  0  7  0
        0  0  7  0
        [torch.DoubleTensor of dimension 3x4]
        ```
        
        
        
        """

   'torch.Tensor.permute':
      'prefix': 'torchTensorpermute'
      'body': """
        <a name="torch.Tensor.permute"></a>
        ### [Tensor] permute(dim1, dim2, ..., dimn) ###
        
        Generalizes the function [transpose()](#torch.Tensor.transpose) and can be used
        as a convenience method replacing a sequence of transpose() calls.
        Returns a tensor where the dimensions were permuted according to the permutation
        given by (dim1, dim2, ... , dimn). The permutation must be specified fully, i.e.
        there must be as many parameters as the tensor has dimensions.
        ```lua
        x = torch.Tensor(3,4,2,5)
        > x:size()
        3
        4
        2
        5
        [torch.LongStorage of size 4]
        
        y = x:permute(2,3,1,4) -- equivalent to y = x:transpose(1,3):transpose(1,2)
        > y:size()
        4
        2
        3
        5
        [torch.LongStorage of size 4]
        
        ```
        
        
        
        """

   'torch.Tensor.unfold':
      'prefix': 'torchTensorunfold'
      'body': """
        <a name="torch.Tensor.unfold"></a>
        ### [Tensor] unfold(dim, size, step) ###
        
        Returns a tensor which contains all slices of size `size` in the dimension `dim`. Step between
        two slices is given by `step`.
        
        If `sizedim` is the original size of dimension `dim`, the size of dimension
        `dim` in the returned tensor will be `(sizedim - size) / step + 1`
        
        An additional dimension of size `size` is appended in the returned tensor.
        
        ```lua
        x = torch.Tensor(7)
        for i=1,7 do x[i] = i end
        > x
        1
        2
        3
        4
        5
        6
        7
        [torch.DoubleTensor of dimension 7]
        
        > x:unfold(1, 2, 1)
        1  2
        2  3
        3  4
        4  5
        5  6
        6  7
        [torch.DoubleTensor of dimension 6x2]
        
        > x:unfold(1, 2, 2)
        1  2
        3  4
        5  6
        [torch.DoubleTensor of dimension 3x2]
        ```
        
        ## Applying a function to a tensor ##
        
        These functions apply a function to each element of the tensor on which called the
        method (self). These methods are much faster than using a `for`
        loop in `Lua`. The results is stored in `self` (if the function returns
        something).
        
        
        """

   'torch.Tensor.apply':
      'prefix': 'torchTensorapply'
      'body': """
        <a name="torch.Tensor.apply"></a>
        ### [self] apply(function) ###
        
        Apply the given function to all elements of self.
        
        The function takes a number (the current element of the tensor) and might return
        a number, in which case it will be stored in self.
        
        Examples:
        ```lua
        i = 0
        z = torch.Tensor(3,3)
        z:apply(function(x)
        i = i + 1
        return i
        end) -- fill up the tensor
        > z
        1  2  3
        4  5  6
        7  8  9
        [torch.DoubleTensor of dimension 3x3]
        
        z:apply(math.sin) -- apply the sin function
        > z
        0.8415  0.9093  0.1411
        -0.7568 -0.9589 -0.2794
        0.6570  0.9894  0.4121
        [torch.DoubleTensor of dimension 3x3]
        
        sum = 0
        z:apply(function(x)
        sum = sum + x
        end) -- compute the sum of the elements
        > sum
        1.9552094821074
        
        > z:sum() -- it is indeed correct!
        1.9552094821074
        ```
        
        
        """

   'torch.Tensor.map':
      'prefix': 'torchTensormap'
      'body': """
        <a name="torch.Tensor.map"></a>
        ### [self] map(tensor, function(xs, xt)) ###
        
        Apply the given function to all elements of self and `tensor`. The number of elements of both tensors
        must match, but sizes do not matter.
        
        The function takes two numbers (the current element of self and `tensor`) and might return
        a number, in which case it will be stored in self.
        
        Example:
        ```lua
        x = torch.Tensor(3,3)
        y = torch.Tensor(9)
        i = 0
        x:apply(function() i = i + 1; return i end) -- fill-up x
        i = 0
        y:apply(function() i = i + 1; return i end) -- fill-up y
        > x
        1  2  3
        4  5  6
        7  8  9
        [torch.DoubleTensor of dimension 3x3]
        
        > y
        1
        2
        3
        4
        5
        6
        7
        8
        9
        [torch.DoubleTensor of dimension 9]
        
        x:map(y, function(xx, yy) return xx*yy end) -- element-wise multiplication
        > x
        1   4   9
        16  25  36
        49  64  81
        [torch.DoubleTensor of dimension 3x3]
        ```
        
        
        """

   'torch.Tensor.map2':
      'prefix': 'torchTensormap2'
      'body': """
        <a name="torch.Tensor.map2"></a>
        ### [self] map2(tensor1, tensor2, function(x, xt1, xt2)) ###
        
        Apply the given function to all elements of self, `tensor1` and `tensor2`. The number of elements of all tensors
        must match, but sizes do not matter.
        
        The function takes three numbers (the current element of self, `tensor1` and `tensor2`) and might return
        a number, in which case it will be stored in self.
        
        Example:
        ```lua
        x = torch.Tensor(3,3)
        y = torch.Tensor(9)
        z = torch.Tensor(3,3)
        
        i = 0; x:apply(function() i = i + 1; return math.cos(i)*math.cos(i) end)
        i = 0; y:apply(function() i = i + 1; return i end)
        i = 0; z:apply(function() i = i + 1; return i end)
        
        > x
        0.2919  0.1732  0.9801
        0.4272  0.0805  0.9219
        0.5684  0.0212  0.8302
        [torch.DoubleTensor of dimension 3x3]
        
        > y
        1
        2
        3
        4
        5
        6
        7
        8
        9
        [torch.DoubleTensor of dimension 9]
        
        > z
        1  2  3
        4  5  6
        7  8  9
        [torch.DoubleTensor of dimension 3x3]
        
        x:map2(y, z, function(xx, yy, zz) return xx+yy*zz end)
        > x
        1.2919   4.1732   9.9801
        16.4272  25.0805  36.9219
        49.5684  64.0212  81.8302
        [torch.DoubleTensor of dimension 3x3]
        ```
        
        
        ## Dividing a tensor into a table of tensors ##
        
        These functions divide a Tensor into a table of Tensors.
        
        
        """

   'torch.split':
      'prefix': 'torchsplit'
      'body': """
        <a name="torch.split"></a>
        ### [result] split([result,] tensor, size, [dim]) ###
        
        Splits Tensor `tensor` along dimension `dim`
        into a `result` table of Tensors of size `size` (a number)
        or less (in the case of the last Tensor). The sizes of the non-`dim`
        dimensions remain unchanged. Internally, a series of
        [narrows](#torch.Tensor.narrow) are performed along
        dimensions `dim`. Argument `dim` defaults to 1.
        
        If `result` is not passed, then a new table is returned, otherwise it
        is emptied and reused.
        
        Example:
        ```lua
        x = torch.randn(3,4,5)
        
        > x:split(2,1)
        {
        1 : DoubleTensor - size: 2x4x5
        2 : DoubleTensor - size: 1x4x5
        }
        
        > x:split(3,2)
        {
        1 : DoubleTensor - size: 3x3x5
        2 : DoubleTensor - size: 3x1x5
        }
        
        > x:split(2,3)
        {
        1 : DoubleTensor - size: 3x4x2
        2 : DoubleTensor - size: 3x4x2
        3 : DoubleTensor - size: 3x4x1
        }
        ```
        
        
        
        """

   'torch.chunk':
      'prefix': 'torchchunk'
      'body': """
        <a name="torch.chunk"></a>
        ### [result] chunk([result,] tensor, n, [dim]) ###
        
        Splits Tensor `tensor` into `n` chunks of approximately equal size along
        dimensions `dim` and returns these as a `result` table of Tensors.
        Argument `dim` defaults to 1.
        
        This function uses [split](#torch.split) internally:
        ```lua
        torch.split(result, tensor, math.ceil(tensor:size(dim)/n), dim)
        ```
        
        Example:
        ```lua
        x = torch.randn(3,4,5)
        
        > x:chunk(2,1)
        {
        1 : DoubleTensor - size: 2x4x5
        2 : DoubleTensor - size: 1x4x5
        }
        
        > x:chunk(2,2)
        {
        1 : DoubleTensor - size: 3x2x5
        2 : DoubleTensor - size: 3x2x5
        }
        
        > x:chunk(2,3)
        {
        1 : DoubleTensor - size: 3x4x3
        2 : DoubleTensor - size: 3x4x2
        }
        ```
        
        ## LuaJIT FFI access ##
        These functions expose Torch's Tensor and Storage data structures, through
        [LuaJIT FFI](http://luajit.org/ext_ffi_api.html).
        This allows extremely fast access to Tensors and Storages, all from Lua.
        
        
        """

   'torch.data':
      'prefix': 'torchdata'
      'body': """
        <a name="torch.data"></a>
        ### [result] data(tensor, [asnumber]) ###
        
        Returns a LuaJIT FFI pointer to the raw data of the tensor.
        If `asnumber` is true, then returns the pointer as a `intptr_t` cdata
        that you can transform to a plain lua number with `tonumber()`.
        
        Accessing the raw data of a Tensor like this is extremely efficient, in fact, it's
        almost as fast as C in lots of cases.
        
        Example:
        ```lua
        t = torch.randn(3,2)
        > t
        0.8008 -0.6103
        0.6473 -0.1870
        -0.0023 -0.4902
        [torch.DoubleTensor of dimension 3x2]
        
        t_data = torch.data(t)
        for i = 0,t:nElement()-1 do t_data[i] = 0 end
        > t
        0 0
        0 0
        0 0
        [torch.DoubleTensor of dimension 3x2]
        ```
        
        WARNING: bear in mind that accessing the raw data like this is dangerous, and should
        only be done on contiguous tensors (if a tensor is not contiguous, then you have to
        use its size and stride information). Making sure a tensor is contiguous is easy:
        ```lua
        t = torch.randn(3,2)
        t_noncontiguous = t:transpose(1,2)
        
        -- it would be unsafe to work with torch.data(t_noncontiguous)
        t_transposed_and_contiguous = t_noncontiguous:contiguous()
        
        -- it is now safe to work with the raw pointer
        data = torch.data(t_transposed_and_contiguous)
        ```
        
        Last, the pointer can be returned as a plain `intptr_t` cdata. This can be useful
        to share pointers between threads (warning: this is dangerous, as the second
        tensor doesn't increment the reference counter on the storage. If the first tensor
        gets freed, then the data of the second tensor becomes a dangling pointer):
        
        ```lua
        t = torch.randn(10)
        p = tonumber(torch.data(t,true))
        s = torch.Storage(10, p)
        tt = torch.Tensor(s)
        -- tt and t are a view on the same data.
        ```
        
        
        """

   'torch.cdata':
      'prefix': 'torchcdata'
      'body': """
        <a name="torch.cdata"></a>
        ### [result] cdata(tensor, [asnumber]) ###
        
        Returns a LuaJIT FFI pointer to the C structure of the tensor.
        Use this with caution, and look at [FFI.lua](https://github.com/torch/torch7/blob/master/FFI.lua)
        for the members of the tensor
        
        ## Reference counting ##
        
        Tensors are reference-counted. It means that each time an object (C or the
        Lua state) need to keep a reference over a tensor, the corresponding
        tensor reference counter will be [increased](#torch.Tensor.retain). The
        reference counter is [decreased]((#torch.Tensor.free)) when the object
        does not need the tensor anymore.
        
        These methods should be used with extreme care. In general, they should
        never be called, except if you know what you are doing, as the handling of
        references is done automatically. They can be useful in threaded
        environments. Note that these methods are atomic operations.
        
        
        """

   'torch.Tensor.retain':
      'prefix': 'torchTensorretain'
      'body': """
        <a name="torch.Tensor.retain"></a>
        ### retain() ###
        
        Increment the reference counter of the tensor.
        
        
        """

   'torch.Tensor.free':
      'prefix': 'torchTensorfree'
      'body': """
        <a name="torch.Tensor.free"></a>
        ### free() ###
        
        Decrement the reference counter of the tensor. Free the tensor if the
        counter is at 0.
        
        """

   'torch.Tester.dok':
      'prefix': 'torchTesterdok'
      'body': """
        <a name="torch.Tester.dok"></a>
        # Tester #
        
        This class provides a generic unit testing framework. It is already 
        being used in [nn](../index.md) package to verify the correctness of classes.
        
        The framework is generally used as follows.
        
        ```lua
        mytest = {}
        
        tester = torch.Tester()
        
        function mytest.TestA()
        local a = 10
        local b = 10
        tester:asserteq(a,b,'a == b')
        tester:assertne(a,b,'a ~= b')
        end
        
        function mytest.TestB()
        local a = 10
        local b = 9
        tester:assertlt(a,b,'a < b')
        tester:assertgt(a,b,'a > b')
        end
        
        tester:add(mytest)
        tester:run()
        
        ```
        
        Running this code will report 2 errors in 2 test functions. Generally it is 
        better to put a single test case in each test function unless several very related
        test cases exist. The error report includes the message and line number of the error.
        
        ```
        
        Running 2 tests
        **  ==> Done 
        
        Completed 2 tests with 2 errors
        
        --------------------------------------------------------------------------------
        TestB
        a < b
        LT(<) violation   val=10, condition=9
        ...y/usr.t7/local.master/share/lua/5.1/torch/Tester.lua:23: in function 'assertlt'
        [string "function mytest.TestB()..."]:4: in function 'f'
        
        --------------------------------------------------------------------------------
        TestA
        a ~= b
        NE(~=) violation   val=10, condition=10
        ...y/usr.t7/local.master/share/lua/5.1/torch/Tester.lua:38: in function 'assertne'
        [string "function mytest.TestA()..."]:5: in function 'f'
        
        --------------------------------------------------------------------------------
        
        ```
        
        
        
        """

   'torch.Tester':
      'prefix': 'torchTester'
      'body': """
        <a name="torch.Tester"></a>
        ### torch.Tester() ###
        
        Returns a new instance of `torch.Tester` class.
        
        
        """

   'torch.Tester.add':
      'prefix': 'torchTesteradd'
      'body': """
        <a name="torch.Tester.add"></a>
        ### add(f, 'name') ###
        
        Adds a new test function with name `name`. The test function is stored in `f`.
        The function is supposed to run without any arguments and not return any values.
        
        
        """

   'torch.Tester.add':
      'prefix': 'torchTesteradd'
      'body': """
        <a name="torch.Tester.add"></a>
        ### add(ftable) ###
        
        Recursively adds all function entries of the table `ftable` as tests. This table 
        can only have functions or nested tables of functions.
        
        
        """

   'torch.Tester.assert':
      'prefix': 'torchTesterassert'
      'body': """
        <a name="torch.Tester.assert"></a>
        ### assert(condition [, message]) ###
        
        Saves an error if condition is not true with the optional message.
        
        
        """

   'torch.Tester.assertlt':
      'prefix': 'torchTesterassertlt'
      'body': """
        <a name="torch.Tester.assertlt"></a>
        ### assertlt(val, condition [, message]) ###
        
        Saves an error if `val < condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertgt':
      'prefix': 'torchTesterassertgt'
      'body': """
        <a name="torch.Tester.assertgt"></a>
        ### assertgt(val, condition [, message]) ###
        
        Saves an error if `val > condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertle':
      'prefix': 'torchTesterassertle'
      'body': """
        <a name="torch.Tester.assertle"></a>
        ### assertle(val, condition [, message]) ###
        
        Saves an error if `val <= condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertge':
      'prefix': 'torchTesterassertge'
      'body': """
        <a name="torch.Tester.assertge"></a>
        ### assertge(val, condition [, message]) ###
        
        Saves an error if `val >= condition` is not true with the optional message.
        
        
        """

   'torch.Tester.asserteq':
      'prefix': 'torchTesterasserteq'
      'body': """
        <a name="torch.Tester.asserteq"></a>
        ### asserteq(val, condition [, message]) ###
        
        Saves an error if `val == condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertne':
      'prefix': 'torchTesterassertne'
      'body': """
        <a name="torch.Tester.assertne"></a>
        ### assertne(val, condition [, message]) ###
        
        Saves an error if `val ~= condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertTensorEq':
      'prefix': 'torchTesterassertTensorEq'
      'body': """
        <a name="torch.Tester.assertTensorEq"></a>
        ### assertTensorEq(ta, tb, condition [, message]) ###
        
        Saves an error if `max(abs(ta-tb)) < condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertTensorNe':
      'prefix': 'torchTesterassertTensorNe'
      'body': """
        <a name="torch.Tester.assertTensorNe"></a>
        ### assertTensorNe(ta, tb, condition [, message]) ###
        
        Saves an error if `max(abs(ta-tb)) >= condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertTableEq':
      'prefix': 'torchTesterassertTableEq'
      'body': """
        <a name="torch.Tester.assertTableEq"></a>
        ### assertTableEq(ta, tb, condition [, message]) ###
        
        Saves an error if `max(abs(ta-tb)) < condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertTableNe':
      'prefix': 'torchTesterassertTableNe'
      'body': """
        <a name="torch.Tester.assertTableNe"></a>
        ### assertTableNe(ta, tb, condition [, message]) ###
        
        Saves an error if `max(abs(ta-tb)) >= condition` is not true with the optional message.
        
        
        """

   'torch.Tester.assertError':
      'prefix': 'torchTesterassertError'
      'body': """
        <a name="torch.Tester.assertError"></a>
        ### assertError(f [, message]) ###
        
        Saves an error if calling the function f() does not return an error, with the optional message.
        
        
        """

   'torch.Tester.run':
      'prefix': 'torchTesterrun'
      'body': """
        <a name="torch.Tester.run"></a>
        ### run() ###
        
        Runs all the test functions that are stored using [add()](#torch.Tester.add) function. 
        While running it reports progress and at the end gives a summary of all errors.
        
        
        
        
        
        
        
        
        """

   'torch.Timer.dok':
      'prefix': 'torchTimerdok'
      'body': """
        <a name="torch.Timer.dok"></a>
        # Timer #
        
        This class is able to measure time (in seconds) elapsed in a particular period. Example:
        ```lua
        timer = torch.Timer() -- the Timer starts to count now
        x = 0
        for i=1,1000000 do
        x = x + math.sin(x)
        end
        print('Time elapsed for 1,000,000 sin: ' .. timer:time().real .. ' seconds')
        ```
        
        
        """

   'torch.Timer':
      'prefix': 'torchTimer'
      'body': """
        <a name="torch.Timer"></a>
        ## Timer Class Constructor and Methods ##
        
        
        """

   'torch.Timer':
      'prefix': 'torchTimer'
      'body': """
        <a name="torch.Timer"></a>
        ### torch.Timer() ###
        
        Returns a new `Timer`. The timer starts to count the time now.
        
        
        """

   'torch.Timer.reset':
      'prefix': 'torchTimerreset'
      'body': """
        <a name="torch.Timer.reset"></a>
        ### [self] reset() ###
        
        Reset the timer accumulated time to `0`. If the timer was running, the timer
        restarts to count the time now. If the timer was stopped, it stays stopped.
        
        
        """

   'torch.Timer.resume':
      'prefix': 'torchTimerresume'
      'body': """
        <a name="torch.Timer.resume"></a>
        ### [self] resume() ###
        
        Resume a stopped timer. The timer restarts to count the time, and addition
        the accumulated time with the time already counted before being stopped.
        
        
        """

   'torch.Timer.stop':
      'prefix': 'torchTimerstop'
      'body': """
        <a name="torch.Timer.stop"></a>
        ### [self] stop() ###
        
        Stop the timer. The accumulated time counted until now is stored.
        
        
        """

   'torch.Timer.time':
      'prefix': 'torchTimertime'
      'body': """
        <a name="torch.Timer.time"></a>
        ### [table] time() ###
        
        Returns a table reporting the accumulated time elapsed until now. Following the UNIX shell `time` command,
        there are three fields in the table:
        * `real`: the wall-clock elapsed time.
        * `user`: the elapsed CPU time. Note that the CPU time of a threaded program sums time spent in all threads.
        * `sys`: the time spent in system usage.
        
        
        """

   'nn.traningneuralnet.dok':
      'prefix': 'nntraningneuralnetdok'
      'body': """
        <a name="nn.traningneuralnet.dok"></a>
        # Training a neural network #
        
        Training a neural network is easy with a [simple `for` loop](#nn.DoItYourself).
        While doing your own loop provides great flexibility, you might
        want sometimes a quick way of training neural
        networks. [StochasticGradient](#nn.StochasticGradient), a simple class
        which does the job for you is provided as standard.
        
        
        """

   'nn.StochasticGradient.dok':
      'prefix': 'nnStochasticGradientdok'
      'body': """
        <a name="nn.StochasticGradient.dok"></a>
        ## StochasticGradient ##
        
        `StochasticGradient` is a high-level class for training [neural networks](#nn.Module), using a stochastic gradient
        algorithm. This class is [serializable](https://github.com/torch/torch7/blob/master/doc/serialization.md#serialization).
        
        
        """

   'nn.StochasticGradient':
      'prefix': 'nnStochasticGradient'
      'body': """
        <a name="nn.StochasticGradient"></a>
        ### StochasticGradient(module, criterion) ###
        
        Create a `StochasticGradient` class, using the given [Module](module.md#nn.Module) and [Criterion](criterion.md#nn.Criterion).
        The class contains [several parameters](#nn.StochasticGradientParameters) you might want to set after initialization.
        
        
        """

   'nn.StochasticGradientTrain':
      'prefix': 'nnStochasticGradientTrain'
      'body': """
        <a name="nn.StochasticGradientTrain"></a>
        ### train(dataset) ###
        
        Train the module and criterion given in the
        [constructor](#nn.StochasticGradient) over `dataset`, using the
        internal [parameters](#nn.StochasticGradientParameters).
        
        StochasticGradient expect as a `dataset` an object which implements the operator
        `dataset[index]` and implements the method `dataset:size()`. The `size()` methods
        returns the number of examples and `dataset[i]` has to return the i-th example.
        
        An `example` has to be an object which implements the operator
        `example[field]`, where `field` might take the value `1` (input features)
        or `2` (corresponding label which will be given to the criterion). 
        The input is usually a Tensor (except if you use special kind of gradient modules,
        like [table layers](table.md#nn.TableLayers)). The label type depends of the criterion.
        For example, the [MSECriterion](criterion.md#nn.MSECriterion) expects a Tensor, but the
        [ClassNLLCriterion](criterion.md#nn.ClassNLLCriterion) except a integer number (the class).
        
        Such a dataset is easily constructed by using Lua tables, but it could any `C` object
        for example, as long as required operators/methods are implemented. 
        [See an example](#nn.DoItStochasticGradient).
        
        
        """

   'nn.StochasticGradientParameters':
      'prefix': 'nnStochasticGradientParameters'
      'body': """
        <a name="nn.StochasticGradientParameters"></a>
        ### Parameters ###
        
        `StochasticGradient` has several field which have an impact on a call to [train()](#nn.StochasticGradientTrain).
        
        * `learningRate`: This is the learning rate used during training. The update of the parameters will be `parameters = parameters - learningRate * parameters_gradient`. Default value is `0.01`.
        * `learningRateDecay`: The learning rate decay. If non-zero, the learning rate (note: the field learningRate will not change value) will be computed after each iteration (pass over the dataset) with: `current_learning_rate =learningRate / (1 + iteration * learningRateDecay)`
        * `maxIteration`: The maximum number of iteration (passes over the dataset). Default is `25`.
        * `shuffleIndices`: Boolean which says if the examples will be randomly sampled or not. Default is `true`. If `false`, the examples will be taken in the order of the dataset.
        * `hookExample`: A possible hook function which will be called (if non-nil) during training after each example forwarded and backwarded through the network. The function takes `(self, example)` as parameters. Default is `nil`.
        * `hookIteration`: A possible hook function which will be called (if non-nil) during training after a complete pass over the dataset. The function takes `(self, iteration, currentError)` as parameters. Default is `nil`.
        
        
        """

   'nn.DoItStochasticGradient':
      'prefix': 'nnDoItStochasticGradient'
      'body': """
        <a name="nn.DoItStochasticGradient"></a>
        ## Example of training using StochasticGradient ##
        
        We show an example here on a classical XOR problem.
        
        __Dataset__
        
        We first need to create a dataset, following the conventions described in
        [StochasticGradient](#nn.StochasticGradientTrain).
        ```lua
        dataset={};
        function dataset:size() return 100 end -- 100 examples
        for i=1,dataset:size() do 
        local input = torch.randn(2);     -- normally distributed example in 2d
        local output = torch.Tensor(1);
        if input[1]*input[2]>0 then     -- calculate label for XOR function
        output[1] = -1;
        else
        output[1] = 1
        end
        dataset[i] = {input, output}
        end
        ```
        
        __Neural Network__
        
        We create a simple neural network with one hidden layer.
        ```lua
        require "nn"
        mlp = nn.Sequential();  -- make a multi-layer perceptron
        inputs = 2; outputs = 1; HUs = 20; -- parameters
        mlp:add(nn.Linear(inputs, HUs))
        mlp:add(nn.Tanh())
        mlp:add(nn.Linear(HUs, outputs))
        ```
        
        __Training__
        
        We choose the Mean Squared Error criterion and train the dataset.
        ```lua
        criterion = nn.MSECriterion()  
        trainer = nn.StochasticGradient(mlp, criterion)
        trainer.learningRate = 0.01
        trainer:train(dataset)
        ```
        
        __Test the network__
        
        ```lua
        x = torch.Tensor(2)
        x[1] =  0.5; x[2] =  0.5; print(mlp:forward(x))
        x[1] =  0.5; x[2] = -0.5; print(mlp:forward(x))
        x[1] = -0.5; x[2] =  0.5; print(mlp:forward(x))
        x[1] = -0.5; x[2] = -0.5; print(mlp:forward(x))
        ```
        
        You should see something like:
        ```lua
        > x = torch.Tensor(2)
        > x[1] =  0.5; x[2] =  0.5; print(mlp:forward(x))
        
        -0.3490
        [torch.Tensor of dimension 1]
        
        > x[1] =  0.5; x[2] = -0.5; print(mlp:forward(x))
        
        1.0561
        [torch.Tensor of dimension 1]
        
        > x[1] = -0.5; x[2] =  0.5; print(mlp:forward(x))
        
        0.8640
        [torch.Tensor of dimension 1]
        
        > x[1] = -0.5; x[2] = -0.5; print(mlp:forward(x))
        
        -0.2941
        [torch.Tensor of dimension 1]
        ```
        
        
        """

   'nn.DoItYourself':
      'prefix': 'nnDoItYourself'
      'body': """
        <a name="nn.DoItYourself"></a>
        ## Example of manual training of a neural network ##
        
        We show an example here on a classical XOR problem.
        
        __Neural Network__
        
        We create a simple neural network with one hidden layer.
        ```lua
        require "nn"
        mlp = nn.Sequential();  -- make a multi-layer perceptron
        inputs = 2; outputs = 1; HUs = 20; -- parameters
        mlp:add(nn.Linear(inputs, HUs))
        mlp:add(nn.Tanh())
        mlp:add(nn.Linear(HUs, outputs))
        ```
        
        __Loss function__
        
        We choose the Mean Squared Error criterion.
        ```lua
        criterion = nn.MSECriterion()  
        ```
        
        __Training__
        
        We create data _on the fly_ and feed it to the neural network.
        
        ```lua
        for i = 1,2500 do
        -- random sample
        local input= torch.randn(2);     -- normally distributed example in 2d
        local output= torch.Tensor(1);
        if input[1]*input[2] > 0 then  -- calculate label for XOR function
        output[1] = -1
        else
        output[1] = 1
        end
        
        -- feed it to the neural network and the criterion
        criterion:forward(mlp:forward(input), output)
        
        -- train over this example in 3 steps
        -- (1) zero the accumulation of the gradients
        mlp:zeroGradParameters()
        -- (2) accumulate gradients
        mlp:backward(input, criterion:backward(mlp.output, output))
        -- (3) update parameters with a 0.01 learning rate
        mlp:updateParameters(0.01)
        end
        ```
        
        __Test the network__
        
        ```lua
        x = torch.Tensor(2)
        x[1] =  0.5; x[2] =  0.5; print(mlp:forward(x))
        x[1] =  0.5; x[2] = -0.5; print(mlp:forward(x))
        x[1] = -0.5; x[2] =  0.5; print(mlp:forward(x))
        x[1] = -0.5; x[2] = -0.5; print(mlp:forward(x))
        ```
        
        You should see something like:
        ```lua
        > x = torch.Tensor(2)
        > x[1] =  0.5; x[2] =  0.5; print(mlp:forward(x))
        
        -0.6140
        [torch.Tensor of dimension 1]
        
        > x[1] =  0.5; x[2] = -0.5; print(mlp:forward(x))
        
        0.8878
        [torch.Tensor of dimension 1]
        
        > x[1] = -0.5; x[2] =  0.5; print(mlp:forward(x))
        
        0.8548
        [torch.Tensor of dimension 1]
        
        > x[1] = -0.5; x[2] = -0.5; print(mlp:forward(x))
        
        -0.5498
        [torch.Tensor of dimension 1]
        ```
        
        """

   'nn.transfer.dok':
      'prefix': 'nntransferdok'
      'body': """
        <a name="nn.transfer.dok"></a>
        # Transfer Function Layers #
        Transfer functions are normally used to introduce a non-linearity after a parameterized layer like [Linear](simple.md#nn.Linear) and  [SpatialConvolution](convolution.md#nn.SpatialConvolution). Non-linearities allows for dividing the problem space into more complex regions than what a simple logistic regressor would permit.
        
        
        """

   'nn.HardTanh':
      'prefix': 'nnHardTanh'
      'body': """
        <a name="nn.HardTanh"></a>
        ## HardTanh ##
        
        Applies the `HardTanh` function element-wise to the input Tensor,
        thus outputting a Tensor of the same dimension.
        
        `HardTanh` is defined as:
        
        * `f(x)` = `1, if x >`  `1,`
        * `f(x)` = `-1, if x <`  `-1,`
        * `f(x)` = `x,` `otherwise.`
        
        The range of the linear region `[-1 1]` can be adjusted by specifying arguments in declaration, for example `nn.HardTanh(min_value, max_value)`.
        Otherwise, `[min_value max_value]` is set to `[-1 1]` by default.
        
        
        ```lua
        ii=torch.linspace(-2,2)
        m=nn.HardTanh()
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/htanh.png)
        
        
        
        """

   'nn.HardShrink':
      'prefix': 'nnHardShrink'
      'body': """
        <a name="nn.HardShrink"></a>
        ## HardShrink ##
        
        `module = nn.HardShrink(lambda)`
        
        Applies the hard shrinkage function element-wise to the input
        [Tensor](https://github.com/torch/torch7/blob/master/doc/tensor.md). The output is the same size as the input.
        
        `HardShrinkage` operator is defined as:
        
        * `f(x) = x, if x > lambda`
        * `f(x) = x, if x < -lambda`
        * `f(x) = 0, otherwise`
        
        ```lua
        ii=torch.linspace(-2,2)
        m=nn.HardShrink(0.85)
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/hshrink.png)
        
        
        """

   'nn.SoftShrink':
      'prefix': 'nnSoftShrink'
      'body': """
        <a name="nn.SoftShrink"></a>
        ## SoftShrink ##
        
        `module = nn.SoftShrink(lambda)`
        
        Applies the soft shrinkage function element-wise to the input
        [Tensor](https://github.com/torch/torch7/blob/master/doc/tensor.md). The output is the same size as the input.
        
        `SoftShrinkage` operator is defined as:
        
        * `f(x) = x-lambda, if x > lambda`
        * `f(x) = x+lambda, if x < -lambda`
        * `f(x) = 0, otherwise`
        
        ```lua
        ii=torch.linspace(-2,2)
        m=nn.SoftShrink(0.85)
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/sshrink.png)
        
        
        
        """

   'nn.SoftMax':
      'prefix': 'nnSoftMax'
      'body': """
        <a name="nn.SoftMax"></a>
        ## SoftMax ##
        
        Applies the `Softmax` function to an n-dimensional input Tensor,
        rescaling them so that the elements of the n-dimensional output Tensor
        lie in the range (0,1) and sum to 1.
        
        `Softmax` is defined as `f_i(x)` = `exp(x_i-shift) / sum_j exp(x_j-shift)`,
        where `shift` = `max_i x_i`.
        
        
        ```lua
        ii=torch.exp(torch.abs(torch.randn(10)))
        m=nn.SoftMax()
        oo=m:forward(ii)
        gnuplot.plot({'Input',ii,'+-'},{'Output',oo,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/softmax.png)
        
        Note that this module doesn't work directly with [ClassNLLCriterion](criterion.md#nn.ClassNLLCriterion), which expects the `nn.Log` to be computed between the `SoftMax` and itself. Use [LogSoftMax](#nn.LogSoftMax) instead (it's faster).
        
        
        """

   'nn.SoftMin':
      'prefix': 'nnSoftMin'
      'body': """
        <a name="nn.SoftMin"></a>
        ## SoftMin ##
        
        Applies the `Softmin` function to an n-dimensional input Tensor,
        rescaling them so that the elements of the n-dimensional output Tensor
        lie in the range (0,1) and sum to 1.
        
        `Softmin` is defined as `f_i(x)` = `exp(-x_i-shift) / sum_j exp(-x_j-shift)`,
        where `shift` = `max_i -x_i`.
        
        
        ```lua
        ii=torch.exp(torch.abs(torch.randn(10)))
        m=nn.SoftMin()
        oo=m:forward(ii)
        gnuplot.plot({'Input',ii,'+-'},{'Output',oo,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/softmin.png)
        
        
        """

   'nn.SoftPlus':
      'prefix': 'nnSoftPlus'
      'body': """
        <a name="nn.SoftPlus"></a>
        ### SoftPlus ###
        
        Applies the `SoftPlus` function to an n-dimensioanl input Tensor.
        `SoftPlus` is a smooth approximation to the [ReLU](#nn.ReLU) function and can be used to constrain the output of a machine to always be positive. For numerical stability the implementation reverts to the linear function for inputs above a certain value (20 by default).
        
        `SoftPlus` is defined as `f_i(x)` = `1/beta * log(1 + exp(beta * x_i))`.
        
        ```lua
        ii=torch.linspace(-3,3)
        m=nn.SoftPlus()
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/softplus.png)
        
        
        """

   'nn.SoftSign':
      'prefix': 'nnSoftSign'
      'body': """
        <a name="nn.SoftSign"></a>
        ## SoftSign ##
        
        Applies the `SoftSign` function to an n-dimensioanl input Tensor.
        
        `SoftSign` is defined as `f_i(x) = x_i / (1+|x_i|)`
        
        ```lua
        ii=torch.linspace(-5,5)
        m=nn.SoftSign()
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/softsign.png)
        
        
        """

   'nn.LogSigmoid':
      'prefix': 'nnLogSigmoid'
      'body': """
        <a name="nn.LogSigmoid"></a>
        ## LogSigmoid ##
        
        Applies the `LogSigmoid` function to an n-dimensional input Tensor.
        
        `LogSigmoid` is defined as `f_i(x)` = `log(1/(1+ exp(-x_i)))`.
        
        
        ```lua
        ii=torch.randn(10)
        m=nn.LogSigmoid()
        oo=m:forward(ii)
        go=torch.ones(10)
        gi=m:backward(ii,go)
        gnuplot.plot({'Input',ii,'+-'},{'Output',oo,'+-'},{'gradInput',gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/logsigmoid.png)
        
        
        
        """

   'nn.LogSoftMax':
      'prefix': 'nnLogSoftMax'
      'body': """
        <a name="nn.LogSoftMax"></a>
        ## LogSoftMax ##
        
        Applies the `LogSoftmax` function to an n-dimensional input Tensor.
        
        `LogSoftmax` is defined as `f_i(x)` = `log(1/a exp(x_i))`,
        where  `a` = `sum_j exp(x_j)`.
        
        ```lua
        ii=torch.randn(10)
        m=nn.LogSoftMax()
        oo=m:forward(ii)
        go=torch.ones(10)
        gi=m:backward(ii,go)
        gnuplot.plot({'Input',ii,'+-'},{'Output',oo,'+-'},{'gradInput',gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/logsoftmax.png)
        
        
        """

   'nn.Sigmoid':
      'prefix': 'nnSigmoid'
      'body': """
        <a name="nn.Sigmoid"></a>
        ## Sigmoid ##
        
        Applies the `Sigmoid` function element-wise to the input Tensor,
        thus outputting a Tensor of the same dimension.
        
        `Sigmoid` is defined as `f(x)` = `1/(1+exp(-x))`.
        
        ```lua
        ii=torch.linspace(-5,5)
        m=nn.Sigmoid()
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/sigmoid.png)
        
        
        """

   'nn.Tanh':
      'prefix': 'nnTanh'
      'body': """
        <a name="nn.Tanh"></a>
        ## Tanh ##
        
        Applies the `Tanh` function element-wise to the input Tensor,
        thus outputting a Tensor of the same dimension.
        
        `Tanh` is defined as `f(x)` = `(exp(x)-exp(-x))/(exp(x)+exp(-x))`.
        
        ```lua
        ii=torch.linspace(-3,3)
        m=nn.Tanh()
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/tanh.png)
        
        
        """

   'nn.ReLU':
      'prefix': 'nnReLU'
      'body': """
        <a name="nn.ReLU"></a>
        ## ReLU ##
        
        Applies the rectified linear unit (`ReLU`) function element-wise to the input Tensor,
        thus outputting a Tensor of the same dimension.
        
        `ReLU` is defined as `f(x)` = `max(0,x)`
        
        Can optionally do its operation in-place without using extra state memory:
        ```lua
        m=nn.ReLU(true) -- true = in-place, false = keeping separate state.
        ```
        
        ```lua
        ii=torch.linspace(-3,3)
        m=nn.ReLU()
        oo=m:forward(ii)
        go=torch.ones(100)
        gi=m:backward(ii,go)
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/relu.png)
        
        
        """

   'nn.PReLU':
      'prefix': 'nnPReLU'
      'body': """
        <a name="nn.PReLU"></a>
        ## PReLU ##
        
        Applies parametric ReLU, which parameter varies the slope of the negative part:
        
        `PReLU` is defined as `f(x)` = `max(0,x) + a * min(0,x)`
        
        When called without a number on input as ```nn.PReLU()``` uses shared version, meaning
        has only one parameter. Otherwise if called ```nn.PReLU(nOutputPlane)``` has ```nOutputPlane```
        parameters, one for each input map. The output dimension is always equal to input dimension.
        Note that weight decay should not be used on it. For reference see [Delving Deep into Rectifiers](http://arxiv.org/abs/1502.01852).
        
        ![](image/prelu.png)
        
        
        """

   'nn.RReLU':
      'prefix': 'nnRReLU'
      'body': """
        <a name="nn.RReLU"></a>
        ## RReLU ##
        
        Applies the randomized leaky rectified linear unit (RReLU) element-wise to the input tensor, thus outputting a tensor of the same dimension. Informally the RReLU is also known as 'insanity' layer.
        
        `RReLU` is defined as `f(x)` = `max(0,x) + a * min(0,x)`, where `a` ~ `U(l,u)`.
        
        In training mode negative inputs are multiplied by a factor `a` drawn from a uniform random distribution `U(l, u)`. In evaluation mode a RReLU behaves like a LeakyReLU with a constant mean factor `a` = `(l+u)/2`.
        
        Syntax:
        ```lua
        m=nn.ReLU(
        l,       -- minimum factor for negative inputs, default: 1/8;
        u,       -- maximum factor for negative inputs, default: 1/3;
        inplace  -- if true the result will be written to the input tensor, default: false;
        )
        ```
        If `l == u` a RReLU effectively becomes a LeakyReLU. Regardless of operating in in-place mode a RReLU will internally allocate an input-sized `noise` tensor to store random factors for negative inputs. The backward() operation assumes that forward() has been called before.
        
        For reference see [Empirical Evaluation of Rectified Activations in Convolutional Network](http://arxiv.org/abs/1505.00853).
        ```lua
        ii=torch.linspace(-3, 3)
        m=nn.RReLU()
        oo=m:forward(ii):clone()
        gi=m:backward(ii,torch.ones(100))
        gnuplot.plot({'f(x)',ii,oo,'+-'},{'df/dx',ii,gi,'+-'})
        gnuplot.grid(true)
        ```
        ![](image/rrelu.png)
        
        
        """

   'nn.ELU':
      'prefix': 'nnELU'
      'body': """
        <a name="nn.ELU"></a>
        ## ELU ##
        
        Applies exponential linear unit (ELU), which parameter a varies the convergence value of the exponential function below zero:
        
        `ELU` is defined as `f(x)` = `max(0,x) + min(0,a*(exp(x)-1))`
        
        It is called with the parameter a as ```nn.ELU(a)``` with the default value `a=1`. The output dimension is always equal to input dimension.
        
        For reference see [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](http://arxiv.org/abs/1511.07289).
        ```lua
        require 'nn'
        require 'gnuplot'
        
        xs = torch.linspace(-3,3,200)
        go = torch.ones(xs:size(1))
        function f(a) return nn.ELU(a):forward(xs) end
        function df(a) local m = nn.ELU(a) m:forward(xs) return m:backward(xs, go) end
        
        gnuplot.plot({'fw ELU, alpha=0.1', xs,  f(0.1), '-'},
        {'fw ELU, alpha=1.0', xs,  f(1.0), '-'},
        {'bw ELU, alpha=0.1', xs, df(0.1), '-'},
        {'bw ELU, alpha=1.0', xs, df(1.0), '-'})
        gnuplot.grid(true)
        ```
        ![](image/elu.png)
        
        
        """

   'nn.LeakyReLU':
      'prefix': 'nnLeakyReLU'
      'body': """
        <a name="nn.LeakyReLU"></a>
        ## LeakyReLU ##
        
        Applies Leaky ReLU, which parameter `a` sets the slope of the negative part:
        
        `LeakyReLU` is defined as `f(x)` = `max(0,x) + a * min(0,x)`
        
        Can optionally do its operation in-place without using extra state memory:
        
        ```lua
        m=nn.LeakyReLU(a,true) -- true = in-place, false = keeping separate state.
        ```
        
        
        """

   'nn.SpatialSoftMax':
      'prefix': 'nnSpatialSoftMax'
      'body': """
        <a name="nn.SpatialSoftMax"></a>
        ## SpatialSoftMax ##
        
        Applies [SoftMax](#nn.SoftMax) over features to each spatial location (height x width of planes).
        The module accepts 1D (vector), 2D (batch of vectors), 3D (vectors in space) or 4D (batch of vectors in space) tensor as input.
        Functionally it is equivalent to [SoftMax](#nn.SoftMax) when 1D or 2D input is used.
        The output dimension is always the same as input dimension.
        
        ```lua
        ii=torch.randn(4,8,16,16)  -- batchSize x features x height x width
        m=nn.SpatialSoftMax()
        oo = m:forward(ii)
        ```
        
        
        """

   'nn.AddConstant':
      'prefix': 'nnAddConstant'
      'body': """
        <a name="nn.AddConstant"></a>
        ## AddConstant ##
        
        Adds a (non-learnable) scalar constant.  This module is sometimes useful for debugging purposes:  `f(x)` = `x + k`, where `k` is a scalar.
        
        Can optionally do its operation in-place without using extra state memory:
        ```lua
        m=nn.AddConstant(k,true) -- true = in-place, false = keeping separate state.
        ```
        In-place mode restores the original input value after the backward pass, allowing its use after other in-place modules, like [MulConstant](#nn.MulConstant).
        
        
        """

   'nn.MulConstant':
      'prefix': 'nnMulConstant'
      'body': """
        <a name="nn.MulConstant"></a>
        ## MulConstant ##
        
        Multiplies input tensor by a (non-learnable) scalar constant.  This module is sometimes useful for debugging purposes:  `f(x)` = `k * x`, where `k` is a scalar.
        
        Can optionally do its operation in-place without using extra state memory:
        ```lua
        m=nn.MulConstant(k,true) -- true = in-place, false = keeping separate state.
        ```
        In-place mode restores the original input value after the backward pass, allowing its use after other in-place modules, like [AddConstant](#nn.AddConstant).
        
        """

   'torch.utility.dok':
      'prefix': 'torchutilitydok'
      'body': """
        <a name="torch.utility.dok"></a>
        # Torch utility functions #
        
        These functions are used in all Torch package for creating and handling classes.
        The most interesting function is probably [`torch.class()`](#torch.class) which allows
        the user to create easily new classes. [`torch.typename()`](#torch.typename) might
        also be interesting to check what is the class of a given *Torch7* object.
        
        The other functions are for more advanced users.
        
        
        
        """

   'torch.class':
      'prefix': 'torchclass'
      'body': """
        <a name="torch.class"></a>
        ### [metatable] torch.class(name, [parentName], [module]) ###
        
        Creates a new `Torch` class called `name`. If `parentName` is provided, the class will inherit
        `parentName` methods. A class is a table which has a particular metatable.
        
        If `module` is not provided and if `name` is of the form
        `package.className` then the class `className` will be added to the
        specified `package`. In that case, `package` has to be a valid (and
        already loaded) package. If `name` does not contain any `.`, then the class
        will be defined in the global environment.
        
        If `module` is provided table, the class will be defined in this table at
        key `className`.
        
        One \[or two\] (meta)tables are returned. These tables contain all the method
        provided by the class [and its parent class if it has been provided]. After
        a call to `torch.class()` you have to fill-up properly the metatable.
        
        After the class definition is complete, constructing a new class `name` will be achieved by a call to `name()`.
        This call will first call the method ```lua__init()``` if it exists, passing all arguments of `name()`.
        
        ```lua
        -- for naming convenience
        do
        --- creates a class "Foo"
        local Foo = torch.class('Foo')
        
        --- the initializer
        function Foo:__init()
        self.contents = 'this is some text'
        end
        
        --- a method
        function Foo:print()
        print(self.contents)
        end
        
        --- another one
        function Foo:bip()
        print('bip')
        end
        
        end
        
        --- now create an instance of Foo
        foo = Foo()
        
        --- try it out
        foo:print()
        
        --- create a class torch.Bar which
        --- inherits from Foo
        do
        local Bar, parent = torch.class('torch.Bar', 'Foo')
        
        --- the initializer
        function Bar:__init(stuff)
        --- call the parent initializer on ourself
        parent.__init(self)
        
        --- do some stuff
        self.stuff = stuff
        end
        
        --- a new method
        function Bar:boing()
        print('boing!')
        end
        
        --- override parent's method
        function Bar:print()
        print(self.contents)
        print(self.stuff)
        end
        end
        
        --- create a new instance and use it
        bar = torch.Bar('ha ha!')
        bar:print() -- overrided method
        bar:boing() -- child method
        bar:bip()   -- parent's method
        ```
        
        For advanced users, it is worth mentionning that `torch.class()` actually
        calls [`torch.newmetatable()`](#torch.newmetatable) with a particular
        constructor. The constructor creates a Lua table and set the right
        metatable on it, and then calls ```lua__init()``` if it exists in the
        metatable. It also sets a [factory](#torch.factory) field ```lua__factory``` such that it
        is possible to create an empty object of this class.
        
        
        
        """

   'torch.type':
      'prefix': 'torchtype'
      'body': """
        <a name="torch.type"></a>
        ### [string] torch.type(object) ###
        
        Checks if `object` has a metatable. If it does, and if it corresponds to a
        `Torch` class, then returns a string containing the name of the
        class. Otherwise, it returns the Lua `type(object)` of the object.
        Unlike [`torch.typename()`](#torch.typename), all outputs are strings:
        
        ```lua
        > torch.type(torch.Tensor())
        torch.DoubleTensor
        > torch.type({})
        table
        > torch.type(7)
        number
        ```
        
        
        
        """

   'torch.typename':
      'prefix': 'torchtypename'
      'body': """
        <a name="torch.typename"></a>
        ### [string] torch.typename(object) ###
        
        Checks if `object` has a metatable. If it does, and if it corresponds to a
        `Torch` class, then returns a string containing the name of the
        class. Returns `nil` in any other cases.
        
        ```lua
        > torch.typename(torch.Tensor())
        torch.DoubleTensor
        > torch.typename({})
        
        > torch.typename(7)
        
        ```
        
        A Torch class is a class created with [`torch.class()`](#torch.class) or
        [`torch.newmetatable()`](#torch.newmetatable).
        
        
        
        """

   'torch.typename2id':
      'prefix': 'torchtypename2id'
      'body': """
        <a name="torch.typename2id"></a>
        ### [userdata] torch.typename2id(string) ###
        
        Given a Torch class name specified by `string`, returns a unique
        corresponding id (defined by a `lightuserdata` pointing on the internal
        structure of the class). This might be useful to do a *fast* check of the
        class of an object (if used with [`torch.id()`](#torch.id)), avoiding string
        comparisons.
        
        Returns `nil` if `string` does not specify a Torch object.
        
        
        
        """

   'torch.id':
      'prefix': 'torchid'
      'body': """
        <a name="torch.id"></a>
        ### [userdata] torch.id(object) ###
        
        Returns a unique id corresponding to the `class` of the given *Torch7* object.
        The id is defined by a `lightuserdata` pointing on the internal structure
        of the class.
        
        Returns `nil` if `object` is not a Torch object.
        
        This is different from the `object` id returned by [`torch.pointer()`](#torch.pointer).
        
        
        
        """

   'torch.isTypeOf':
      'prefix': 'torchisTypeOf'
      'body': """
        <a name="torch.isTypeOf"></a>
        ### [boolean] isTypeOf(object, typeSpec) ###
        
        Checks if a given `object` is an instance of the type specified by `typeSpec`.
        `typeSpec` can be a string (including a `string.find` pattern) or the constructor
        object for a Torch class. This function traverses up the class hierarchy,
        so if b is an instance of B which is a subclass of A, then
        `torch.isTypeOf(b, B)` and `torch.isTypeOf(b, A)` will both return `true`.
        
        
        
        """

   'torch.newmetatable':
      'prefix': 'torchnewmetatable'
      'body': """
        <a name="torch.newmetatable"></a>
        ### [table] torch.newmetatable(name, parentName, constructor) ###
        
        Register a new metatable as a Torch type with the given string `name`. The new metatable is returned.
        
        If the string `parentName` is not `nil` and is a valid Torch type (previously created
        by `torch.newmetatable()`) then set the corresponding metatable as a metatable to the returned new
        metatable.
        
        If the given `constructor` function is not `nil`, then assign to the variable `name` the given constructor.
        The given `name` might be of the form `package.className`, in which case the `className` will be local to the
        specified `package`. In that case, `package` must be a valid and already loaded package.
        
        
        
        """

   'torch.factory':
      'prefix': 'torchfactory'
      'body': """
        <a name="torch.factory"></a>
        ### [function] torch.factory(name) ###
        
        Returns the factory function of the Torch class `name`. If the class name is invalid or if the class
        has no factory, then returns `nil`.
        
        A Torch class is a class created with [`torch.class()`](#torch.class) or
        [`torch.newmetatable()`](#torch.newmetatable).
        
        A factory function is able to return a new (empty) object of its corresponding class. This is helpful for
        [object serialization](file.md#torch.File.serialization).
        
        
        
        """

   'torch.getmetatable':
      'prefix': 'torchgetmetatable'
      'body': """
        <a name="torch.getmetatable"></a>
        ### [table] torch.getmetatable(string) ###
        
        Given a `string`, returns a metatable corresponding to the Torch class described
        by `string`. Returns `nil` if the class does not exist.
        
        A Torch class is a class created with [`torch.class()`](#torch.class) or
        [`torch.newmetatable()`](#torch.newmetatable).
        
        Example:
        
        ```lua
        > for k, v in pairs(torch.getmetatable('torch.CharStorage')) do print(k, v) end
        
        __index__       function: 0x1a4ba80
        __typename      torch.CharStorage
        write           function: 0x1a49cc0
        __tostring__    function: 0x1a586e0
        __newindex__    function: 0x1a4ba40
        string          function: 0x1a4d860
        __version       1
        read            function: 0x1a4d840
        copy            function: 0x1a49c80
        __len__         function: 0x1a37440
        fill            function: 0x1a375c0
        resize          function: 0x1a37580
        __index         table: 0x1a4a080
        size            function: 0x1a4ba20
        ```
        
        
        
        """

   'torch.isequal':
      'prefix': 'torchisequal'
      'body': """
        <a name="torch.isequal"></a>
        ### [boolean] torch.isequal(object1, object2) ###
        
        If the two objects given as arguments are *Lua* tables (or *Torch7* objects), then returns `true` if and only if the
        tables (or Torch objects) have the same address in memory. Returns `false` in any other cases.
        
        A Torch class is a class created with [`torch.class()`](#TorchClass) or
        [`torch.newmetatable()`](#torch.newmetatable).
        
        
        
        """

   'torch.getdefaulttensortype':
      'prefix': 'torchgetdefaulttensortype'
      'body': """
        <a name="torch.getdefaulttensortype"></a>
        ### [string] torch.getdefaulttensortype() ###
        
        Returns a string representing the default tensor type currently in use
        by *Torch7*.
        
        
        
        """

   'torch.getenv':
      'prefix': 'torchgetenv'
      'body': """
        <a name="torch.getenv"></a>
        ### [table] torch.getenv(function or userdata) ###
        
        Returns the Lua `table` environment of the given `function` or the given
        `userdata`.  To know more about environments, please read the documentation
        of [`lua_setfenv()`](http://www.lua.org/manual/5.1/manual.html#lua_setfenv)
        and [`lua_getfenv()`](http://www.lua.org/manual/5.1/manual.html#lua_getfenv).
        
        
        
        """

   'torch.version':
      'prefix': 'torchversion'
      'body': """
        <a name="torch.version"></a>
        ### [number] torch.version(object) ###
        
        Returns the field ```lua__version``` of a given object. This might
        be helpful to handle variations in a class over time.
        
        
        
        """

   'torch.pointer':
      'prefix': 'torchpointer'
      'body': """
        <a name="torch.pointer"></a>
        ### [number] torch.pointer(object) ###
        
        Returns a unique id (pointer) of the given `object`, which can be a *Torch7*
        object, a table, a thread or a function.
        
        This is different from the `class` id returned by [`torch.id()`](#torch.id).
        
        
        
        """

   'torch.setdefaulttensortype':
      'prefix': 'torchsetdefaulttensortype'
      'body': """
        <a name="torch.setdefaulttensortype"></a>
        ### torch.setdefaulttensortype([typename]) ###
        
        Sets the default tensor type for all the tensors allocated from this
        point on. Valid types are:
        
        * `torch.ByteTensor`
        * `torch.CharTensor`
        * `torch.ShortTensor`
        * `torch.IntTensor`
        * `torch.FloatTensor`
        * `torch.DoubleTensor`
        
        
        
        """

   'torch.setenv':
      'prefix': 'torchsetenv'
      'body': """
        <a name="torch.setenv"></a>
        ### torch.setenv(function or userdata, table) ###
        
        Assign `table` as the Lua environment of the given `function` or the given
        `userdata`.  To know more about environments, please read the documentation
        of [`lua_setfenv()`](http://www.lua.org/manual/5.1/manual.html#lua_setfenv)
        and [`lua_getfenv()`](http://www.lua.org/manual/5.1/manual.html#lua_getfenv).
        
        
        
        """

   'torch.setmetatable':
      'prefix': 'torchsetmetatable'
      'body': """
        <a name="torch.setmetatable"></a>
        ### [object] torch.setmetatable(table, classname) ###
        
        Set the metatable of the given `table` to the metatable of the Torch
        object named `classname`.  This function has to be used with a lot
        of care.
        
        
        
        """

   'torch.getconstructortable':
      'prefix': 'torchgetconstructortable'
      'body': """
        <a name="torch.getconstructortable"></a>
        ### [table] torch.getconstructortable(string) ###
        
        BUGGY
        Return the constructor table of the Torch class specified by `string`.
        
        
        
        """

   'torch.totable':
      'prefix': 'torchtotable'
      'body': """
        <a name="torch.totable"></a>
        ### [table] torch.totable(object) ###
        
        Converts a Tensor or a Storage to a lua table. Also available as methods: `tensor:totable()` and `storage:totable()`.
        Multidimensional Tensors are converted to a set of nested tables, matching the shape of the source Tensor.
        
        ```lua
        > print(torch.totable(torch.Tensor({1, 2, 3})))
        {
        1 : 1
        2 : 2
        3 : 3
        }
        ```
        
        """

 